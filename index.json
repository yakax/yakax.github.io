[{"categories":["并发编程学习"],"content":"整体结构图 ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:1","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"put 插入 final V putVal(K key, V value, boolean onlyIfAbsent) { // k v 不能为空 if (key == null || value == null) throw new NullPointerException(); // 让高位和地位进行异或运算 充分利用所有位数 int hash = spread(key.hashCode()); // 0表示当前可以放值进去，2表示可能是红黑 int binCount = 0; for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // ----1 这个判断初始化 if (tab == null || (n = tab.length) == 0) tab = initTable(); //----2 算出下标 并且下标下面无头结点 else if ((f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) { // 失败会再次自旋 走其他条件 if (casTabAt(tab, i, null, new Node\u003cK,V\u003e(hash, key, value, null))) break; // no lock when adding to empty bin } // ----3 能到这个条件说明数组存在 并且 对应下标头结点存在 判断当前节点是否正在扩容 else if ((fh = f.hash) == MOVED) // 协助扩容 tab = helpTransfer(tab, f); else { // ----4 当前节点存在头结点 并且可能是链表 也可能是红黑树 V oldVal = null; // 加锁 头结点 头结点赋值在第二个条件赋值的 synchronized (f) { // 再次判断是否是我想要操作的下标头节点 if (tabAt(tab, i) == f) { // fh 默认是0 大于0说明是正常链表 因为treebin头结点hash是 -2 if (fh \u003e= 0) { // 会影响到addcount() binCount = 1; // 循环链表 每次加1 for (Node\u003cK,V\u003e e = f;; ++binCount) { K ek; // 判断key 是否相同 if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { oldVal = e.val; // onlyIfAbsent 是否覆盖原有的值 true 不覆盖 if (!onlyIfAbsent) e.val = value; break; } Node\u003cK,V\u003e pred = e; // 找到尾结点 if ((e = e.next) == null) { pred.next = new Node\u003cK,V\u003e(hash, key, value, null); break; } } } // 如果是红黑树 加入红黑树 else if (f instanceof TreeBin) { Node\u003cK,V\u003e p; // 会影响到addcount() binCount = 2; // 判断 节点与红黑树节点是否冲突、不冲突会返回null值 if ((p = ((TreeBin\u003cK,V\u003e)f).putTreeVal(hash, key, value)) != null) { oldVal = p.val; if (!onlyIfAbsent) //覆盖 p.val = value; } } } } //binCount默认0 如果不为0说明是链表或者红黑 if (binCount != 0) { // 如果链表长度数量大于等于8了转红黑 !但是里面也判断了整个数组长度64的限制 if (binCount \u003e= TREEIFY_THRESHOLD) treeifyBin(tab, i); if (oldVal != null) return oldVal; break; } } } // 增加count数亮 addCount(1L, binCount); return null; } ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:2","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"addCount 算数 fullAddCount 这个方法可以直接看longAdder的longAccumulate这个方法 一模一样 private final void addCount(long x, int check) { CounterCell[] cs; long b, s; //当不存在counterCells 写basecount不成功会进入条件 当counterCells存在不会尝试写basecount if ((cs = counterCells) != null || !U.compareAndSetLong(this, BASECOUNT, b = baseCount, s = b + x)) { CounterCell c; long v; int m; boolean uncontended = true; // 判断counterCells长度和是否存在，存在就尝试写countcell 失败会进入fullAddCount if (cs == null || (m = cs.length - 1) \u003c 0 || (c = cs[ThreadLocalRandom.getProbe() \u0026 m]) == null || !(uncontended = U.compareAndSetLong(c, CELLVALUE, v = c.value, v + x))) { fullAddCount(x, uncontended); return; } // 说明是remove之类的方法进来的：可以直接返回 if (check \u003c= 1) return; //获取当前散列表元素个数，这是一个期望值 不会是最终值 s = sumCount(); } // 大于等于0说明一定是put之类的方法进来的 if (check \u003e= 0) { Node\u003cK,V\u003e[] tab, nt; int n, sc; // s目前是总个数 //s \u003e= (long)(sc = sizeCtl) // true-\u003e 1.当前sizeCtl为一个负数 表示正在扩容中..可以进入帮助扩容 // true-\u003e 2.当前sizeCtl是一个正数，s大于扩容阈值 ..可以进入帮助扩容 //另外条件都是正常成立 while (s \u003e= (long)(sc = sizeCtl) \u0026\u0026 (tab = table) != null \u0026\u0026 (n = tab.length) \u003c MAXIMUM_CAPACITY) { // 获取扩容唯一标识 int rs = resizeStamp(n); // 说明正在扩容 可以协扩容 if (sc \u003c 0) { // 判断扩容戳是否是本次 或者transferIndex小于0说明扩容结束 if ((sc \u003e\u003e\u003e RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || (nt = nextTable) == null || transferIndex \u003c= 0) break; // 让SIZECTL 低位+1 if (U.compareAndSetInt(this, SIZECTL, sc, sc + 1)) // 协助扩容 持有一个链表 transfer(tab, nt); } // 第一次 扩容 会把SIZECTL 改为负数，并且sc的低16位会表示扩容存在的线程 以便于上面if的判断 else if (U.compareAndSetInt(this, SIZECTL, sc, (rs \u003c\u003c RESIZE_STAMP_SHIFT) + 2)) transfer(tab, null); s = sumCount(); } } } 扩容固定标识说明 // 比如 14长度数组超过阀值 这一批的调用的扩容参数返回 都是1000000000011100 static final int resizeStamp(int n) { return Integer.numberOfLeadingZeros(n) | (1 \u003c\u003c (RESIZE_STAMP_BITS - 1)); } ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:3","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"transfer 扩容代码 private final void transfer(Node\u003cK,V\u003e[] tab, Node\u003cK,V\u003e[] nextTab) { int n = tab.length, stride; //根据cpu核心数算出每个线程的扩容数量区间 stride 默认为 16 if ((stride = (NCPU \u003e 1) ? (n \u003e\u003e\u003e 3) / NCPU : n) \u003c MIN_TRANSFER_STRIDE) stride = MIN_TRANSFER_STRIDE; // subdivide range // 说明第一次进入扩容方法 做准备工作 if (nextTab == null) { // initiating try { @SuppressWarnings(\"unchecked\") //创建了一个比扩容之前大一倍的table Node\u003cK,V\u003e[] nt = (Node\u003cK,V\u003e[])new Node\u003c?,?\u003e[n \u003c\u003c 1]; nextTab = nt; } catch (Throwable ex) { // try to cope with OOME sizeCtl = Integer.MAX_VALUE; return; } //新的扩容后的Node[] nextTable = nextTab; //标记需要迁移的长度 这是原来的node长度 transferIndex = n; } //表示新Node[]的长度 int nextn = nextTab.length; // 新的node 设置为fwd 节点 ForwardingNode\u003cK,V\u003e fwd = new ForwardingNode\u003cK,V\u003e(nextTab); //推进标记 用于迁移的时候自循环判断 boolean advance = true; //完成标记 boolean finishing = false; // to ensure sweep before committing nextTab // 表示执行到的区间与上限 int i = 0, bound = 0; for (;;) { //头结点 与头结点hash Node\u003cK,V\u003e f; int fh; // 维护每个线程步长任务区间 while (advance) { //分配任务的开始下标 nextBound代表长度 int nextIndex, nextBound; //CASE1: //成立：表示当前线程的任务尚未完成，还有相应的区间的桶位要处理，--i 就让当前线程处理下一个 桶位. if (--i \u003e= bound || finishing) advance = false; //CASE2: 说明已经分配完毕 else if ((nextIndex = transferIndex) \u003c= 0) { i = -1; advance = false; } //CASE3:分配任务区间 else if (U.compareAndSwapInt (this, TRANSFERINDEX, nextIndex, nextBound = (nextIndex \u003e stride ? nextIndex - stride : 0))) { bound = nextBound; i = nextIndex - 1; advance = false; } } //CASE1：未分配到任务 if (i \u003c 0 || i \u003e= n || i + n \u003e= nextn) { int sc; // 表示扩容完成 使新node 赋值给table if (finishing) { nextTable = null; table = nextTab; //sizeCtl 恢复 等于扩容阀值 sizeCtl = (n \u003c\u003c 1) - (n \u003e\u003e\u003e 1); return; } //未分配到任务 依然要修改SIZECTL -1 代表自己退出了 if (U.compareAndSwapInt(this, SIZECTL, sc = sizeCtl, sc - 1)) { // 如果分配成功 判断自己是否是最后一个线程 不是就正常退出 是的话就让赋值finishing 在上面条件退出 // 这里其实会检查 会执行到上面while循环做检查 如果有遗漏的还会执行 知道执行到上面条件 if ((sc - 2) != resizeStamp(n) \u003c\u003c RESIZE_STAMP_SHIFT) return; finishing = advance = true; i = n; // recheck before commit } } //CASE2: //条件成立：说明当前桶位未存放数据，只需要将此处设置为fwd节点即可。 else if ((f = tabAt(tab, i)) == null) advance = casTabAt(tab, i, null, fwd); //CASE3: //条件成立：如果被迁移过 就再次循环 else if ((fh = f.hash) == MOVED) advance = true; // already processed //CASE4: //前置条件：当前桶位有数据，而且node节点 不是 fwd节点，说明这些数据需要迁移。 else { //sync 加锁当前桶位的头结点 synchronized (f) { //防止在你加锁头对象之前，当前桶位的头对象被其它写线程修改过，导致你目前加锁对象错误... if (tabAt(tab, i) == f) { //ln 表示低位链表引用 //hn 表示高位链表引用 Node\u003cK,V\u003e ln, hn; //条件成立：表示当前桶位是链表桶位 TREEBIN 是-2 if (fh \u003e= 0) { //lastRun //可以获取出 当前链表 末尾连续高位不变的 node // 先算出头结点hash位置 这时这个n是原数组长度 int runBit = fh \u0026 n; Node\u003cK,V\u003e lastRun = f; // 这个循环可以拿到一个连续的节点 优化迁移 for (Node\u003cK,V\u003e p = f.next; p != null; p = p.next) { int b = p.hash \u0026 n; if (b != runBit) { runBit = b; lastRun = p; } } //条件成立：说明lastRun引用的链表为 低位链表，那么就让 ln 指向 低位链表 if (runBit == 0) { ln = lastRun; hn = null; } //否则，说明lastRun引用的链表为 高位链表，就让 hn 指向 高位链表 else { hn = lastRun; ln = null; } // 遍历到lastRun 节点 说明已经到最后了 因为后面是指向一个地方的 for (Node\u003cK,V\u003e p = f; p != lastRun; p = p.next) { int ph = p.hash; K pk = p.key; V pv = p.val; if ((ph \u0026 n) == 0) ln = new Node\u003cK,V\u003e(ph, pk, pv, ln); else hn = new Node\u003cK,V\u003e(ph, pk, pv, hn); } // 迁移节点 //get 的时候是通过(n - 1) \u0026 h 会发现 用原数组 长度 就可以判断出来了 // 新的长度就是原长度左移了一位 所以只需要把这一位加进去就ok // 重点关注图片中hash后面几位 变的其实就是加了旧数组长度 setTabAt(nextTab, i, ln); setTabAt(nextTab, i + n, hn); setTabAt(tab, i, fwd); advance = true; } //条件成立：表示当前桶位是 红黑树 代理结点TreeBin else if (f instanceof TreeBin) { //转换头结点为 treeBin引用 t TreeBin\u003cK,V\u003e t = (TreeBin\u003cK,V\u003e)f; //低位双向链表 lo 指向低位链表的头 loTail 指向低位链表的尾巴 TreeNode\u003cK,V\u003e lo = null, loTail = null; //高位双向链表 lo 指向高位链表的头 loTail 指向高位链表的尾巴 TreeNode\u003cK,V\u003e hi = null, hiTail = null; //lc 表示低位链表元素数量 //hc 表示高位链表元素数量 int lc = 0, hc = 0; //迭代TreeBin中的双向链表，从头结点 至 尾节点 for (Node\u003cK,V\u003e e = t.first; e != null; e = e.next) { // h 表示循环处理当前元素的 hash int h = e.hash; //使用当前节点 构建出来的 新的 TreeNode TreeNode\u003cK,V\u003e p = new TreeNode\u003cK,V\u003e (h, e.key, e.val, null, null); //条件成立：表示当前循环节点 属于低位链 节点 if ((h \u0026 n) == 0) { //条件成立：说明当前低位链表 还没有数据 if ((p.prev = loTail) == null) lo = p; //说明 低位链表已经有数据了，此时当前元素 追加到 低位链表","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:4","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"helpTransfer协助扩容代码 final Node\u003cK,V\u003e[] helpTransfer(Node\u003cK,V\u003e[] tab, Node\u003cK,V\u003e f) { Node\u003cK,V\u003e[] nextTab; int sc; // 这些条件基本都恒成立 if (tab != null \u0026\u0026 (f instanceof ForwardingNode) \u0026\u0026 (nextTab = ((ForwardingNode\u003cK,V\u003e)f).nextTable) != null) { // 拿到表示戳 int rs = resizeStamp(tab.length); // 等知道扩容完成 才返回 // sc=sizectl 小于0的情况下说明还可以协助 这一点是在第一次参与扩容时算的负值 while (nextTab == nextTable \u0026\u0026 table == tab \u0026\u0026 (sc = sizeCtl) \u003c 0) { // 判断扩容戳是否是本次 或者transferIndex小于0说明扩容结束 if ((sc \u003e\u003e\u003e RESIZE_STAMP_SHIFT) != rs || sc == rs + 1 || sc == rs + MAX_RESIZERS || transferIndex \u003c= 0) break; // 如果还需要协助扩容 加入扩容 if (U.compareAndSwapInt(this, SIZECTL, sc, sc + 1)) { transfer(tab, nextTab); break; } } return nextTab; } return table; } 扩容的最细粒度是通过当前机器的核心算出来了。单核一个线程最小粒度是16个长度 协助不了的线程会在上面代码一直while。因为sc的值代表了能允许多少线程帮助扩容。 扩容时会产生两个链 一个是要移动的链，一个是不移动的，因为扩容后有些数据已经不再原有位置。 ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:5","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"get() 获取元素 public V get(Object key) { Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e e, p; int n, eh; K ek; // 算出此key 的hash 这里如果key为null 就会报空指针 int h = spread(key.hashCode()); // 判断 node是否存在并且 是否能定位到值 if ((tab = table) != null \u0026\u0026 (n = tab.length) \u003e 0 \u0026\u0026 (e = tabAt(tab, (n - 1) \u0026 h)) != null) { // 判断这个node头元素是否等于是否真的 等于我的key // 因为就算你的下标相同 key也可能不相同 因为你的hash很长但是\u0026的长度不一定很长 最终运算的只有后面几位 if ((eh = e.hash) == h) { if ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek))) // 相同就返回 return e.val; } // 只有一个两种情况 treebin节点或者fwd节点 else if (eh \u003c 0) // 去查询值 return (p = e.find(h, key)) != null ? p.val : null; // 如果是链表 就循环查询 while ((e = e.next) != null) { if (e.hash == h \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) return e.val; } } return null; } ForwardingNode.find() Node\u003cK,V\u003e find(int h, Object k) { // loop to avoid arbitrarily deep recursion on forwarding nodes outer: for (Node\u003cK,V\u003e[] tab = nextTable;;) { Node\u003cK,V\u003e e; int n; // 数据不存在 if (k == null || tab == null || (n = tab.length) == 0 || (e = tabAt(tab, (n - 1) \u0026 h)) == null) return null; for (;;) { int eh; K ek; // 存在 并且key完全相同 if ((eh = e.hash) == h \u0026\u0026 ((ek = e.key) == k || (ek != null \u0026\u0026 k.equals(ek)))) return e; // 红黑 或者fwd节点 if (eh \u003c 0) { // 如果是fwd节点 就赋值 重新循环(高并发情况下 可能扩容后再次扩容) if (e instanceof ForwardingNode) { tab = ((ForwardingNode\u003cK,V\u003e)e).nextTable; continue outer; } // 调用红黑树查询 else return e.find(h, k); } // 赋值下一个循环元素-- 如果是最后节点还没找到就退出 if ((e = e.next) == null) return null; } } } ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:6","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"remove() 删除元素 // 要删除的key 删除对应的value 要替换的value // 意思就是如果传了value 和 cv 如果传值进来的cv与key对应的value 相同 就替换成传进来的value final V replaceNode(Object key, V value , Object cv) { int hash = spread(key.hashCode()); // 老规矩 赋值 自循环 for (Node\u003cK,V\u003e[] tab = table;;) { Node\u003cK,V\u003e f; int n, i, fh; // 不存在就退出 if (tab == null || (n = tab.length) == 0 || (f = tabAt(tab, i = (n - 1) \u0026 hash)) == null) break; // 如果是在扩容的就协助扩容 else if ((fh = f.hash) == MOVED) tab = helpTransfer(tab, f); else { V oldVal = null; // 用于后面 判断是否删除值 boolean validated = false; synchronized (f) { // 判断加锁对象是否已经改变 if (tabAt(tab, i) == f) { // 这是链表情况 fwd情况已经在加锁的时候规避了 if (fh \u003e= 0) { validated = true; for (Node\u003cK,V\u003e e = f, pred = null;;) { K ek; // 判断key是否相同 if (e.hash == hash \u0026\u0026 ((ek = e.key) == key || (ek != null \u0026\u0026 key.equals(ek)))) { V ev = e.val; //然后判断是否需要替换值 if (cv == null || cv == ev || (ev != null \u0026\u0026 cv.equals(ev))) { oldVal = ev; // 如果不替换 if (value != null) e.val = value; // 改变链表情况 比如不是头结点情况下 让上一个结点指向当前节点的下一个节点完成链表链接 else if (pred != null) pred.next = e.next; else // 如果是头结点 setTabAt(tab, i, e.next); } break; } pred = e; // 再次赋值 循环 if ((e = e.next) == null) break; } } else if (f instanceof TreeBin) { validated = true; TreeBin\u003cK,V\u003e t = (TreeBin\u003cK,V\u003e)f; TreeNode\u003cK,V\u003e r, p; // 查询 是否存在 if ((r = t.root) != null \u0026\u0026 (p = r.findTreeNode(hash, key, null)) != null) { V pv = p.val; // 是否替换 if (cv == null || cv == pv || (pv != null \u0026\u0026 cv.equals(pv))) { oldVal = pv; if (value != null) p.val = value; // 如果removeTreeNode 返回真 说明需要红黑转链表 else if (t.removeTreeNode(p)) // 转链表 setTabAt(tab, i, untreeify(t.first)); } } } } } // 是否删除成功 if (validated) { if (oldVal != null) { if (value == null) // 如果不是替换就减少元素个数 addCount(-1L, -1); return oldVal; } break; } } } return null; } ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:7","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"removeTreeNode() 转链表代码 if (first == null) { root = null; // 如果头结点都为null 转链表 return true; } // root.right == null // 右孩子为空 或 // || root.left == null // 左孩子为空 或 // || root.left.left == null // 左孙子为空 if ((r = root) == null || r.right == null || // too small (rl = r.left) == null || rl.left == null) return true; 这里有点没理解 网上答案也模糊 看面试问不问了，如果问了 直接问面试官。 ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:8","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"红黑树基本说明 红黑树的头结点会设置hash 为-2 以代表这个节点下面是红黑树 红黑树可以允许多个读取，读取的时候会尝试修改 TreeBin的 lockState的值 增加4 红黑树写的时候就会尝试cas修改lockState 为1。如果是0就会成功，如果不为0，就会阻塞等待。 锁住的都是头结点 并且修改的都是头结点的值,也就是TreeBin。 ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:9","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"统计map 大小代码 通过两个值来存储元素个数 baseCount 与CounterCell[]数组 CounterCell里面有个value存储值 先通过cas操作basecount操作，如果存在冲突就 然后通过随机函数下标修改contercell的值 如果失败 进入fullAddCount 如果没初始化就初始化。 通过cas修改cellsBusy 标识加锁。修改CounterCell的值。 总的来说就是通过cas修改baseCount 失败，再尝试修改CounterCell 数组里面的值，还失败就产生一个新的CounterCell 。 ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:10","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"退化情况 在扩容时小于6的时候退化 或者删除元素时判断树太小的时候退化，也就是判断左右节点和父节点的左节点为空的情况。 ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:11","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"1.7与1.8版本结构对比 1.8 是数组 +单向链表+红黑 node链表 treebin头与TreeNode节点 1.7是数组+segment链表 每段数组是个segment 对象 里面存储是多个entry 主要优化还是并发协助扩容这里优化; 和查询性能优化 元素个数 1.7是遍历segment 1.8是遍历CounterCell数组+basecount 相对数量少 红黑树的查询在量大的情况下O(logN)肯定好于O(N) ","date":"2021-01-22","objectID":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:12","tags":["ConcurrentHashMap"],"title":"ConcurrentHashMap源码学习","uri":"/concurrenthashmap%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"基本属性 /* Possible state transitions: * NEW -\u003e COMPLETING -\u003e NORMAL * NEW -\u003e COMPLETING -\u003e EXCEPTIONAL * NEW -\u003e CANCELLED * NEW -\u003e INTERRUPTING -\u003e INTERRUPTED */ private volatile int state; //线程状态 private static final int NEW = 0; // 进来未执行的状态 private static final int COMPLETING = 1; // 表示正在结束，临时状态，用于结束前cas操作;异常和有结果前都会有 private static final int NORMAL = 2; // 正常结束 private static final int EXCEPTIONAL = 3; // 异常结束 private static final int CANCELLED = 4; // 任务被取消 private static final int INTERRUPTING = 5; // 中断中 也是临时状态 private static final int INTERRUPTED = 6; // 已中断 // 运行任务 private Callable\u003cV\u003e callable; // 整个任务周期中 存放值的对象，会有异常或者正常的值 private Object outcome; // 正在执行任务的线程 private volatile Thread runner; // get任务阻塞的线程 会存在这个队列里面 private volatile WaitNode waiters; 构造方法 public FutureTask(Callable\u003cV\u003e callable) { if (callable == null) throw new NullPointerException(); //callable就是程序员自己实现的业务类 this.callable = callable; //设置当前任务状态为 NEW this.state = NEW; // ensure visibility of callable } public FutureTask(Runnable runnable, V result) { // 利用适配器模式 把runnable 换成Callable this.callable = Executors.callable(runnable, result); this.state = NEW; // ensure visibility of callable } 状态相关方法 // 是否中断 public boolean isCancelled() { return state \u003e= CANCELLED; } // 只要是不等于new 都属于线程已经开始执行的标记 public boolean isDone() { return state != NEW; } 取消任务 public boolean cancel(boolean mayInterruptIfRunning) { // 如果状态等于new 并且 修改状态成功 说明可以取消 否则返回false 取消是失败 // mayInterruptIfRunning 是否会发生中断情况 说白了就是以中断抛异常结束， if (!(state == NEW \u0026\u0026 UNSAFE.compareAndSwapInt(this, stateOffset, NEW, mayInterruptIfRunning ? INTERRUPTING : CANCELLED))) return false; try { // in case call to interrupt throws exception if (mayInterruptIfRunning) { try { Thread t = runner; if (t != null) // 加中断标记，具体看线程是否响应中断，捕获异常 自己处理 t.interrupt(); } finally { // final state //设置任务状态为 中断完成。 UNSAFE.putOrderedInt(this, stateOffset, INTERRUPTED); } } } finally { // 使阻塞队列的线程全部唤醒退出 finishCompletion(); } return true; } 执行任务方法 public void run() { // 判断线程状态 是否已经执行过或 cas失败 说明runnerOffset 已经被别的线程修改了 // runnerOffset 时futuretask的局部变量 if (state != NEW || !UNSAFE.compareAndSwapObject(this, runnerOffset, null, Thread.currentThread())) return; try { Callable\u003cV\u003e c = callable; // 判断任务是否存在 状态是否是新状态 if (c != null \u0026\u0026 state == NEW) { V result; boolean ran; try { // 执行任务 result = c.call(); ran = true; } catch (Throwable ex) { // 抛异常 result = null; ran = false; // 设置异常值 setException(ex); } if (ran) // 设置返回值 set(result); } } finally { //设置当前线程引用为空 runner = null; int s = state; // 如果正在中断中 if (s \u003e= INTERRUPTING) // 直到中断结束 或者改变成其他状态 handlePossibleCancellationInterrupt(s); } } 设置返回值 protected void setException(Throwable t) { //使用CAS方式设置当前任务状态为 完成中.. //失败的话 外部线程等不及了，直接在set执行CAS之前 将 task取消了。 很小概率事件。 if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) { //引用的是 callable 向上层抛出来的异常。 outcome = t; //将当前任务的状态 修改为 EXCEPTIONAL UNSAFE.putOrderedInt(this, stateOffset, EXCEPTIONAL); // final state // 唤醒获取数据的节点 finishCompletion(); } } protected void set(V v) { if (UNSAFE.compareAndSwapInt(this, stateOffset, NEW, COMPLETING)) { outcome = v; //将结果赋值给 outcome之后，马上会将当前任务状态修改为 NORMAL 正常结束状态。 UNSAFE.putOrderedInt(this, stateOffset, NORMAL); finishCompletion(); } } 唤醒所有节点 注意 done();方法 private void finishCompletion() { //q指向waiters 链表的头结点。 for (WaitNode q; (q = waiters) != null;) { //使用cas设置 waiters 为 null 为了保证只有一个线程在循环这个唤醒操作 因为外部方法cancel也会触发唤醒节点操作 if (UNSAFE.compareAndSwapObject(this, waitersOffset, q, null)) { //自循环 唤醒所有获取数据的线程 for (;;) { //获取当前node节点封装的 thread Thread t = q.thread; //条件成立：说明当前线程不为null if (t != null) { q.thread = null;//help GC //唤醒当前节点对应的线程 LockSupport.unpark(t); } //next 当前节点的下一个节点 继续唤醒 WaitNode next = q.next; // 当下一个几点为空时 说明是最后节点咯 if (next == null) break; q.next = null; // unlink to help gc q = next; } break; } } // 这个方法 就是我们执行完task任务的回调方法 一般我们都会重写逻辑在里面 done(); //将callable 设置为null helpGC callable = null; } get 方法阻塞队列 // 可以带有超时时间 public V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException {","date":"2021-01-22","objectID":"/futuretask%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:0","tags":["FutureTask"],"title":"FutureTask源码学习","uri":"/futuretask%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"ReentrantLock 里面所使用到的基本结构 持有锁的waitstatus 状态 来标识node线程状态 重入是通过改变Lock本身的 state值值来说明线程的重入的。并且重入只改变了属性，并没有再次加锁。 线程的中断提醒**lock.lock()**情况下是不会提醒的 但是 此线程的 “打扰标志”会被设置， 可以通过isInterrupted()查看并 作出处理 lock是无法主动停止线程的，java也不推荐主动停止线程，一般都是通过中断标记，用户自己来处理线程后续的情况。 这也就是线程在被park阻塞 后被unpark唤醒时会返回线程的中断标记，到上层来通知调用者来处理线程的中断情况。 lock.lockInterruptibly() 会有提醒 并且会抛出异常来做中断处理; 非公平与公平的区别就在于非公平锁实现调用加锁时不是直接进入队列，而是先去cas一次尝试加锁。 并且在加锁前公平锁判断了当前node前面是否还有排队的node。而非公平锁没有判断，是直接通过cas尝试加锁。 cas 是更改的state值，通过偏移量寻找0去尝试更改为1 只有在第一次加锁会通过state的值做操作。 解锁就是唤醒下一个线程：先是改变state值 直到state=0才会执行unpark操作才会唤醒下一个线程，意味着lock与unlock需要成对出现。 ","date":"2021-01-22","objectID":"/reentrantlock-aqs%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:0","tags":["ReentrantLock","AQS"],"title":"ReentrantLock + AQS源码学习","uri":"/reentrantlock-aqs%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"代码分析–公平锁路线 加锁–lock public final void acquire(int arg) { // 条件一：!tryAcquire 尝试获取锁 获取成功返回true 获取失败 返回false // 条件一：如果是第一个线程 连node 都不会创建就直接加锁了 // 条件二.1 尝试添加进入AQS队列 // 条件二.2 acquireQueued 挂起当前线程 唤醒后相关的逻辑.. // acquireQueued 返回true 表示挂起过程中线程被中断唤醒过.. false 表示未被中断过.. if (!tryAcquire(arg) \u0026\u0026 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } 尝试加锁–第一个线程加锁 protected final boolean tryAcquire(int acquires) { final Thread current = Thread.currentThread(); int c = getState(); // 如果当前状态是0 可以尝试加锁 if (c == 0) { // 判断是否有前置node存在 有就返回true 说明当前线程只能入队 if (!hasQueuedPredecessors() \u0026\u0026 compareAndSetState(0, acquires)) { // 如果当前线程前面没得队列 那么可以尝试加锁 并且设置当前加锁线程给自己 setExclusiveOwnerThread(current); return true; } }else if (current == getExclusiveOwnerThread()) { // 线程相等 说明重入 int nextc = c + acquires; if (nextc \u003c 0) throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false; } 加锁失败–添加进入AQS队列 private Node addWaiter(Node mode) { // 将当前线程封装成node Node node = new Node(Thread.currentThread(), mode); // 尾巴结点拿出来赋值 据了解 别人这样写完全是更直观的看代码 Node pred = tail; // 如果node已经存在就尝试加入尾结点后面 if (pred != null) { // 设置当前节点上一个节点为 之前的尾结点 node.prev = pred; if (compareAndSetTail(pred, node)) { // 加入尾结点成功 设置之前尾结点的下一个几点为当前节点 pred.next = node; return node; } } // 1.当前队列是空队列 tail == null // 2.CAS竞争入队失败..会来到这里.. enq(node); return node; } // 自循环尝试加入队列 private Node enq(final Node node) { for (;;) { Node t = tail; // 如果是空 队列 初始化一个node 为头 if (t == null) { // Must initialize if (compareAndSetHead(new Node())) tail = head; } else { // 后面循环肯定会进入这里 node.prev = t; // 尝试在尾巴节点加入自己 自循环知道成功为止 if (compareAndSetTail(t, node)) { t.next = node; return t; } } } } 挂起线程 final boolean acquireQueued(final Node node, int arg) { // 标识抢占锁是否成功 boolean failed = true; try { // 中断状态 boolean interrupted = false; for (;;) { // 自循环 // 拿到上一个节点 final Node p = node.predecessor(); // 如果上一个节点是头结点 说明有权利去抢占锁 if (p == head \u0026\u0026 tryAcquire(arg)) { // 加锁成功 设置自己为头结点 setHead(node); p.next = null; // help GC failed = false; return interrupted; } // 条件1 判断当前线程是否需要挂起 if (shouldParkAfterFailedAcquire(p, node) \u0026\u0026 parkAndCheckInterrupt()) // park 的时候会通过Thread.interrupted() 这个返回当前线程中断情况 // park 阻塞当前线程 如果中断 会返回interrupted以便selfInterrupt()这个方法使用，让外部线程知道中断状态 interrupted = true; } } finally { // 正常不会进来的 中断异常加锁方式 抛异常 会出来lockInterruptibly() 这个加锁方式 if (failed) cancelAcquire(node); } } // 记住一点 外面是自循环 还会进来的 最终还是会执行到parkAndCheckInterrupt来park线程 private static boolean shouldParkAfterFailedAcquire(Node pred, Node node) { int ws = pred.waitStatus; // 拿到上一个节点的 状态 如果是signal=-1 说明 是一个持有锁的线程或者是一个可以唤醒下一个节点的线程 if (ws == Node.SIGNAL) return true; if (ws \u003e 0) { // 找到前置节点可以唤醒下一个节点的node 让他作为当前线程的前置节点 // 如果前置节点不是signal状态 那么当前线程将永远不会被唤醒 do { node.prev = pred = pred.prev; } while (pred.waitStatus \u003e 0); pred.next = node; } else { // 正常情况 进入这里 //当前node前置节点的状态就是 0 的这一种情况。 //将当前线程node的前置node，状态强制设置为 SIGNAl，表示前置节点释放锁之后需要 喊醒我.. // 这个 需要在释放锁成功后逻辑 看得出来 compareAndSetWaitStatus(pred, ws, Node.SIGNAL); } return false; } 释放锁|唤醒后继节点–unlock public final boolean release(int arg) { //尝试释放锁，tryRelease 返回true 表示当前线程已经完全释放锁 //返回false，说明当前线程尚未完全释放锁.. 解锁需要成对出现 if (tryRelease(arg)) { //头结点 Node h = head; //条件一:成立，说明队列中的head节点已经初始化过了，ReentrantLock 在使用期间 发生过 多线程竞争了... // 只有在第二个线程进来时就会有head //条件二：条件成立，说明当前head后面一定插入过node节点。节点去唤醒 // waitStatus 其他状态 后继节点在park前会改变前节点的值为-1 或者有其他状态 if (h != null \u0026\u0026 h.waitStatus != 0) //唤醒后继节点.. unparkSuccessor(h); return true; } return false; } protected final boolean tryRelease(int releases) { // 状态-1 int c = getState() - releases; // 异常情况 if (Thread.currentThread() != getExclusiveOwnerThread()) throw new IllegalMonitorStateException(); boolean free = false; if (c == 0) { free = true; // 释放锁成功 setExclusiveOwnerThread(null); } // 如果是重复多次 将会返回false setState(c); return free; } // 唤醒后继节点 private void unparkSuccessor(Node node) { int ws = node.waitStatus; // 如果是小于0 说明下一个节点已经设置过你的值 直接开始修改值 这里对应shouldParkAfterFailedAcquire 这个方法 if (ws \u003c 0) compareAndSetWaitStatus(node, ws, 0); Node s = node.next; // 当前节点就是tail节点时 s == null // ","date":"2021-01-22","objectID":"/reentrantlock-aqs%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:1","tags":["ReentrantLock","AQS"],"title":"ReentrantLock + AQS源码学习","uri":"/reentrantlock-aqs%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"属性字段说明 //高3位：表示当前线程池运行状态 除去高3位之后的低位：表示当前线程池中所拥有的线程数量 private final AtomicInteger ctl = new AtomicInteger(ctlOf(RUNNING, 0)); //表示在ctl中，低COUNT_BITS位 是用于存放当前线程数量的位。 private static final int COUNT_BITS = Integer.SIZE - 3; //低COUNT_BITS位 所能表达的最大数值。 000 11111111111111111111 =\u003e 5亿多。 private static final int CAPACITY = (1 \u003c\u003c COUNT_BITS) - 1; // runState is stored in the high-order bits //111 000000000000000000 转换成整数，其实是一个负数 private static final int RUNNING = -1 \u003c\u003c COUNT_BITS; //000 000000000000000000 private static final int SHUTDOWN = 0 \u003c\u003c COUNT_BITS; //001 000000000000000000 private static final int STOP = 1 \u003c\u003c COUNT_BITS; //010 000000000000000000 private static final int TIDYING = 2 \u003c\u003c COUNT_BITS; //011 000000000000000000 private static final int TERMINATED = 3 \u003c\u003c COUNT_BITS; // Packing and unpacking ctl //获取当前线程池运行状态 //~000 11111111111111111111 =\u003e 111 000000000000000000000 //c == ctl = 111 000000000000000000111 //111 000000000000000000111 //111 000000000000000000000 //111 000000000000000000000 private static int runStateOf(int c) { return c \u0026 ~CAPACITY; } //获取当前线程池线程数量 //c == ctl = 111 000000000000000000111 //111 000000000000000000111 //000 111111111111111111111 //000 000000000000000000111 =\u003e 7 private static int workerCountOf(int c) { return c \u0026 CAPACITY; } //用在重置当前线程池ctl值时 会用到 //rs 表示线程池状态 wc 表示当前线程池中worker（线程）数量 //111 000000000000000000 //000 000000000000000111 //111 000000000000000111 private static int ctlOf(int rs, int wc) { return rs | wc; } 添加execute public void execute(Runnable command) { //非空判断.. if (command == null) throw new NullPointerException(); //获取ctl最新值赋值给c，ctl ：高3位 表示线程池状态，低位表示当前线程池线程数量。 int c = ctl.get(); //workerCountOf(c) 获取出当前线程数量 //条件成立：表示当前线程数量小于核心线程数，此次提交任务，直接创建一个新的worker，对应线程池中多了一个新的线程。 if (workerCountOf(c) \u003c corePoolSize) { //addWorker 即为创建线程的过程，会创建worker对象，并且将command作为firstTask //core == true 表示采用核心线程数量限制 false表示采用 maximumPoolSize if (addWorker(command, true)) //创建成功后，直接返回。addWorker方法里面会启动新创建的worker，将firstTask执行。 return; //1.存在并发现象，execute方法是可能有多个线程同时调用的，当workerCountOf(c) \u003c corePoolSize成立后， //其它线程可能也成立了，并且向线程池中创建了worker。这个时候线程池中的核心线程数已经达到，所以... //2.当前线程池状态发生改变了。 RUNNING SHUTDOWN STOP TIDYING　TERMINATION //当线程池状态是非RUNNING状态时，addWorker(firstTask!=null, true|false) 一定会失败。 //SHUTDOWN 状态下，也有可能创建成功。前提 firstTask == null 而且当前 queue 不为空。特殊情况。 c = ctl.get(); } //执行到这里有几种情况？ //1.当前线程数量已经达到corePoolSize //2.addWorker失败.. //条件成立：说明当前线程池处于running状态，则尝试将 task 放入到workQueue中。 if (isRunning(c) \u0026\u0026 workQueue.offer(command)) { //执行到这里，说明offer提交任务成功了.. //再次获取ctl保存到recheck。 int recheck = ctl.get(); //条件一：! isRunning(recheck) 成立：说明你提交到队列之后，线程池状态被外部线程给修改 比如：shutdown()shutdownNow() //这种情况 需要把刚刚提交的任务删除掉。 //条件二：remove(command) 有可能成功，也有可能失败 //成功：提交之后，线程池中的线程还未消费（处理） //失败：提交之后，在shutdown() shutdownNow()之前，就被线程池中的线程 给处理。 if (! isRunning(recheck) \u0026\u0026 remove(command)) //提交之后线程池状态为 非running 且 任务出队成功，走个拒绝策略。 reject(command); //有几种情况会到这里？ //1.当前线程池是running状态(这个概率最大) //2.线程池状态是非running状态 但是remove提交的任务失败. //担心 当前线程池是running状态，但是线程池中的存活线程数量是0，这个时候，如果是0的话，会很尴尬，任务没线程去跑了, //这里其实是一个担保机制，保证线程池在running状态下，最起码得有一个线程在工作。 else if (workerCountOf(recheck) == 0) addWorker(null, false); }else if (!addWorker(command, false)) //执行到这里，有几种情况？ //1.offer失败 //2.当前线程池是非running状态 //1.offer失败，需要做什么？ 说明当前queue 满了！这个时候 如果当前线程数量尚未达到maximumPoolSize的话，会创建新的worker直接执行command //假设当前线程数量达到maximumPoolSize的话，这里也会失败，也走拒绝策略。 //2.线程池状态为非running状态，这个时候因为 command != null addWorker 一定是返回false。 reject(command); } 添加工作线程 从这里就可以看出线程是不区分核心不核心的，知识一个判断而已 成功：创建worker 成功 并且 启动成功 失败：线程数量判断、线程池状态、worker创建失败、worker启动失败 private boolean addWorker(Runnable firstTask, boolean core) { retry: // 自旋 并且有retry这个跳出 for (;;) { // 获取ctl 这个ctl是能算出线程状态和线程数量 int c = ctl.get(); // 获取状态 int rs = runStateOf(c); // 大于等于SHUTDOWN 说明不是运行状态 // 第二个条件是当前线程是SHUTDOWN 但是还有任务没执行完 也不运行添加任务 if (rs \u003e= SHUTDOWN \u0026\u0026 ! (rs == SHUTDOWN \u0026\u0026 firstTask == null \u0026\u0026! workQueue.isEmpty())) return false; for (;;) { // 获取线程数 int wc = workerCountOf(c); // 线程数判断 if (wc \u003e= CAPACITY || wc \u003e= (core ? corePoolSize : maximumPoolSize)) return fa","date":"2021-01-22","objectID":"/threadpoolexecutor%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:0","tags":["ThreadPoolExecutor","线程池"],"title":"ThreadPoolExecutor源码学习","uri":"/threadpoolexecutor%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"一条sql执行基本流程 在老版本的MySQL中sql通过连接进来时会有查询缓存这一块的逻辑。 查询解析器解析sql 是否正确后，然后基本优化sql，生成执行计划，选择引擎执行。 而InnoDB 在执行时 为了保证对数据的执行更快，省去磁盘IO开销 引入了Buffer Pool ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:0:0","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"Buffer Pool 当需要访问某个页的数据时，就会把完整的页的数据全部加载到内存中，也就是说即使我们只需要访问一个页的一条记录，那也需要先把整个页的数据加载到内存中，从某种意义上来说buffer pool 完全可以替代了查询缓存。 默认情况下 Buffer Pool 只有128M大小，参数innodb_buffer_pool_size; Buffer pool 通常存储的是一个个数据页，一个页大小通常16k,不过每个数据页里面会有描述信息。 ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:0","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"free与 flush链表 Buffer pool 里面每个数据页都会有描述两个链表 一个free链表描述空闲缓存链 一个flush描述被修改过的数据块。 free链表 与flush链表都如下图 ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:1","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"缓存数据的淘汰策略 LRU链表 正常的LRU链表可能会导致MySQL会频繁的淘汰末尾数据，然后去加载数据库的数据到缓存页。(主要是全表扫描、查询预读导致) 引入冷热数据LRU链表结构 冷热数据比例是根据 innodb_old_blocks_pct 默认37 冷数据占比 数据第一次会被加载进入冷数据区域，1s后你又继续访问，就会进入热数据区域。1s内访问是不会加载进去的。 若这个数据页在LRU链表中存在的时间超过了1秒，就把它移动到冷数据链表头部 这样也能 保证预读进来的数据、或者全表扫描进来的数据，不会进入热数据区域。 还有个细节，热数据的位置移动：比如有100个热数据页，当你访问后面75个才会移动，前面25个是不会移动的。 冷数据只要在1s后被访问是会进入热数据链表的。并且会在头部 上面这个思想，完全可以用在业务系统上面的、比如redis热数据的缓存预加载 后台线程 定时把flush链表数据刷入磁盘，定时把冷数据删除并刷入磁盘，来腾出链表空间。 ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:2","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"多个Buffer Pool实例 在多线程环境下，访问Buffer Pool中的各种链表都需要加锁处理啥的，为了应对吞吐量，我们需要多个buffer pool,每个Buffer Pool都称为一个实例，它们都是独立的，独立的去申请内存空间，独立的管理各种链表。 参数innodb_buffer_pool_instances 当innodb_buffer_pool_size的值小于1G的 时候设置多个实例是无效的，InnoDB会默认把innodb_buffer_pool_instances 的值修改为1 innodb_buffer_pool_size = 8589934592 #8g innodb_buffer_pool_instances = 4 #4个 innodb_buffer_pool_size必须是innodb_buffer_pool_chunk_size × innodb_buffer_pool_instances的倍数 （这主要是想保证每一个Buffer Pool实例中包含的chunk数量相同）。 假设我们指定的innodb_buffer_pool_chunk_size的值是128M，innodb_buffer_pool_instances的值是16，那么这两个值的乘积就是2G，也就是说innodb_buffer_pool_size的值必须是2G或者2G的整数倍。 buffer pool 内存由多个 check组成 默认128M大小。 在运行期间，如果我们机器内存变大了。他会自己申请chunk来自动增加总体缓存大小。 在实际生产环境中：buffer pool的大小、数量、chunk的大小都是需要优化的东西。很影响并发情况； 如果设置不好：比如说设置小了。当缓存满了。你会去频繁的刷入缓存页尾部数据到磁盘腾出空间，然后从磁盘加载你的数据到缓存 这就是典型的缓存也使用很快，刷数据到磁盘很慢的情况。如果要加快刷盘，因为这也是磁盘io频繁，影响性能。这关键在于你的buffer pool大小情况。如果够大就好。 所以一般数据库服务器是需要大内存的。经验值3/4 ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:3","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"查看系统pool情况 SHOW ENGINE INNODB STATUS Total memory allocated，这就是说buffer pool最终的总大小是多少 Buffer pool size，这就是说buffer pool一共能容纳多少个缓存页 Free buffers，这就是说free链表中一共有多少个空闲的缓存页是可用的 Database pages和Old database pages，就是说lru链表中一共有多少个缓存页，以及冷数据区域里的缓存页数量 Modified db pages，这就是flush链表中的缓存页数量 Pending reads和Pending writes，等待从磁盘上加载进缓存页的数量，还有就是即将从lru链表中刷入磁盘的数量、即将从flush链表中刷入磁盘的数量 Pages made young和not young，这就是说已经lru冷数据区域里访问之后转移到热数据区域的缓存页的数量，以及在lru冷数据区域里1s内被访问了没进入热数据区域的缓存页的数量 youngs/s和not youngs/s，这就是说每秒从冷数据区域进入热数据区域的缓存页的数量，以及每秒在冷数据区域里被访问了但是不能进入热数据区域的缓存页的数量 Pages read xxxx, created xxx, written xxx，xx reads/s, xx creates/s, 1xx writes/s，这里就是说已经读取、创建和写入了多少个缓存页，以及每秒钟读取、创建和写入的缓存页数量 Buffer pool hit rate xxx / 1000，这就是说每1000次访问，有多少次是直接命中了buffer pool里的缓存的 young-making rate xxx / 1000 not xx / 1000，每1000次访问，有多少次访问让缓存页从冷数据区域移动到了热数据区域，以及没移动的缓存页数量 LRU len：这就是lru链表里的缓存页的数量 I/O sum：最近50s读取磁盘页的总数 I/O cur：现在正在读取磁盘页的数量 ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:4","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"普通索引与唯一索引在change buffer的情况 change buffer用的是buffer pool里的内存，因此不能无限增大。change buffer的大小， 可以通过参数innodb_change_buffer_max_size来动态设置。 这个参数设置为50的时候，表示change buffer的大小最多只能占用bufferpool的50%。 select id from T where k=5 查询过程 对于普通索引来说，查找到满足条件的第一个记录(5,500)后，需要查找下一个记录，直到碰到第一个不满足k=5条件的记录 对于唯一索引来说，由于索引定义了唯一性，查找到第一个满足条件的记录后，就会停止继续检索 更新过程: 记录页在内存中 对于唯一索引来说，找到3和5之间的位置，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，找到3和5之间的位置，插入这个值，语句执行结束。 记录也不在内存中 对于唯一索引来说，需要将数据页读入内存，判断到没有冲突，插入这个值，语句执行结束； 对于普通索引来说，则是将更新记录在change buffer，语句执行就结束了。 将数据从磁盘读入内存涉及随机IO的访问，是数据库里面成本最高的操作之一。change buffer因为减少了随机磁盘访问,所以对更新性能的提升是会很明显的。 对于写多读少的业务来说，页面在写完以后马上被访问到的概率比较小，此时change buffer的使用效果最好 假设一个业务的更新模式是写入之后马上会做查询，那么即使满足了条件，将更新先记录在change buffer，但之后由于马上要访问这个数据页，会立即触发merge过程。这样随机访问IO的次数不会减少，反而增加了change buffer的维护代价 所以：如果所有的更新后面，都马上伴随着对这个记录的查询，那么你应该关闭change buffer。普通索引和change buffer的配合使用，对于数据量大的表的更新优化还是很明显。 change buffer 与redo log Page 1在内存中，直接更新内存； Page 2没有在内存中，就在内存的change buffer区域，记录下“我要往Page 2插入一行”这个信息 将上述两个动作记入redo log中 redo log主要节省的是随机写磁盘的I0消耗(转成顺序写) , 而change buffer主要节省的则是随机读磁盘的IO消耗。 ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:5","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"总结 磁盘太慢，用内存作为缓存很有必要。 Buffer Pool本质上是InnoDB向操作系统申请的一段连续的内存空间，可以通过innodb_buffer_pool_size来调整它的大小。 Buffer Pool向操作系统申请的连续内存由控制块和缓存页组成，每个控制块和缓存页都是一一对应的，在填充足够多的控制块和缓存页的组合后，BufferPool剩余的空间可能产生不够填充一组控制块和缓存页，这部分空间不能被使用，也被称为碎片。 InnoDB使用了许多链表来管理Buffer Pool。 free链表中每一个节点都代表一个空闲的缓存页，在将磁盘中的页加载到Buffer Pool时，会从free链表中寻找空闲的缓存页。 为了快速定位某个页是否被加载到Buffer Pool，使用表空间号 + 页号作为key，缓存页作为value，建立哈希表。 在Buffer Pool中被修改的页称为脏页，脏页并不是立即刷新，而是被加入到flush链表中，待之后的某个时刻同步到磁盘上。 LRU链表分为young和old两个区域，可以通过innodb_old_blocks_pct来调节old区域所占的比例。首次从磁盘上加载到Buffer Pool的页会被放到old区域的头部，在innodb_old_blocks_time间隔时间内访问该页不会把它移动到young区域头部。在Buffer Pool没有可用的空闲缓存页时，会首先淘汰掉old区域的一些页。 young 和old 分别对应热、冷 我们可以通过指定innodb_buffer_pool_instances来控制Buffer Pool实例的个数，每个Buffer Pool实例中都有各自独立的链表，互不干扰。 自MySQL 5.7.5版本之后，可以在服务器运行过程中调整Buffer Pool大小。每个Buffer Pool实例由若干个chunk组成，每个chunk的大小可以在服务器启动时通过启动参数调整。 可以用下边的命令查看Buffer Pool的状态信息(命中情况、死锁等等)：SHOW ENGINE INNODB STATUS ","date":"2021-01-20","objectID":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/:1:6","tags":["MySQL","Buffer Pool"],"title":"MySQL-Buffer Pool 学习","uri":"/mysql-buffer-pool-%E5%AD%A6%E4%B9%A0-copy/"},{"categories":["MySQL性能调优"],"content":"执行计划各列详解 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:0","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"id 在连接查询的执行计划中，每个表都会对应一条记录，这些记录的id列的值是相同的，出现在前边的表表示驱动表，出现在后边的表表示被驱动表，每出现一个SELECT关键字，就为它分配一个唯一的id值 比如子查询 可能id就不想同，但查询优化器可能对涉及子查询的查询语句进行重写，从而转换为连接查询，id就相同了。 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:1","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"select_type MySQL为每一个SELECT关键字代表的小查询都定义了一个称之为select_type的属性，意思是我们只要知道了某个小查询的select_type属性，就知道了这个小查询在整个大查询中扮演了一个什么角色。 SIMPLE 查询语句中不包含UNION或者子查询的查询都算作是SIMPLE类型、当然，连接查询也算是SIMPLE类型 PRIMARY 对于包含UNION、UNION ALL或者子查询的大查询来说，它是由几个小查询组成的，其中最左边的那个查询的select_type值就是PRIMARY EXPLAIN SELECT * FROM info_flow UNION SELECT * FROM info_flow UNION 对于包含UNION或者UNION ALL的大查询来说，它是由几个小查询组成的，其中除了最左边的那个小查询以外，其余的小查询的select_type值就是UNION UNION RESULT MySQL选择使用临时表来完成UNION查询的去重工作，针对该临时表的查询的select_type就是UNION RESULT SUBQUERY 物化：在SQL执行过程中，第一次需要子查询结果时执行子查询并将子查询的结果保存为临时表，后续对子查询结果集的访问将直接通过临时表获得 semi-join：只需要满足匹配一张表。半连接 如果包含子查询的查询语句不能够转为对应的semi-join的形式,一般在in语句带子查询时。并且该子查询是不相关子查询, 并且查询优化器决定采用将该子查询物化的方案来执行该子查询时。子查询由于会被物化，所以只需要执行一遍 EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2) OR key3 = 'a'; DEPENDENT SUBQUERY 如果包含子查询的查询语句不能够转为对应的semi-join的形式，并且该子查询是相关子查询，则该子查询的第一个SELECT关键字代表的那个查询的select_type就是DEPENDENT SUBQUERY。需要注意这个查询类型没有被物化，查询可能会被执行多次 EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE s1.key2 = s2.key2) OR key3 = 'a'; DEPENDENT UNION 在包含UNION或者UNION ALL的大查询中，如果各个小查询都依赖于外层查询的话，那除了最左边的那个小查询之外，其余的小查询的select_type的值就是DEPENDENT UNION。 EXPLAIN SELECT * FROM s1 WHERE key1 IN (SELECT key1 FROM s2 WHERE key1 = 'a' UNION SELECT key1 FROM s1 WHERE key1 = 'b') 这个查询比较复杂啊，大查询里包含了一个子查询，子查询里又是由UNION连起来的两个小查询。 SELECT key1 FROM s2 WHERE key1 = ‘a’这个小查询由于是子查询中第一个查询，所以它的select_type是DEPENDENT SUBQUERY， 而SELECT key1 FROM s1 WHERE key1 = ‘b’这个查询的select_type就是DEPENDENT UNION DERIVED 对于采用物化的方式执行的包含派生表的查询，该派生表对应的子查询的select_type就是DERIVED EXPLAIN SELECT * FROM (SELECT id, count(*) as count FROM info_flow GROUP BY id) AS derived_s1 where id \u003e 1 从执行计划中可以看出，id为2的记录就代表子查询的执行方式，它的select_type是DERIVED，说明该子查询是以物化的方式执行的。id为1的记录代表外层查询，大家注意看它的table列显示的是，表示该查询是针对将派生表物化之后的表进行查询的。 MATERIALIZED 当查询优化器在执行包含子查询的语句时，选择将子查询物化之后与外层查询进行连接查询时，该子查询对应的select_type属性就是MATERIALIZED EXPLAIN SELECT * FROM info_flow where area_id in(SELECT biz_id FROM biz_dispatch_record) 执行计划的第三条记录的id值为2，说明该条记录对应的是一个单表查询，从它的select_type值为MATERIALIZED可以看出，查询优化器是要把子查询先转换成物化表。然后看执行计划的前两条记录的id值都为1，说明这两条记录对应的表进行连接查询，需要注意的是第二条记录的table列的值是，说明该表其实就是id为2对应的子查询执行之后产生的物化表，然后将s1和该物化表进行连接查询。 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:2","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"table 不论我们的查询语句有多复杂，里边儿包含了多少个表，到最后也是需要对每个表进行单表访问的，所以MySQL规定EXPLAIN语句输出的每条记录都对应着某个单表的访问方法，该条记录的table列代表着该表的表名 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:3","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"partitions 分区 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:4","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"type 主要关注 执行计划的一条记录就代表着MySQL对某个表的执行查询时的访问方法，其中的type列就表明了这个访问方法是个啥 一般来说，这些访问方法按照我们介绍它们的顺序性能依次变差。其中除了All这个访问方法外，其余的访问方法都能用到索引，除了index_merge访问方法外，其余的访问方法都最多只能用到一个索引。 const 当我们根据主键或者唯一二级索引列与常数进行等值匹配时，对单表的访问方法就是const。 EXPLAIN SELECT * FROM s1 WHERE id = 5; eq_ref 在连接查询时，如果被驱动表是通过主键或者唯一二级索引列等值匹配的方式进行访问的（如果该主键或者唯一二级索引是联合索引的话，所有的索引列都必须进行等值比较），则对该被驱动表的访问方法就是eq_ref EXPLAIN SELECT * FROM s1 INNER JOIN s2 ON s1.id = s2.id; ref 当通过普通的二级索引列与常量进行等值匹配时来查询某个表，那么对该表的访问方法就可能是ref SELECT * FROM single_table WHERE key1 = 'abc'; ref_or_null 对普通二级索引进行等值匹配查询，该索引列的值也可以是NULL值时，那么对该表的访问方法就可能是ref_or_null。 EXPLAIN SELECT * FROM s1 WHERE key1 = 'a' OR key1 IS NULL; index_merge 索引合并 unique_subquery unique_subquery是针对在一些包含IN子查询的查询语句中，如果查询优化器决定将IN子查询转换为EXISTS子查询，而且子查询可以使用到主键进行等值匹配的话，那么该子查询执行计划的type列的值就是unique_subquery EXPLAIN SELECT * FROM s1 WHERE key2 IN (SELECT id FROM s2 where s1.key1 = s2.key1) OR key3 = 'a'; index_subquery index_subquery与unique_subquery类似，只不过访问子查询中的表时使用的是普通的索引 EXPLAIN SELECT * FROM s1 WHERE common_field IN (SELECT key3 FROM s2 where s1.key1 = s2.key1) OR key3 = 'a'; range 如果使用索引获取某些范围区间的记录，那么就可能使用到range访问方法 EXPLAIN SELECT * FROM s1 WHERE key1 IN ('a', 'b', 'c'); EXPLAIN SELECT * FROM s1 WHERE key1 \u003e 'a' AND key1 \u003c 'b'; 只要索引列和常数使用=、\u003c=\u003e、IN、NOT IN、IS NULL、IS NOT NULL、\u003e、\u003c、\u003e=、\u003c=、BETWEEN、!=（不等于也可以写成\u003c\u003e）或者LIKE操作符连接起来，就可以产生一个所谓的区间，（LIKE操作符比较特殊，只有在匹配完整字符串或者匹配字符串前缀时才可以利用索引） IN操作符的效果和若干个等值匹配操作符=之间用OR连接起来是一样 未使用索引的情况 SELECT * FROM single_table WHERE key2 \u003e 100 AND common_field = 'abc'; 这个查询语句中能利用的索引只有idx_key2一个，而idx_key2这个二级索引的记录中又不包含common_field这个字段，所以在使用二级索引idx_key2定位记录的阶段用不到common_field = ‘abc’这个条件，这个条件是在回表获取了完整的用户记录后才使用的,而范围区间是为了到索引中取记录中提出的概念，所以在确定范围区间的时候不需要考虑common_field = ‘abc’这个条件。 这也就说说明如果我们强制使用idx_key2执行查询的话，对应的范围区间就是(-∞, +∞)而不是(100, +∞)，也就是需要将全部二级索引的记录进行回表，这个代价肯定比直接全表扫描都大了。所以mysql 很可能会直接扫描聚簇索引。 index 当我们可以使用索引覆盖，但需要扫描全部的索引记录时，也就是要回表时。 EXPLAIN SELECT key_part2 FROM s1 WHERE key_part3 = 'a'; 上述查询中的搜索列表中只有key_part2一个列，而且搜索条件中也只有key_part3一个列，这两个列又恰好包含在idx_key_part这个索引中，可是搜索条件key_part3不能直接使用该索引进行ref或者range方式的访问，只能扫描整个idx_key_part索引的记录。 对于使用InnoDB存储引擎的表来说，二级索引的记录只包含索引列和主键列的值，而聚簇索引中包含用户定义的全部列以及一些隐藏列，所以扫描二级索引的代价比直接全表扫描，也就是扫描聚簇索引的代价更低一些。 all 最直接的查询执行方式就是我们已经提了无数遍的全表扫描，对于InnoDB表来说也就是直接扫描聚簇索引，设计MySQL的大叔把这种使用全表扫描执行查询的方式称之为：all ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:5","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"possible_keys和和key 主要关注 possible_keys列表示在某个查询语句中，对某个表执行单表查询时可能用到的索引有哪些，key列表示实际用到的索引有哪些 EXPLAIN SELECT * FROM s1 WHERE key1 \u003e 'z' AND key3 = 'a'; 一般情况possible_keys列的值是idx_key1,idx_key3。后key列的值是其中一个：优化器会根据数据量 成本来确定用那个索引。 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:6","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"key_len key_len列表示当优化器决定使用某个索引执行查询时，该索引记录的最大长度 对于使用固定长度类型的索引列来说，它实际占用的存储空间的最大长度就是该固定值，对于指定字符集的变长类型的索引列来说，比如某个索引列的类型是VARCHAR(100)，使用的字符集是utf8那么该列实际占用的最大存储空间就是100 × 3 = 300个字节。 如果该索引列可以存储NULL值，则key_len比不可以存储NULL值时多1个字节。 对于变长字段来说，都会有2个字节的空间来存储该变长列的实际长度。 所以一般VARCHAR(100) 可以存储空 就会有303字节 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:7","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"ref 当使用索引列等值匹配的条件去执行查询时，也就是在访问方法是const、eq_ref、ref、ref_or_null、unique_subquery、index_subquery其中之一时，ref 列展示的就是与索引列作等值匹配的东东是个啥。 EXPLAIN SELECT* FROM lms_clue WHERE phone='15183904781' EXPLAIN SELECT* FROM lms_clue s1 INNER JOIN app_user s2 ON s1.phone = s2.phone 显示具体索引匹配字段 EXPLAIN SELECT* FROM lms_clue s1 INNER JOIN app_user s2 ON s1.phone = UPPER(s2.phone) 显示函数func ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:8","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"rows 如果查询优化器决定使用全表扫描的方式对某个表执行查询时，执行计划的rows列就代表预计需要扫描的行数，如果使用索引来执行查询时，执行计划的rows列就代表预计扫描的索引记录行数；但这个是预估的 不一定准。 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:9","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"filtered 如果使用的是全表扫描的方式执行的单表查询，那么计算驱动表扇出时需要估计出满足搜索条件的记录到底有多少条。 如果使用的是索引执行的单表扫描，那么计算驱动表扇出的时候需要估计出满足除使用到对应索引的搜索条件外的其他搜索条件的记录有多少条。 EXPLAIN SELECT* FROM lms_clue s1 WHERE biz_id\u003e100000 and phone \u003e'15183904781' 从执行计划可以看出来，满足biz_id的记录有231284条。执行计划的filtered列就代表查询优化器预测在这266条记录中，有多少条记 录满足其余的搜索条件，也就是phone \u003e'15183904781' 这个条件的百分比。此处filtered列的值是25.00。都是预估值 如果是连表查询 也就是驱动表与被驱动表的匹配条数。 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:10","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Extra 这个里面有很多类型，列举一些比较常见的。 No tables used : 顾名思义 没有表 Impossible WHERE：查询语句的WHERE子句永远为FALSE时将会提示 No matching min/max row：当查询列表处有MIN或者MAX聚集函数，但是并没有符合WHERE子句中的搜索条件的记录时 EXPLAIN SELECT MIN(key1) FROM s1 WHERE key1 = ‘abcdefg’; Using index：当我们的查询列表以及搜索条件中只包含属于某个索引的列，也就是在可以使用索引覆盖的情况下 Using index condition: 其实就是索引条件下推-在索引那篇文章学习过 我们说回表操作其实是一个随机IO，比较耗时，所以上述修改虽然只改进了一点点，但是可以省去好多回表操作的成本。设计MySQL的大叔们把他们的这个改进称之为索引条件下推（英文名：IndexCondition Pushdown）。 Using where: 当我们使用全表扫描来执行对某个表的查询，并且该语句的WHERE子句中有针对该表的搜索条件时 Using join buffer (Block Nested Loop)：在连接查询执行过程中，当被驱动表不能有效的利用索引加快访问速度，MySQL一般会为其分配一块名叫join buffer的内存块来加快查询速度，也就是我们所讲的基于块的嵌套循环算法–在连接查询详解文章里面有写。 Not exists: 连接查询时发现where条件与on条件不匹配 Using intersect(…)、Using union(…)和Using sort_union(…):使用了索引合并 Zero limit: limit 0 Using filesort：排序操作无法在索引上，只能采用内存或者磁盘排序的时候，如果数据量大最好还是优化为按索引顺序排序。、 Using temporary：在许多查询的执行过程中，MySQL可能会借助临时表来完成一些功能，比如去重、排序之类的，比如我们在执行许多包含DISTINCT、GROUP BY、UNION等子句的查询过程中，如果不能有效利用索引来完成查询，MySQL很有可能寻求通过建立内部的临时表来执行查询。 如果临时表分组了,mysql会默认加上orderby来排序。意味着：你查询某个没索引的字段，并且你要对这个字段分组。那么mysql 会建立临时表并且对记录进行分组排序。如果不想排序。我们需要显式指定order by null 来减少Using filesort这个成本 Start temporary, End temporary: 查询优化器会优先尝试将IN子查询转换成semi-join，而semi-join又有好多种执行策略，当执行策略为DuplicateWeedout时，也就是通过建立临时表来实现为外层查询中的记录进行去重操作时，驱动表查询执行计划的Extra列将显示Start temporary提示，被驱动表查询执行计划的Extra列将显示End temporary提示 LooseScan：在将In子查询转为semi-join时，如果采用的是LooseScan执行策略 FirstMatch(tbl_name)：在将In子查询转为semi-join时，如果采用的是FirstMatch执行策略 semi-join 具体看连接查询篇 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:1:11","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"查看额外执行计划信息 在我们使用EXPLAIN语句查看了某个查询的执行计划后，紧接着还可以使用SHOW WARNINGS语句查看与这个查询的执行计划有关的一些扩展信息 EXPLAIN SELECT* FROM lms_clue s1 INNER JOIN app_user s2 ON s1.phone = s2.phone; show WARNINGS; 分别是Level、Code、Message。我们最常见的就是Code为1003的信息，当Code值为1003时，Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句。 Message字段展示的信息类似于查询优化器将我们的查询语句重写后的语句，并不是等价于，也就是说Message字段展示的信息并不是标准的查询语句，在很多情况下并不能直接拿到黑框框中运行，它只能作为帮助我们理解查MySQL将如何执行查询语句的一个参考依据而已 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:2:0","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"查看执行计划优化过程 如果要看具体执行计划为什么选择某个索引，我们需要通过optimizer_trace来查看。 默认optimizer_trace 这个是关闭的。 SET optimizer_trace=\"enabled=on\"; SELECT * FROM s1 WHERE key1 \u003e 'z' AND key2 \u003c 1000000 AND key3 IN ('a', 'b', 'c') AND common_field = 'abc'; SELECT * FROM information_schema.OPTIMIZER_TRACE\\G ## 当你停止查看语句的优化过程时，把optimizer trace功能关闭 SET optimizer_trace=\"enabled=off\" information_schema数据库下的OPTIMIZER_TRACE表字段有4个 QUERY：表示我们的查询语句。 TRACE：表示优化过程的JSON格式文本。 MISSING_BYTES_BEYOND_MAX_MEM_SIZE：由于优化过程可能会输出很多，如果超过某个限制时，多余的文本将不会被显示，这个字段展示了被忽略的文本字节数。 INSUFFICIENT_PRIVILEGES：表示是否没有权限查看优化过程，默认值是0，只有某些特殊情况下才会是1 优化过程大致分为了三个阶段：prepare阶段、optimize阶段、execute阶段 我们所说的基于成本的优化主要集中在optimize阶段。 对于单表查询来说，我们主要关注optimize阶段的\"rows_estimation\"这个过程 对于多表连接查询来说，我们更多需要关注\"considered_execution_plans\"这个过程 ","date":"2021-01-20","objectID":"/mysql-explain%E5%AD%A6%E4%B9%A0/:3:0","tags":["MySQL","Explain"],"title":"MySQL-Explain学习","uri":"/mysql-explain%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"优化器未使用你想用的索引 select * from t force index(a) where a between 10000 and 20000; // 利用参数force index 前缀索引 直接创建完整索引，这样可能比较占用空间； 创建前缀索引，节省空间，但会增加查询扫描次数，并且不能使用覆盖索引； count区别 MyISAM索引结构的优势count快就不说了而Innodb由于MVCC与可重复读的原因速度会相对慢点 count(*)、count(主键id)和count(1) 都表示返回满足条件的结果集的总行数；而count(字段），则表示返回满足条件的数据行里面，参数“字段”不为NULL的总个数 对于count(主键id)来说，InnoDB引擎会遍历整张表，把每一行的id值都取出来，返回给server层。server层拿到id后，判断是不可能为空的，就按行累加。 对于count(1)来说，InnoDB引擎遍历整张表，但不取值。server层对于返回的每一行，放一个数字“1”进去，判断是不可能为空的，按行累加。 单看这两个用法的差别的话，你能对比出来，count(1)执行得要比count(主键id)快。因为从引擎返回id会涉及到解析数据行，以及拷贝字段值的操作。 对于count(字段)来说： 如果这个“字段”是定义为not null的话，一行行地从记录里面读出这个字段，判断不能为null，按行累加； 如果这个“字段”定义允许为null，那么执行的时候，判断到有可能是null，还要把值取出来再判断一下，不是null才累加。 但是count(*)是例外，并不会把全部字段取出来，而是专门做了优化，不取值。count(*)肯定不是null，按行累加 所以结论是：按照效率排序的话，count(字段)\u003ccount(主键id)\u003ccount(1)≈count(*)，所以我建议你，尽量使用count(*) Order By 排序流程 内存排序 当Extra这个字段出现Using filesort表示的就是需要排序。 select city,name,age from t where city='杭州' order by name limit 1000 ; 通常情况下，这个语句执行流程如下所示 ： 1. 初始化sort_buffer，确定放入name、city、age这三个字段； 2. 从索引city找到第一个满足city='杭州’条件的主键id，也就是图中的ID_X； 3. 到主键id索引取出整行，取name、city、age三个字段的值，存入sort_buffer中； 4. 从索引city取下一个记录的主键id； 5. 重复步骤3、4直到city的值不满足查询条件为止。 6. 对sort_buffer中的数据按照字段name做快速排序； 7. 按照排序结果取前1000行返回给客户端。 按name排序”可能在内存中完成，当参数sort_buffer_size内存不够时，会使用临时文件辅助排序。 max_length_for_sort_data 是MySQL中专门控制用于排序的行数据的长度的一个参数。它的意思是，如果单行的长度超过这个值，MySQL就认为单行太大，要换一个算法。city、name、age 这三个字段的定义总长度是36 新的算法放入sort_buffer的字段，只有要排序的列（即name字段）和主键id。 但这时，排序的结果就因为少了city和age字段的值，不能直接返回了，整个执行流程就变成如下所示的样子： 1. 初始化sort_buffer，确定放入两个字段，即name和id； 2. 从索引city找到第一个满足city='杭州’条件的主键id； 3. 到主键id索引取出整行，取name、id这两个字段，存入sort_buffer中； 4. 从索引city取下一个记录的主键id； 5. 重复步骤3、4直到不满足city='杭州’条件为止。； 6. 对sort_buffer中的数据按照字段name进行排序； 7. 遍历排序结果，取前1000行，并按照id的值回到原表中取出city、name和age三个字段返回给客户端。 联合索引排序 并不是所有的order by语句，都需要排序操作的。从上面分析的执行过程，我们可以看到，MySQL之所以需要生成临时表，并且在临时表上做排序操作，其原因都是无序的，如果我们建立联合索引，保证天然有序就好了 1. 从索引(city,name)找到第一个满足city='杭州’条件的主键id； 2. 到主键id索引取出整行，取name、city、age三个字段的值，作为结果集的一部分直接返回； 3. 从索引(city,name)取下一个记录主键id； 4. 重复步骤2、3，直到查到第1000条记录，或者是不满足city='杭州’条件时循环结束。 Group By 分组优化总结 Using index，表示这个语句使用了覆盖索引，选择了索引a，不需要回表 Using temporary，表示使用了临时表； Using filesort，表示需要排序。 “order by null”和“不加order by”不等价 这个语句的执行流程是这样的： 1. 创建内存临时表，表里有两个字段m和c，主键是m； 2. 扫描表t1的索引a，依次取出叶子节点上的id值，计算id%10的结果，记为x 1. 如果临时表中没有主键为x的行，就插入一个记录(x,1); 2. 如果表中有主键为x的行，就将x这一行的c值加1； 3. 遍历完成后，再根据字段m做排序，得到结果集返回给客户端。 如果对group by语句的结果没有排序要求，要在语句后面加 order by null； 尽量让group by过程用上表的索引，确认方法是explain结果里没有Using temporary 和 Using filesort；能用索引肯定更好 如果group by需要统计的数据量不大，尽量只使用内存临时表；也可以通过适当调大tmp_table_size临时表大小参数，来避免用 到磁盘临时表；(group by语句中需要放到临时表上的数据量特别大，就会“先放到内存临时表，插入一部分数据后，发现内存临时表不够用了再转成磁盘临时表”) 如果数据量实在太大，使用SQL_BIG_RESULT这个提示直接使用磁盘临时表，少去内存转临时的过程，来告诉优化器直接使用排序算法得到group by的结果。 select SQL_BIG_RESULT id%100 as m, count(*) as c from t1 group by m; 其实DISTINCT 跟group by 逻辑基本一样。也是采用临时表 创建一个临时表，临时表有一个字段a，并且在这个字段a上创建一个唯一索引； 遍历表t，依次取数据插入临时表中： 如果发现唯一键冲突，就跳过； 否则插入成功； 遍历完成后，将临时表作为结果集返回给客户端。 隐式转换 1.查询的时候注意字段隐式转换 select * from tradelog where tradeid=110717; 交易编号tradeid这个字段上，本来就有索引，但是explain的结果却显示，这条语句需要走全表扫描。 tradeid的字段类型是varchar(32)，而输入的参数却是整型，所以需要做类型转换。 2.连接查询时主要表与表之前的编码问题。 大表扫描200G数据问题 server层采用的是边读边发的操作。 1. 获取一行，写到net_buffer中。这块内存的大小是由参数`net_buffer_length`定义的，默认是16k。 2. 重复获取行，直到net_buffer写满，调用网络接口发出去。 3. 如果发送成功，就清空net_buffer，然后继续取下一行，并写入net_buffer。 4. 如果发送函数返回EAGAIN或WSAEWOULDBLOCK，表示本地（socket send buffer）写满了,进入等待。直到网络栈重新可写，再继续发送。 一个查询在发送过程中，占用的MySQL内部的内存最大就是net_buffer_length这么大，并不会达到200G； socket send buffer 也不可能达到200G（默认定义/proc/sys/net/core/wmem_default），如果socket send buffer被写 满，就会暂停读数据的流程。 InnoDB引擎处理 由于冷热数据链的原因，大表扫描并不会影响young区域，只会占用old区域。 由于MySQL采用的是边算边发的逻辑，因此对于数据量很大的查询结果来说，不会在server端保存完整的结果集。所以， 如果客户端读结果不及时，会堵住MySQL的查询过程，但是不会把内存打爆。 而对于InnoDB引擎内部，由于有淘汰策略，大查询也不会导致内存暴涨。并且，由于InnoDB对LRU算法做了改进，冷数 据的全表扫描，对Buffer Pool的影响也能做到可控。 当然，全表扫描还是比较耗费IO资源的，所以业务高峰期还是不能直接在线上主库执行全表扫描。 刷脏抖动问题 刷redolog日志到磁盘导致抖动问题 由于刷盘时在执行sql 会导致sql执行时间偶尔变长， 优化mysql 的iops 参数：innodb_io_capacity:它会告诉InnoDB你的磁盘能力。这个值我建议你设置成磁盘的IOPS。磁盘的IOPS可以通过fio这个工具来测试 如果是SSD同时设置innodb_flush_neighbors为0，让他每次别刷临近缓存页，减少刷缓存页的数量，这样就可","date":"2021-01-20","objectID":"/mysql-%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/:0:0","tags":["MySQL","group by","order by"],"title":"MySQL-一些问题","uri":"/mysql-%E4%B8%80%E4%BA%9B%E9%97%AE%E9%A2%98/"},{"categories":["MySQL性能调优"],"content":"多事务并发处理数据产生的问题 脏读脏写 都是因为读取了事务未提交时的事务回滚情况 脏写 事务A与事务B同时更新一条数据，由于其中一个事务回滚 导致另外事务执行成功的数据不存在了。 脏读 事务A还没提交事务时事务B拿到值做了大量的操作，事务A回滚后出现的数据异常问题。 不可重复读 与可重复读 事务A修改了值并且提交了事务 B读取到 然后B在执行事务期间 C又修改了值并且提交了。这时B读取到的值又变了。 这就时不可重复读问题。 可重复读就相反 并且读取到的值是相同的。 幻读 其实就是在事务执行期间第二次查询相比第一次查询出现了之前没出现过的数据。 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:1:0","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"事务的4种隔离级别 读未提交RU 可以读未提交 但是不允许脏写 READ UNCOMMITTED 读已提交RC 人家事务没提交的情况下修改的值，你是绝对读不到的 不会发生脏读脏写情况 READ COMMITTED 可重复读RR 不会生脏写、脏读和不可重复读 但是可能发生幻读 REPEATABLE READ 串行化 SERIALIZABLE SET GLOBAL TRANSACTION ISOLATION LEVEL SERIALIZABLE; 全局 SET SESSION TRANSACTION ISOLATION LEVEL SERIALIZABLE; 会话 MySQL默认都是 RR 可重复读级别 并且通过mvcc机制也不会发生幻读的情况。 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:1:1","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"MVCC 多版本并发控制机制 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:2:0","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"版本链 trx_id：每次一个事务对某条聚簇索引记录进行改动时，都会把该事务的事务id赋值给trx_id隐藏列。 roll_pointer：每次对某条聚簇索引记录进行改动时，都会把旧的版本写入到undo日志中，然后这个隐藏列就相当于一个指针，可以通过它来找到该记录修改前的信息。 实际上insert undo只在事务回滚时起作用，当事务提交后，该类型的undo日志就没用了，它占用的Undo Log Segment也会被系统回收（也就是该undo日志占用的Undo页面链表要么被重用，要么被释放）。 版本链的头节点就是当前记录最新的值 ,另外，每个版本中还包含生成该版本时对应的事务id，这个信息很重要 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:2:1","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"ReadView 每次执行一次事务会产生一个ReadView视图 一个是m_ids，这个就是说此时有哪些事务在MySQL里执行还没提交的； 一个是min_trx_id，就是m_ids里最小的值； 一个是max_trx_id，这是说mysql下一个要生成的事务id，就是最大事务id；并不一定是m_ids 最大的事务id 事务id是递增分配的。比方说现在有id为1，2，3这三个事务，之后id为3的事务提交了。那么一个新的读事务在生成ReadView时，m_ids就包 括1和2，min_trx_id的值就是1，max_trx_id的值就是4。 一个是creator_trx_id，就是你这个事务的id。 如果被访问版本的trx_id属性值与ReadView中的creator_trx_id值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值小于ReadView中的min_trx_id值，表明生成该版本的事务在当前事务生成ReadView前已经提交，所以该版本可以被当前事务访问。 如果被访问版本的trx_id属性值大于ReadView中的max_trx_id值，表明生成该版本的事务在当前事务生成ReadView后才开启，所以该版本不可以被当前事务访问。 如果被访问版本的trx_id属性值在ReadView的min_trx_id和max_trx_id之间，那就需要判断一下trx_id属性值是不是在m_ids列表中，如果在，说明创建ReadView时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建ReadView时生成该版本的事务已经被提交，该版本可以被访问。 简单来说MySQL在每次查询的时候会判断数据的trxid是否小于等于自己，如果小于等于自己就可以查询出来。 如果不等于 会基于这条数据去找undolog链条找指向的小于等于我当前事务id的这条数据的版本出来。 期间判断如果数据的事务id直接大于我当前视图最大事务id，会直接忽略。 事务id 只有在提交事务后才会到数据上面。 RR 机制也就是判断m_ids中如果查询出来的数据事务版本号存在这里面说明 在开启视图的时候这些事务就存在了还没提交，我不能查询出来这也就解决了不可重复度问题，由于新插入进来的数据版本号必定大于我当前视图最大id，也查询不到，也就解决了幻读问题。 RC 的机制是可以读取别人已经提交的事务数据 就是根据 判断事务id是否存在于m_ids中，存在就不查询出来，不存在说明可以查询出来 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:2:2","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"小结 ​ 所谓的MVCC（Multi-Version Concurrency Control ，多版本并发控制）指的就是在使用READ COMMITTD、REPEATABLE READ这两种隔离级别的事务在执行普通的SEELCT操作时访问记录的版本链的过程，这样子可以使不同事务的读-写、写-读操作并发执行，从而提升系统性能。 ​ READ COMMITTD、REPEATABLEREAD这两个隔离级别的一个很大不同就是：生成ReadView的时机不同，READ COMMITTD在每一次进行普通SELECT操作前都会生成一个ReadView，而REPEATABLEREAD只在第一次进行普通SELECT操作前生成一个ReadView，之后的查询操作都重复使用这个ReadView就好了。 ​ 之前说执行DELETE语句或者更新主键的UPDATE语句并不会立即把对应的记录完全从页面中删除，而是执行一个所谓的delete mark操作，相当于只是对记录打上了一个删除标志位，这主要就是为MVCC服务。 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:2:3","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"关于purge ​ 我们说insert undo在事务提交之后就可以被释放掉了，而update undo由于还需要支持MVCC，不能立即删除掉。为了支持MVCC，对于delete mark操作来说，仅仅是在记录上打一个删除标记，并没有真正将它删除掉。随着系统的运行，在确定系统中包含最早产生的那个ReadView的事务不会再访问某些update undo日志以及被打了删除标记的记录后，有一个后台运行的purge线程会把它们真正的删除掉。 ","date":"2021-01-20","objectID":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/:2:4","tags":["MySQL","MVCC","事务","ReadView"],"title":"MySQL-事务学习","uri":"/mysql-%E4%BA%8B%E5%8A%A1%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":" MySQL执行一个查询可以有不同的执行方案，它会选择其中成本最低，或者说代价最低的那种方案去真正的执行查询 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:0:0","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"基于成本的优化步骤 根据搜索条件，找出所有可能使用的索引 计算全表扫描的代价 计算使用不同索引执行查询的代价 对比各种执行方案的代价，找出成本最低的那一个 CREATE TABLE single_table ( id INT NOT NULL AUTO_INCREMENT, key1 VARCHAR(100), key2 INT, key3 VARCHAR(100), key_part1 VARCHAR(100), key_part2 VARCHAR(100), key_part3 VARCHAR(100), common_field VARCHAR(100), PRIMARY KEY (id), KEY idx_key1 (key1), UNIQUE KEY idx_key2 (key2), KEY idx_key3 (key3), KEY idx_key_part(key_part1, key_part2, key_part3) ) Engine=InnoDB CHARSET=utf8; ## 执行sql语句 SELECT * FROM single_table WHERE key1 IN ('a', 'b', 'c') AND key2 \u003e 10 AND key2 \u003c 1000 AND key3 \u003e key2 AND key_part1 LIKE '%hello%' AND common_field = '123'; ## possible keys只有idx_key1和idx_key2。 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:1:0","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"计算成本 SHOW TABLE STATUS LIKE 'single_table' ## 查看表的统计信息 Rows: 记录条数 #估算值 Data_length：Data_length = 聚簇索引的页面数量 x 16k(页面大小) #聚簇索引空间大小 I/O成本 聚簇索引的页面数量 = 1589248 ÷ 16 ÷ 1024 = 97 97 x 1.0 + 1.1 = 98.1 97指的是聚簇索引占用的页面数，1.0指的是加载一个页面的成本常数，后边的1.1是一个微调值 CPU成本： 9693 x 0.2 + 1.0 = 1939.6 9693指的是统计数据中表的记录数，对于InnoDB存储引擎来说是一个估计值，0.2指的是访问一条记录所需的成本常数，后边的1.0是一个微调值。 I/O +CPU=总成本 计算索引执行的成本 MySQL查询优化器先分析使用唯一二级索引的成本，再分析使用普通索引的成本，所以我们也先分析idx_key2的成本，然后再看使用idx_key1的成本。 idx_key2 idx_key2对应的搜索条件是：key2 \u003e 10 AND key2 \u003c 1000，也就是说对应的范围区间就是：(10, 1000)，使用idx_key2搜索的示意图就是这样子 范围区间数量 不论某个范围区间的二级索引到底占用了多少页面，查询优化器粗暴的认为读取索引的一个范围区间的I/O成本和读取一个页面是相同的。所以这个二级索引付出的I/O成本就是：1 x 1.0 = 1.0 需要回表的记录数 先根据key2 \u003e 10这个条件访问一下idx_key2对应的B+树索引，找到满足key2 \u003e 10这个条件的第一条记录，我们把这条记录称之为区间最左记录。我们前头说过在B+数树中定位一条记录的过程是贼快的。 然后再根据key2 \u003c 1000这个条件继续从idx_key2对应的B+树索引中找出第一条满足这个条件的记录，我们把这条记录称之为区间最右记录。 步骤3：如果区间最左记录和区间最右记录相隔不太远（在MySQL 5.7.21这个版本里，只要相隔不大于10个页面即可），那就可以精确统计出满足key2 \u003e 10 AND key2 \u003c 1000条件的二级索引记录条数。否则只沿着区间最左记录向右读10个页面，计算平均每个页面中包含多少记录，然后用这个平均值乘以区间最左记录和区间最右记录之间的页面数量就可以了 b+tree 每一个目录项都对应一个数据页，定位到目录项时就知道两个页中相差多少数据页了。相当于计算它们父节点的目录项记录之间隔着几条记录 I/O成本： 1.0 + 95 x 1.0 = 96.0 (范围区间的数量 + 预估的二级索引记录条数) CPU成本： 95 x 0.2 + 0.01 + 95 x 0.2 = 38.01 （读取二级索引记录的成本 + 读取并检测回表后聚簇索引记录的成本） idx_key1 idx_key1对应的搜索条件是：key1 IN (‘a’, ‘b’, ‘c’)，也就是说相当于3个单点区间。 这三个单点区间总共需要回表的记录数就是：35 + 44 + 39 = 118 I/O成本： 3.0 + 118 x 1.0 = 121.0 (范围区间的数量 + 预估的二级索引记录条数) CPU成本： 118 x 0.2 + 0.01 + 118 x 0.2 = 47.21 （读取二级索引记录的成本 + 读取并检测回表后聚簇索引记录的成本） 是否会使用索引合并 本例中有关key1和key2的搜索条件是使用AND连接起来的，而对于idx_key1和idx_key2都是范围查询，也就是说查找到的二级索引记录并不是按照主键值进行排序的，并不满足使用Intersection索引合并的条件，所以并不会使用索引合并 全表扫描的成本：2037.7 使用idx_key2的成本：134.01 选择 使用idx_key1的成本：168.21 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:1:1","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"基于索引统计数据的成本计算 查看索引统计数据SHOW INDEX FROM single_table Table 索引所属表的名称。 Non_unique 索引列的值是否是唯一的，聚簇索引和唯一二级索引的该列值为0，普通二级索引该列值为1。 Key_name 索引的名称。 Seq_in_index 索引列在索引中的位置，从1开始计数，联合索引顺序 Column_name 索引列的名称。 Collation 索引列中的值是按照何种排序方式存放的，值为A时代表升序存放，为NULL时代表降序存放。 Cardinality 索引列中不重复值的数量。后边我们会重点看这个属性的。 Sub_part 对于存储字符串或者字节串的列来说，有时候我们只想对这些串的前n个字符或字节建立索引，这个属性表示的就是那个n值。如果对完整的列建立索引的话，该属性的值就是NULL。 Packed 索引列如何被压缩，NULL值表示未被压缩。这个属性我们暂时不了解，可以先忽略掉。 Null 该索引列是否允许存储NULL值。 Index_type 使用索引的类型，我们最常见的就是BTREE，其实也就是B+树索引。 Comment 索引列注释信息。 Index_comment 索引注释信息。 有时候使用索引执行查询时会有许多单点区间 SELECT * FROM single_table WHERE key1 IN ('aa1', 'aa2', 'aa3', ... , 'zzz') 很显然，这个查询可能使用到的索引就是idx_key1，由于这个索引并不是唯一二级索引，所以并不能确定一个单点区间对应的二级索引记录的条数有多少，需要我们去计算。计算方式跟上边一样，就是先获取索引对应的B+树的区间最左记录和区间最右记录，然后再计算这两条记录之间有多少记录（记录条数少的时候可以做到精确计算，多的时候只能估算） 参数SHOW VARIABLES LIKE '%dive%'; eq_range_index_dive_limit 代表in最大的参数，如果大于200就会根据索引的估值进行估算进行成本分析了。 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:1:2","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"两表连接查询成本分析 连接查询总成本 = 单次访问驱动表的成本 + 驱动表扇出数 x 单次访问被驱动表的成本 外连接 分别为驱动表和被驱动表选择成本最低的访问方法。 内连接 不同的表作为驱动表最终的查询成本可能是不同的，也就是需要考虑最优的表连接顺序。 然后分别为驱动表和被驱动表选择成本最低的访问方法。 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:1:3","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"执行计划查看成本 EXPLAIN FORMAT=JSON SELECT * FROM s1 INNER JOIN s2 ON s1.key1 = s2.key2 WHERE s1.common_field = 'a' EXPLAIN: { \"query_block\": { \"select_id\": 1, # 整个查询语句只有1个SELECT关键字，该关键字对应的id号为1 \"cost_info\": { \"query_cost\": \"3197.16\" # 整个查询的执行成本预计为3197.16 }, \"nested_loop\": [ # 几个表之间采用嵌套循环连接算法执行 # 以下是参与嵌套循环连接算法的各个表的信息 { \"table\": { \"table_name\": \"s1\", # s1表是驱动表 \"access_type\": \"ALL\", # 访问方法为ALL，意味着使用全表扫描访问 \"possible_keys\": [ # 可能使用的索引 \"idx_key1\" ], \"rows_examined_per_scan\": 9688, # 查询一次s1表大致需要扫描9688条记录 \"rows_produced_per_join\": 968, # 驱动表s1的扇出是968 \"filtered\": \"10.00\", # condition filtering代表的百分比 \"cost_info\": { \"read_cost\": \"1840.84\", # 稍后解释 \"eval_cost\": \"193.76\", # 稍后解释 \"prefix_cost\": \"2034.60\", # 单次查询s1表总共的成本 \"data_read_per_join\": \"1M\" # 读取的数据量 }, \"used_columns\": [ # 执行查询中涉及到的列 \"id\", \"key1\", \"key2\", \"key3\", \"key_part1\", \"key_part2\", \"key_part3\", \"common_field\" ], # 对s1表访问时针对单表查询的条件 \"attached_condition\": \"((`xiaohaizi`.`s1`.`common_field` = 'a') and (`xiaohaizi`.`s1`.`key1` is not null))\" } }, { \"table\": { \"table_name\": \"s2\", # s2表是被驱动表 \"access_type\": \"ref\", # 访问方法为ref，意味着使用索引等值匹配的方式访问 \"possible_keys\": [ # 可能使用的索引 \"idx_key2\" ], \"key\": \"idx_key2\", # 实际使用的索引 \"used_key_parts\": [ # 使用到的索引列 \"key2\" ], \"key_length\": \"5\", # key_len \"ref\": [ # 与key2列进行等值匹配的对象 \"xiaohaizi.s1.key1\" ], \"rows_examined_per_scan\": 1, # 查询一次s2表大致需要扫描1条记录 \"rows_produced_per_join\": 968, # 被驱动表s2的扇出是968（由于后边没有多余的表进行连接，所以这个值也没啥用） \"filtered\": \"100.00\", # condition filtering代表的百分比 # s2表使用索引进行查询的搜索条件 \"index_condition\": \"(`xiaohaizi`.`s1`.`key1` = `xiaohaizi`.`s2`.`key2`)\", \"cost_info\": { \"read_cost\": \"968.80\", # 稍后解释 \"eval_cost\": \"193.76\", # 稍后解释 \"prefix_cost\": \"3197.16\", # 单次查询s1、多次查询s2表总共的成本 \"data_read_per_join\": \"1M\" # 读取的数据量 }, \"used_columns\": [ # 执行查询中涉及到的列 \"id\", \"key1\", \"key2\", \"key3\", \"key_part1\", \"key_part2\", \"key_part3\", \"common_field\" ] } } ] } } ## 两个表分别成本 \"cost_info\": { \"read_cost\": \"1840.84\", \"eval_cost\": \"193.76\", \"prefix_cost\": \"2034.60\", \"data_read_per_join\": \"1M\" } \"cost_info\": { \"read_cost\": \"968.80\", \"eval_cost\": \"193.76\", \"prefix_cost\": \"3197.16\", \"data_read_per_join\": \"1M\" } 由于s2表是被驱动表，所以可能被读取多次，这里的read_cost和eval_cost是访问多次s2表后累加起来的值 单次查询s1表和多次查询s2表后的成本的和968.80 + 193.76 + 2034.60 = 3197.16 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:1:4","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"统计数据怎么收集的 innodb_stats_persistent 参数是代表统计信息是否存储到磁盘。在MySQL 5.6.6之后默认打开的 或者我们直接在创建表或修改表时设置STATS_PERSISTENT属性的值为0，那么该表的统计数据就是非永久性的了 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:2:0","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"基于磁盘的永久性统计数据 SHOW TABLES FROM mysql LIKE 'innodb%'; SELECT * FROM mysql.innodb_table_stats; 查询 innodb_table_stats表存储了关于表的统计数据，每一条记录对应着一个表的统计数据 innodb_index_stats表存储了关于索引的统计数据，每一条记录对应着一个索引的一个统计项的统计数据 innodb_table_stats: database_name 数据库名、table_name 表名、last_update 本条记录最后更新时间、n_rows 表中记录的条数 clustered_index_size 表的聚簇索引占用的页面数量、sum_of_other_index_sizes 表的其他索引占用的页面数量 n_rows估算值：可以理解为按照一定算法（并不是纯粹随机的）选取几个叶子节点页面，计算每个页面中主键值记录数量，然后计算平均一个页面中主键值的记录数量乘以全部叶子节点的数量就算是该表的n_rows值。 innodb_index_stats: database_name 数据库名、table_name 表名、index_name 索引名、last_update 本条记录最后更新时间 stat_name 统计项的名称、stat_value 对应的统计项的值、sample_size 为生成统计数据而采样的页面数量、 stat_description 对应的统计项的描述 针对index_name列相同的记录，stat_name表示针对该索引的统计项名称，stat_value展示的是该索引在该统计项上的值，stat_description指的是来描述该统计项的含义的 n_leaf_pages：表示该索引的叶子节点占用多少页面。 size：表示该索引共占用多少页面。 n_diff_pfxNN：表示对应的索引列不重复的值有多少。 n_diff_pfx01表示的是统计key_part1这单单一个列不重复的值有多少。 n_diff_pfx02表示的是统计key_part1、key_part2这两个列组合起来不重复的值有多少。以此类推 更新统计数据 参数innodb_stats_auto_recalc 默认是开启的。 如果发生变动的记录数量超过了表大小的10%，并且自动重新计算统计数据的功能是打开的，那么服务器会重新进行一次 统计数据的计算，并且更新innodb_table_stats和innodb_index_stats表。不过自动重新计算统计数据的过程是异步发生 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:2:1","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"基于内存 这个没啥说的。 哈哈 ","date":"2021-01-20","objectID":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/:2:2","tags":["MySQL","成本"],"title":"MySQL-成本与统计数据学习","uri":"/mysql-%E6%88%90%E6%9C%AC%E4%B8%8E%E7%BB%9F%E8%AE%A1%E6%95%B0%E6%8D%AE%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"更新语句的基本流程 更新数据前 会把数据写入uodo 日志文件 更新数据后 会把数据写入redo缓冲池，这意味着这个缓冲池落地磁盘，跟事务有关系，可以通过innodb_flush_log_at_trx_commit 配置。 当参数是1时，就是必须把redo log写入磁盘才能提交事务。0 的时候就是不会有redo日志、2 的时候是先写oscache 这两种都有丢失数据的风险 buffer pool就是我们一直在操作的数据的缓冲区。 redo log 文件里面也是一块一块追加在一起的 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:1:0","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Redo Log redo日志会把事务在执行过程中对数据库所做的所有修改都记录下来，在之后系统奔溃重启后可以把事务所做的任何修改都恢复出来 一个事务可能包含很多语句，即使是一条语句也可能修改许多页面，倒霉催的是该事务修改的这些页面可能并不相邻，这就意味着在将某个事务修改的BufferPool中的页面刷新到磁盘时，需要进行很多的随机IO，随机IO比顺序IO要慢。 我们只是想让已经提交了的事务对数据库中数据所做的修改永久生效，即使后来系统崩溃，在重启后也能把这种修改恢复出来。 所以，只需要把修改了哪些东西记录一下就好，比方说某个事务将系统表空间中的第100号页面中偏移量为1000处的那个字节的值1改成2我们只需要记录一下： redo日志是顺序写入磁盘的,所以相对随机写入性能肯定更好。 InnoDB的redo log是固定大小的，比如可以配置为一组4个文件，每个文件的大小是1GB，那么这块“粉板”总共就可以记录4GB的操作。从头开始写，写到末尾就又回到开头循环写 `innodb_log_group_home_dir`:该参数指定了redo日志文件所在的目录，默认值就是当前的数据目录。 `innodb_log_file_size`该参数指定了每个redo日志文件的大小，在MySQL 5.7.21这个版本中的默认值为48MB， `innodb_log_files_in_group`该参数指定redo日志文件的个数，默认值为2，最大值为100。 ## 如果是现在常见的几个TB的磁盘的话，就不要太小气了，直接将redo log设置为4个文件、每个文件1GB吧 写入过程 语句在执行过程中可能修改若干个页面。还会更新聚簇索引和二级索引对应B+树中的页面。由于对这些页面的更改都发生在Buffer Pool中，所以在修改完页面之后，需要记录一下相应的redo日志。写入的时候会被分成组提交。 log buffer 这些redo日志是一个不可分割的组，所以其实并不是每生成一条redo日志，就将其插入到logbuffer中，而是每个组运行过程中产生的日志先暂时存到一个地方，当该组结束的时候，将过程中产生的一组redo日志再全部复制到log buffer中。而写入 log buffer 也是顺序写。 当log buffer空间不足时、事务提交时、后台线程大约每秒刷新log buffer中的redo日志到磁盘、正常关闭服务器时、刷flush链表时：这些都会让log buffer 强制写磁盘。并且最后会把执行过程中可能修改过的页面加入到Buffer Pool的flush链表 写入时机 为了控制redo log的写入策略，InnoDB提供了innodb_flush_log_at_trx_commit参数，它有三种可能取值 设置为0的时候，表示每次事务提交时都只是把redo log留在redo log buffer中; 设置为1的时候，表示每次事务提交时都将redo log直接持久化到磁盘； 设置为2的时候，表示每次事务提交时都只是把redo log写到page cache。 InnoDB有一个后台线程，每隔1秒，就会把redo log buffer中的日志，调用write写到文件系统的page cache，然后调用 fsync持久化到磁盘。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:2:0","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Undo log 每当我们做了修改数据的操作时，我们需要有回滚操作，undo log 就是保证回滚这一操作的日志，把回滚时所需的东 西都给记下来,要注意的是undo log 日志是维护事务版本链的重要东西。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:3:0","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Insert日志 主要就是记录了新增数据的信息而已。 会发现自增id的申请在这之前，所以回滚了。自增id也不会回滚 undo no 是在事务的版本链里面会用到。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:3:1","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"delete日志 删除数据是没有真正删除，在删除的过程中有中间状态，不过事务提交后。后台会有线程把删除状态的数据加入到链表头部。 删除链表的数据的空间是会被新插入的数据利用的，但是如果不够用，或者用不了。就会出现页合并的情况。 在删除语句所在的事务提交之前，只会经历标记数据阶段，也就是delete mark阶段（提交之后我们就不用回滚了，所以只需考虑对 删除操作的标记影响进行回滚）。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:3:2","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"update日志 不更新主键 在不更新主键的情况下，undo log日志会直接记录更新前的数据在日志里(UPDATE语句更新的列大小都没有改动，所以可以采用就地更新的方式)，把老数据真的删除，插入更新后的数据。 更新主键的情况 在聚簇索引中，记录是按照主键值的大小连成了一个单向链表的，如果我们更新了某条记录的主键值，意味着这条记录在聚簇索引中的位置将会发生改变，比如你将记录的主键值从1更新为10000，如果还有非常多的记录的主键值分布在1 ~ 10000之间的话，那么这两条记录在聚簇索引中就有可能离得非常远，甚至中间隔了好多个页面。针对UPDATE语句中更新了记录主键值的这种情况，InnoDB在聚簇索引中分了两步处理 将旧记录进行delete mark操作 之所以只对旧记录做delete mark操作，是因为别的事务同时也可能访问这条记录，如果把它真正的删除加入到垃圾链表后，别的事务就访问不到了。这个功能就是所谓的MVCC。数据在事务提交后才由专门的线程做purge操作，把它加入到垃圾链表中。 根据更新后各列的值创建一条新记录，并将其插入到聚簇索引中（需重新定位插入的位置）。 针对UPDATE语句更新记录主键值的这种情况，在对该记录进行delete mark操作前，会记录一条类型delete的undo日志；之后插入新记录时，会记录一条类型为insert的undo日志，也就是说每对一条记录的主键值做改动时，会记录2条undo日志。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:3:3","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"undo链表 一个事务可能执行多个update insert语句, 在对记录的改变记录不同的链表来维持记录的变动。 事务提交后，undolog链表如果是比较小的是会被重用的。具体undo写入过程就不写了。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:3:4","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"binlog Redo log是InnoDB独有的，是为了崩溃恢复，数据持久 而MySQL本身独有的文件是binlog：是在事务提交的时候同时写入binlog ,并且会完善redo数据的正确性commit。以便崩溃恢复redolog数据的正确性。binlog是MySQL用来全量回复到某个时间点的日志 binlog刷盘也是会有异步同步的情况，异步是写入oscache 默认是异步写入。 binlog是逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1 ”。 redo log是循环写的；binlog是可以追加写入的。指binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志。 通过两阶段提交保证数据正确性，简单说，redo log和binlog都可以用于表示事务的提交状态，而两阶段提交就是让这两个状态保持逻辑上的一致。 处于prepare阶段的redo log加上完整binlog，重启就能恢复 binlog写入时机 binlog的写入逻辑比较简单：事务执行过程中，先把日志写到binlog cache，事务提交的时候，再把binlog cache写 到binlog文件中。一个事务的binlog是不能被拆开的，因此不论这个事务多大，也要确保一次性写入。 系统给binlog cache分配了一片内存，每个线程一个，参数binlog_cache_size用于控制单个线程内binlog cache所占内存 的大小。如果超过了这个参数规定的大小，就要暂存到磁盘。 write:指的就是指把日志写入到文件系统的page cache，并没有把数据持久化到磁盘，所以速度比较快 fsync:才是将数据持久化到磁盘的操作。一般情况下，我们认为fsync才占磁盘的IOPS。 write 和fsync的时机，是由参数sync_binlog控制的： sync_binlog=0的时候，表示每次提交事务都只write，不fsync； sync_binlog=1的时候，表示每次提交事务都会执行fsync； sync_binlog=N(N\u003e1)的时候，表示每次提交事务都write，但累积N个事务后才fsync。 因此，在出现IO瓶颈的场景里，将sync_binlog设置成一个比较大的值，可以提升性能。在实际的业务场景中，考虑到丢 失日志量的可控性，一般不建议将这个参数设成0，比较常见的是将其设置为100~1000中的某个数值。但是，将sync_binlog设置为N，对应的风险是：如果主机发生异常重启，会丢失最近N个事务的binlog日志。 通常我们说MySQL的“双1”配置，指的就是sync_binlog和innodb_flush_log_at_trx_commit都设置成 1。也就是说，一个事 务完整提交前，需要等待两次刷盘，一次是redo log（prepare 阶段），一次是binlog。在配合组提交。这样tps相对压力还好。 redo log 和 binlog都是顺序写，磁盘的顺序写比随机写速度要快；组提交机制，可以大幅度降低磁盘的IOPS消耗。 binlog的三种格式 statement:记录的是具体的事务语句。当主从设置这个格式是很危险的，因为相当于针对语句，没有针对具体数据。 row：row格式的binlog里没有了SQL语句的原文，binlog里面记录了真实操作的主键id。 mixed: 因为有些statement格式的binlog可能会导致主备不一致，所以要使用row格式。但row格式的缺点是，很占空间。比如你用一个delete语句删掉10万行数据，用statement的话就是一个SQL语句被记录到binlog中，占用几十个字节的空间。但如果用row格式的binlog，就要把这10万条记录都写到binlog中。这样做会占用更大的空间，同时写binlog也要耗费IO资源，影响执行速度。 所以，MySQL就取了个折中方案，也就是有了mixed格式的binlog。 mixed格式的意思是，MySQL自己会判断这条SQL语句是否可能引起主备不一致，如有可能，就用row格式，否则就用statement格式。 ","date":"2021-01-20","objectID":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/:4:0","tags":["MySQL","redo log","undo log","binlog"],"title":"MySQL-日志学习","uri":"/mysql-%E6%97%A5%E5%BF%97%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"条件化简 ## 移除不必要的括号 ((a = 5 AND b = c) OR ((a \u003e c) AND (c \u003c 5))) 改 (a = 5 and b = c) OR (a \u003e c AND c \u003c 5) ## 常量传递（constant_propagation） a = 5 AND b \u003e a 改 a = 5 AND b \u003e 5 ## 移除没用的条件（trivial_condition_removal） (a \u003c 1 and b = b) OR (a = 6 OR 5 != 5) 很明显，b = b这个表达式永远为TRUE，5 != 5这个表达式永远为FALSE，所以简化后的表达式就是这样的： a \u003c 1 OR a = 6 ## 表达式计算 a = 5 + 1 改成 a=6 表达式不会对带有符号或者函数的进行化简,只有搜索条件中索引列和常数使用某些运算符连接起来才可能使用到索引 ##HAVING子句和WHERE子句的合并 如果查询语句中没有出现诸如SUM、MAX等等的聚集函数以及GROUP BY子句，优化器就把HAVING子句和WHERE子句合并起来。 ","date":"2021-01-20","objectID":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/:1:0","tags":["MySQL","子查询"],"title":"MySQL-查询-SQL简化与子查询的物化","uri":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/"},{"categories":["MySQL性能调优"],"content":"外连接消除 具体可以先看连接查询 内连接的驱动表和被驱动表的位置可以相互转换，而左（外）连接和右（外）连接的驱动表和被驱动表是固定的。 这就导致内连接可能通过优化表的连接顺序来降低整体的查询成本，而外连接却 无法优化表的连接顺序。 外连接和内连接的本质区别就是：对于外连接的驱动表的记录来说，如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录仍然会被加入到结果集中，对应的被驱动表 记录的各个字段使用NULL值填充；而内连接的驱动表的记录如果无法在被驱动表中找到匹配ON子句中的过滤条件的记录，那么该记录会被舍弃 凡是不符合WHERE子句中条件的记录都不会参与连接。只要我们在搜索条件中指定关于被驱动表相关列的值不为NULL，那么外连接中在被驱动表中找不到符合ON子 句条件的驱动表记录也就被排除出最后的结果集了，也就是说：在这种情况下：外连接和内连接也就没有什么区别了！ ","date":"2021-01-20","objectID":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/:2:0","tags":["MySQL","子查询"],"title":"MySQL-查询-SQL简化与子查询的物化","uri":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/"},{"categories":["MySQL性能调优"],"content":"子查询 ## 写法 [NOT] IN/ANY/SOME/ALL子查询 ANY/SOME相同 ","date":"2021-01-20","objectID":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/:3:0","tags":["MySQL","子查询"],"title":"MySQL-查询-SQL简化与子查询的物化","uri":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/"},{"categories":["MySQL性能调优"],"content":"子查询查询方式 SELECT * FROM s1 WHERE key1 = (SELECT common_field FROM s2 WHERE key3 = 'a' LIMIT 1); 它的执行方式和年少的我想的一样： 先单独执行(SELECT common_field FROM s2 WHERE key3 = 'a' LIMIT 1)这个子查询。 然后在将上一步子查询得到的结果当作外层查询的参数再执行外层查询SELECT * FROM s1 WHERE key1 = ...。 ","date":"2021-01-20","objectID":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/:3:1","tags":["MySQL","子查询"],"title":"MySQL-查询-SQL简化与子查询的物化","uri":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/"},{"categories":["MySQL性能调优"],"content":"IN子查询优化 在MySQL5.5以及之前的版本没有引进semi-join和物化的方式优化子查询时，优化器都会把IN子查询转换为EXISTS子查询，所以当时好多声音都是建议大家把子查询转为连接，不过随着MySQL的发展，最近的版本中引入了非常多的子查询优化策略，大家可以稍微 放心的使用子查询了，内部的转换工作优化器会为大家自动实现。 SELECT * FROM s1 WHERE key1 IN (SELECT common_field FROM s2 WHERE key3 = 'a'); 不直接将不相关子查询的结果集当作外层查询的参数，而是将该结果集写入一个临时表里 (称为物化) 一般情况下子查询结果集不会大的离谱，所以会为它建立基于内存的使用Memory存储引擎的临时表，而且会为该表建立哈希索引。 如果子查询的结果集非常大，超过了系统变量tmp_table_size或者max_heap_table_size，临时表会转而使用基于磁盘的存储引擎来保存结果集中的记录，索引类型也对应转变为B+树索引。 最后如果物化表没重复值甚至还会与物化表进行连接查询 SELECT s1.* FROM s1 INNER JOIN materialized_table ON key1 = m_val; 下面条件是会转半连接SEMI JOIN 的情况 对于s1表的某条记录来说，我们只关心在s2表中是否存在与之匹配的记录是否存在，而不关心具体有多少条记录与之匹配，最终的结果集中只保留s1表的记录。 该子查询必须是和IN语句组成的布尔表达式，并且在外层查询的WHERE或者ON子句中出现。 外层查询也可以有其他的搜索条件，只不过和IN子查询的搜索条件必须使用AND连接起来。 该子查询必须是一个单一的查询，不能是由若干查询由UNION连接起来的形式。 该子查询不能包含GROUP BY或者HAVING语句或者聚集函数 ","date":"2021-01-20","objectID":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/:3:2","tags":["MySQL","子查询"],"title":"MySQL-查询-SQL简化与子查询的物化","uri":"/mysql-%E6%9F%A5%E8%AF%A2-sql%E7%AE%80%E5%8C%96%E4%B8%8E%E5%AD%90%E6%9F%A5%E8%AF%A2%E7%9A%84%E7%89%A9%E5%8C%96/"},{"categories":["MySQL性能调优"],"content":" MySQL在一般情况下执行一个查询时最多只会用到单个二级索引，但是，在这些特殊情况下也可能在一个查询中使用到多个二级索引，设计MySQL的把这种使用到多个索引来完成一次查询的执行方法称之为：index merge 所有的索引合并往往还是需要看数据回表的数量与 mysql 的优化器！优化器只有在单独根据搜索条件从某个二级索引中 获取的记录数比较少，通过Union索引合并后进行访问的代价比全表扫描更小时才会使用Union索引合并 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/:0:0","tags":["MySQL","索引合并"],"title":"MySQL-索引-索引合并学习","uri":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Intersection合并 Intersection翻译过来的意思是交集。这里是说某个查询可以使用多个二级索引，将从多个二级索引中查询到的结果取交集 SELECT * FROM single_table WHERE key1 = 'a' AND key3 = 'b'; 假设这个查询使用Intersection合并的方式执行的话: 从idx_key1二级索引对应的B+树中取出key1 = ‘a’的相关记录。 从idx_key3二级索引对应的B+树中取出key3 = ‘b’的相关记录。 二级索引的记录都是由索引列 + 主键构成的，所以我们可以计算出这两个结果集中id值的交集。按照上一步生成的id值列表进行回表操作，也就是从聚簇索引中把指定id值的完整用户记录取出来，返回给用户。 虽然读取多个二级索引比读取一个二级索引消耗性能，但是读取二级索引的操作是顺序I/O，而回表操作是随机I/O， 所以如果只读取一个二级索引时需要回表的记录数特别多，而读取多个二级索引之后取交集的记录数非常少，当节省的因为回表而造成的性能损耗比访问多个二级索引带来的性能损耗更高时，读取多个二级索引后取交集比只读取一个二级索引的成本更低。 什么情况可能会使用到Intersection索引合并： 二级索引列是等值匹配的情况，对于联合索引来说，在联合索引中的每个列都必须等值匹配，不能出现只匹配部分列的情况。 说明 不能有范围查找，并且联合索引情况下所有列都需要出现。 主键列可以是范围匹配 SELECT * FROM single_table WHERE id \u003e 100 AND key1 = 'a'; 对于InnoDB的二级索引来说，记录先是按照索引列进行排序，如果该二级索引是一个联合索引，那么会按照联合索引中的各个列依次排序。 而二级索引的用户记录是由索引列 + 主键构成的，二级索引列的值相同的记录可能会有好多条， 这些索引列的值相同的记录又是按照主键的值进行排序的。 所以重点来了，之所以在二级索引列都是等值匹配的情况下才可能使用Intersection索引合并， 是因为只有在这种情况下根据二级索引查询出的结果集是按照主键值排序的 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/:0:1","tags":["MySQL","索引合并"],"title":"MySQL-索引-索引合并学习","uri":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Union合并 OR我们在写查询语句时经常想把既符合某个搜索条件的记录取出来，也把符合另外的某个搜索条件的记录取出来; Intersection是交集的意思，这适用于使用不同索引的搜索条件之间使用AND连接起来的情况；Union是并集的意思，适用于使用不同索引的搜索条件之间使用OR连接起来的情况。与Intersection索引合并基本相同，并且可以与Intersection一起用，MySQL在某些特定的情况下才可能会使用到Union索引合并 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/:0:2","tags":["MySQL","索引合并"],"title":"MySQL-索引-索引合并学习","uri":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"Sort-Union合并 Union索引合并的使用条件太苛刻，必须保证各个二级索引列在进行等值匹配的条件下才可能被用到，比方说下边这个查询就无法使用 到Union索引合并：SELECT * FROM single_table WHERE key1 \u003c 'a' OR key3 \u003e 'z' 这是因为根据key1 \u003c ‘a’从idx_key1索引中获取的二级索引记录的主键值不是排好序的，根据key3 \u003e ‘z’从idx_key3索引中获取的二级索引记录的主键值也不是排好序的。所以： 先根据key1 \u003c ‘a’条件从idx_key1二级索引总获取记录，并按照记录的主键值进行排序 再根据key3 \u003e ‘z’条件从idx_key3二级索引总获取记录，并按照记录的主键值进行排序 因为上述的两个二级索引主键值都是排好序的，剩下的操作和Union索引合并方式就一样了，把id合并在一起。 我们把上述这种先按照二级索引记录的主键值进行排序，之后按照Union索引合并方式执行的方式称之为Sort-Union索引合并，很显然，这种Sort-Union索引合并比单纯的Union索引合并多了一步对二级索引记录的主键值排序的过程。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/:0:3","tags":["MySQL","索引合并"],"title":"MySQL-索引-索引合并学习","uri":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"联合索引替代Intersection索引合并 这个查询之所以可能使用Intersection索引合并的方式执行，还不是因为idx_key1和idx_key3是两个单独的B+树索引，索引要是把这两个列搞一个联合索引，那直接使用这个联合索引就把事情搞定了，何必用啥索引合并呢。 不过还是要考虑一下单idx_key3索引的查询问题！要么单独建一个，要么在查询上配合上联合查询。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/:0:4","tags":["MySQL","索引合并"],"title":"MySQL-索引-索引合并学习","uri":"/mysql-%E7%B4%A2%E5%BC%95-%E7%B4%A2%E5%BC%95%E5%90%88%E5%B9%B6%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"InnoDB索引结构 每个页里面有最大和最小记录标识record_type=2,3 record_type=1是目录项 0是最终存储记录项 一个页16kb=16384字节 可以存很多记录了基本3层就已经很多条记录了 每个节点的左儿子小于父节点，父节点又小于右儿子 索引每个节点的数据是已经排序好了的，当然如果你是联合索引，那也是排序好了的。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:0","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"索引的代价 空间问题 每建立一个索引都要为它建立一棵B+树，每一棵B+树的每一个节点都是一个数据页，一个页默认会占用16KB的存储空间，一棵很大的B+树由许多数据页组成。一个表上索引建的越多，就会占用越多的存储空间 由于每个非主键索引的叶子节点上都是主键的值。如果用整型做主键，则只要4个字节，如果是长整型（bigint）则是8个字节。 **显然，主键长度越小，普通索引的叶子节点就越小，普通索引占用的空间也就越小。**所以，从性能和存储空间方面考量，自增主键往往是更合理的选择。当然，如果分库分表有时你不得不用上更长的有序id，如雪花id。 时间问题 每次对表中的数据进行增、删、改操作时，都需要去修改各个B+树索引。B+树每层节点都是按照索引列的值从小到大的顺序排序而组成了双向链表。不论是叶子节点中的记录，还是内节点中的记录（也就是不论是用户记录还是目录项记录）都是按照索引列的值从小到大的顺序而形成了一个单向链表。而增、删、改操作可能会对节点和记录的排序造成破坏，所以存储引擎需要额外的时间进行一些记录移位，页面分裂、页面回收啥的操作来维护好节点和记录的排序。如果我们建了许多索引，每个索引对应的B+树都要进行相关的维护操作， 索引越多时 增删改记录的时候性能就越差。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:1","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"聚簇索引特点： 使用记录主键值的大小进行记录和页的排序，这包括三个方面的含义： 页内的记录是按照主键的大小顺序排成一个单向链表。 各个存放用户记录的页也是根据页中用户记录的主键大小顺序排成一个双向链表。 存放目录项记录的页分为不同的层次，在同一层次中的页也是根据页中目录项记录的主键大小顺序排成一个双向链表。 B+树的叶子节点存储的是完整的用户记录。 所谓完整的用户记录，就是指这个记录中存储了所有列的值（包括隐藏列）。 我们把具有这两种特性的B+树称为聚簇索引，所有完整的用户记录都存放在这个聚簇索引的叶子节点处。 这种聚簇索引并不需要我们在MySQL语句中显式的使用INDEX语句去创建 ，InnoDB存储引擎会自动的为我们创建聚簇索引，在InnoDB存储引擎中，聚簇索引就是数据的存储方式（所有的用户记录 都存储在了叶子节点），也就是所谓的索引即数据，数据即索引。 简而言之：叶子节点就是数据页自己本身，那么此时我们就可以称这颗B+树索引为聚簇索引 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:2","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"二级索引 二级索引都会根据自己的数据大小进行从左到右排序建立一个二叉树 叶子节点只存储索引列与主键两列的值。目录项之存储了页号与索引列 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:3","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"回表 如果我们的索引走的是二级索引，根据页目录定位到叶子节点，然后定位到具体记录，然后我们要根据具体记录的id去聚簇索引找到具体记录。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:4","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"联合索引 每条目录项记录都由联合索引的列和页号这三个部分组成，各条记录先按照第一个列的值进行排序，如果记录的相同，则按照第二列的值进行排序。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:5","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"MyISAM索引结构 InnoDB中索引即数据，也就是聚簇索引的那棵B+树的叶子节点中已经把所有完整的用户记录都包含了，而MyISAM的索引方案虽然也使用树形结构，但是却将索引和数据分开存储，意味着都是二级索引。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:6","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"页分裂与索引空间的回收问题 页分裂的过程 在对页中的记录进行增删改操作的过程中，我们必须通过一些诸如记录移动的操作来始终保证MySQL的B+Tree页顺序问题。 为了保证这个状态一直成立：下一个数据页中用户记录的主键值必须大于上一个页中用户记录的主键值。就称为页分裂。 如果移动记录时发现目标页还放不下，这时候需要申请一个新的数据页，然后挪动部分数据过去,性能更受影响 有分裂就有合并。当相邻两个页由于删除了数据，利用率很低之后，会将数据页做合并。合并的过程，可以认为是分裂过程的逆过程 B+树的插入可能会引起数据页的分裂，删除可能会引起数据页的合并，二者都是比较重的IO消耗，所以比较好的方式是顺序插入数据，这也是我们一般使用自增主键的原因之一。 索引可能因为删除，或者页分裂等原因，导致数据页有空洞，重建索引的过程会创建一个新的索引，把数据按顺序插入，这样页面的利用率最高，也就是索引更紧凑、更省空间，删除普通索引影响不大，但是最好不要对主键索引重建，因为会影响表内所有索引树 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:1:7","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"索引基本使用 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:2:0","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"覆盖索引 select ID from T where k between 3 and 5，这时只需要查ID的值，而ID的值已经在k索引树上了，因此可以直接提供查询结果，不需要回表。也就是说，在这个查询里面，索引k已经“覆盖了”我们的查询需求，我们称为覆盖索引。由于覆盖索引可以减少树的搜索次数，显著提升查询性能，所以使用覆盖索引是一个常用的性能优化手段 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:2:1","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"全值匹配 这个不用怎么说 全值匹配索引列完事，如果查询字段也是索引列和id，那也不用回表。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:2:2","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"分组 如果你group by 用到了索引的话，恰巧这个分组顺序又和我们的B+树中的索引列的顺序是一致的，而我们的B+树索引又是按照索引列排好序的。可以直接就分组排序了。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:2:3","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"左匹配原则 字符左匹配比较简单。 需要注意一点的是联合索引的顺序需要跟条件一个顺序，范围查找可能中断顺序。 如果联合索引是 name birthday 下面语句将只能用name的索引 SELECT * FROM person_info WHERE name \u003e ‘Asa’ AND name \u003c ‘Barlow’ AND birthday \u003e ‘1980-01-01’; ORDER BY 的子句后边的列的顺序也必须按照索引列的顺序给出 (name,birthday,phone_number)联合索引 SELECT * FROM person_info WHERE name = ‘A’ ORDER BY birthday, phone_number LIMIT 10; SELECT * FROM person_info WHERE country = ‘China’ ORDER BY name LIMIT 10; 这个查询只能先把符合搜索条件country = ‘China’的记录提取出来后再进行排序，是使用不到索引。 ASC DESC 不能混用：因为索引结构里面顺序都是按照一个方向排序的。 GROUP BY 跟ORDER BY规则一样 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:2:4","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"索引条件下推 select * from tuser where name like ‘张%’ and age=10; 联合索引(name, age) 在5.6之前 只能匹配张%的数据一个个回表。到主键索引上找出数据行，再对比字段age的值。 在MYSQL 5.6过后 可以在索引遍历过程中，对索引中包含的字段先做判断，直接过滤掉不满足条件的记录，减少回表次数。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:2:5","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"索引的选择 考虑数据的基数离散性 数据类型的选择尽量小，这个空间复杂度和时间复杂度都会更好 在字符串太长的情况的索引最好切割开索引比如(name(10)) 不做运算 主键的设计，由于结构的原因是根据大小排序，最好是自增的。 冗余索引与联合索引的顺序优化。比如索引 c 与联合索引 (c,d) ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:3:0","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"基本总结 B+树索引在空间和时间上都有代价，所以没事儿别瞎建索引。 B+树索引适用于下边这些情况： 全值匹配 匹配左边的列 匹配范围值 精确匹配某一列并范围匹配另外一列 用于排序 用于分组 在使用索引时需要注意下边这些事项： 只为用于搜索、排序或分组的列创建索引 为列的基数大的列创建索引 索引列的类型尽量小 可以只对字符串值的前缀建立索引 只有索引列在比较表达式中单独出现才可以适用索引 为了尽可能少的让聚簇索引发生页面分裂和记录移位的情况，建议让主键拥有AUTO_INCREMENT属性。 定位并删除表中的重复和冗余索引 尽量使用覆盖索引进行查询，避免回表带来的性能损耗。 ","date":"2021-01-20","objectID":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/:4:0","tags":["MySQL","索引","b+tree"],"title":"MySQL-索引基本学习","uri":"/mysql-%E7%B4%A2%E5%BC%95%E5%9F%BA%E6%9C%AC%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"连接查询的基本语法 INNER JOIN ## 内连接和外连接的根本区别就是在驱动表中的记录不符合ON子句中的连接条件时不会把该记录加入到最后的结果集 LEFT JOIN ## 右边未匹配的null展示 RIGHT JOIN ## 左边未匹配的null展示 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:1:0","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"连接的原理 需要知道的知识 对于两表连接来说，驱动表只会被访问一遍，但被驱动表却要被访问到好多遍，来匹配记录。 如果我们自己用代码来做连接查询，用不到索引走数去查询被驱动表，性能肯定完全没有mysql那么好 在决定哪个表做驱动表的时候，应该是两个表按照各自的条件过滤，过滤完成之后，计算参与join的各个字段的总数据量，数据量小的那个表，就是“小表”，应该作为驱动表我们应该尽量使用小表做驱动表，这样尽可能的减少IO访问次数 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:2:0","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"嵌套循环连接(Nested-Loop Join) 步骤1：选取驱动表，使用与驱动表相关的过滤条件，选取代价最低的单表访问方法来执行对驱动表的单表查询 步骤2：对上一步骤中查询驱动表得到的结果集中每一条记录，都分别到被驱动表中查找匹配的记录 如果有3个表进行连接的话，那么步骤2中得到的结果集就像是新的驱动表，然后第三个表就成为了被驱动表 所以我们设计的连接查询的时候关联条件，在被驱动表中是可以使用到索引的。但是还是得注意，只有在二级索引 + 回表的代价 比全表扫描的代价更低时才会使用索引。还有就是最终查询出来的字段最好不要是* 这样尽可能的使用覆盖索引查询更好 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:2:1","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"基于块的嵌套循环连接(Block Nested-Loop Join) 这个可以在执行计划Extra字段里面看到“Block Nested Loop”字样。 尽量减少访问被驱动表的次数 MySQL提出了一个join buffer的概念，join buffer就是执行连接查询前申请的一块固定大小的内存，先把若干条驱动表结果集中的记录装在这个join buffer中，然后开始扫描被驱动表，每一条被驱动表的记录一次性和join buffer中的多条驱动表记录做匹配，因为匹配的过程都是在内存中完成的，所以这样可以显著减少被驱动表的I/O代价。最好的情况是join buffer足够大，能容纳驱动表结果集中的所有记录，这样只需要访问一次被驱动表就可以完成连接操作了。 这个join buffer的大小是可以通过启动参数或者系统变量join_buffer_size进行配置，默认大小为262144字节（也就是256KB），最小可以设置为128字节。当然，对于优化被驱动表的查询来说，最好是为被驱动表加上效率高的索引，如果实在不能使用索引，并且自己的机器的内存也比较大可以尝试调大join_buffer_size的值来对连接查询进行优化。 show variables like 'join_buffer_size%' 另外需要注意的是，驱动表的记录并不是所有列都会被放到join buffer中，只有查询列表中的列和过滤条件中的列才会被放到join buffer中，所以再次提醒我们，最好不要把*作为查询列表，只需要把我们关心的列放到查询列表就好了，这样还可以在join buffer中放置更多的记录。如果放不下驱动表的所有数据话，策略很简单，就是分段放 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:2:2","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"优化相关 Multi-Range Read优化 介绍 MRR优化在与在于，定位数据后没有直接回表通过id随机查询，而是在中间用了一块read_rnd_buffer，把大量定位到的数据进行排序后，去主键id索引树上顺序查询记录。但是这个是默认关闭的set optimizer_switch=\"mrr_cost_based=off\" (NLJ的优化BKA)Batched Key Access NLJ算法的优化 MySQL默认内置的优化策略，建议默认开启使用 set optimizer_switch='mrr=on,mrr_cost_based=off,batched_key_access=on'; 前两个参数的作用是要启用MRR。这么做的原因是，BKA算法的优化要依赖于MRR。 NLJ算法执行的逻辑是：从驱动表t1，一行行地取出a的值，再到被驱动表t2去做join。也就是说，对于表t2来说，每次都是匹配一个值。 当用上BKA算法后，他会利用join_buffer 这个空间来临时存储从被驱动表取出来的数据，也就减少了join的次数。 (BNL的优化) MySQL在大表的情况下使用Block Nested-Loop Join算法时，很依赖于join_buffer_size，这个值往往比较小。所以MySQL在大表连接查询性能会不好。 首先明确一点：由于InnoDB对Bufffer Pool的LRU算法做了优化，即：第一次从磁盘读入内存的数据页，会先放在old区域。如果1秒之后这个数据页不再被访问了，就不会被移动到LRU链表头部，这样对Buffer Pool的命中率影响就不大。 但是，如果一个使用BNL算法的join语句，多次扫描一个冷表，而且这个语句执行时间超过1秒，就会在再次扫描冷表的时候，把冷表的数据页移到LRU链表头部。如果这个冷表很大，就会出现另外一种情况：业务正常访问的数据页，没有机会进入young区 大表join操作虽然对IO有影响，但是在语句执行结束后，对IO的影响也就结束了。但是，对Buffer Pool的影响就是持续性的，需要依靠后续的查询请求慢慢恢复内存命中率。 可能会多次扫描被驱动表，占用磁盘IO资源； 判断join条件需要执行M*N次对比（M、N分别是两张表的行数），如果是大表就会占用非常多的CPU资源； 可能会导致Buffer Pool的热数据被淘汰，影响内存命中率。 我们执行语句之前，需要通过理论分析和查看explain结果的方式，确认是否要使用BNL算法。 给被驱动表的join字段加上索引，把BNL算法转成BKA算法 如果被驱动表数据量少，觉得建立索引浪费，那也只能在代码中利用hash 的方法来匹对数据了。 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:3:0","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"相关问题 首先inner join 会根据数据量来确定驱动表。 而left join 或者 right join 指定驱动表，最后MySQL也不一定是这样执行的。 select * from a left join b on(a.f1=b.f1) and (a.f2=b.f2); /*Q1*/ select * from a left join b on(a.f1=b.f1) where (a.f2=b.f2);/*Q2*/ 在MySQL里，NULL跟任何值执行等值判断和不等值判断的结果，都是NULL。这里包括， select NULL = NULL 的结果，也是返回NULL 语句Q2里面where a.f2=b.f2就表示，查询结果里面不会包含b.f2是NULL的行，这样这个left join的语义就是“找到这两个表里面，f1、f2对应相同的行。对于表a中存在，而表b中匹配不到的行，就放弃”,这样，这条语句虽然用的是left join，但是语义跟join是一致的,最终MySQL 就会改写为join。 改写： 我们在看看joinwhere条件和on 的条件 可以看到join将判断条件是否全部放在on部分就没有区别 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:4:0","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"总结 总体来看。在做连接查询时，我们需要根据条件区别出那个表记录是最少的。把他作为驱动表。然后在根据被驱动表的join字段建立索引，这样是比较好的。总之，整体的思路就是，尽量让每一次参与join的驱动表的数据集，越小越好，因为这样我们的驱动表就会越小。 虽然BNL算法和Simple Nested Loop Join 算法都是要判断M*N次（M和N分别是join的两个表的行数），但是Simple Nested Loop Join 算法的每轮判断都要走全表扫描，而BNL是不会影响buffer pool这一块东西的。NLJ这个算法天然会对被驱动表的数据做多次访问，更容易将这些数据页放到Buffer Pool的头部。因此性能上BNL算法执行起来会快很多。 ","date":"2021-01-20","objectID":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/:5:0","tags":["MySQL","连接查询","NLJ","BNL"],"title":"MySQL-连接查询学习","uri":"/mysql-%E8%BF%9E%E6%8E%A5%E6%9F%A5%E8%AF%A2%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"SQL标准规定不同隔离级别 READ UNCOMMITTED :脏读、不可重复读、幻读都可能发生。 READ COMMITTED:隔离级别下，不可重复读、幻读可能发生，脏读不可以发生。 REPEATABLE READ:隔离级别下，幻读可能发生，脏读和不可重复读不可以发生。 SERIALIZABLE:隔离级别下，上述问题都不可以发生。 MySQL在REPEATABLE READ隔离级别实际上就已经解决了幻读问题。 幻读 幻读问题的产生是因为某个事务读了一个范围的记录，之后别的事务在该范围内插入了新记录，该事务再次读取该范围的记录时，可以读到新插入的记录，所以幻读问题准确的说并不是因为读取和写入一条相同记录而产生的，这一点要注意一下。 解决脏读、不可重复读、幻读 我们说过普通的SELECT语句在READ COMMITTED和REPEATABLE READ隔离级别下会使用到MVCC读取记录。在READ COMMITTED隔离级别下，一个事务在执行过程中每次执行SELECT操作时都会生成一个ReadView，ReadView的存在本身就保证了事务不可以读取到未提交的事务所做的更改，也就是避免了脏读现象；REPEATABLE READ隔离级别下，一个事务在执行过程中只有第一次执行SELECT操作才会生成一个ReadView，之后的SELECT操作都复用这个ReadView，这样也就避免了不可重复读和幻读的问题。 ","date":"2021-01-20","objectID":"/mysql-%E9%94%81%E7%9A%84%E5%AD%A6%E4%B9%A0/:0:1","tags":["MySQL","间隙锁","临键锁"],"title":"MySQL-锁的学习","uri":"/mysql-%E9%94%81%E7%9A%84%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"加锁 表级别的锁 LOCK TABLES xxx READ：这是加表级共享锁 LOCK TABLES xxx WRITE：这是加表级独占锁 系统默认加的MDL，当对一个表做增删改查操作的时候，加MDL读锁；当要对表做结构变更操作的时候，加MDL写锁。 所以改表结构要注意点，会阻塞这个表的所有操作 其实走mvcc基本都够了没必要加锁。除非特定的业务需要独占某条数据。 锁定语句 对读取的记录加S锁： 共享锁相互不互斥 SELECT ... LOCK IN SHARE MODE; 对读取的记录加X锁： 排它锁与锁都互斥 SELECT ... FOR UPDATE; 增删改的语句或多或少在获取数据时或者在更新记录时都是会获取X锁的，具体细化看后面 降低锁时间 在InnoDB事务中，行锁是在需要的时候才加上的，但并不是不需要了就立刻释放，而是要等到事务结束时才释放。这个就是两阶段锁协议。 假设你负责实现一个电影票在线交易业务，顾客A要在影院B购买电影票。我们简化一点，这个业务需要涉及到以下操作： 从顾客A账户余额中扣除电影票价； 给影院B的账户余额增加这张电影票价； 记录一条交易日志。 根据两阶段锁协议，不论你怎样安排语句顺序，所有的操作需要的行锁都是在事务提交的时候才释放的。所以，如果你把语句2安排在最后，比如按照3、1、2这样的顺序，那么影院账户余额这一行的锁时间就最少。这就最大程度地减少了事务之间的锁等待，提升了并发度。 死锁检测策略 一种策略是，直接进入等待，直到超时。这个超时时间可以通过参数innodb_lock_wait_timeout来设置。 另一种策略是，发起死锁检测，发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数innodb_deadlock_detect设置为on，表示开启这个逻辑。 间隙锁与临键锁(next-key lock) MVCC不能完美的解决幻读问题 我们都知道 幻读是数据条数的改变，那意味着表里面增加记录的锁时未知的。即使把所有的记录都加上锁，还是阻止不了新插入的记录 这样，当你执行 select * from t where d=5 for update的时候，就不止是给数据库中已有的6个记录加上了行锁，还同时加了7个间隙锁。这样就确保了无法再插入新的记录。 跟间隙锁存在冲突关系的，是“往这个间隙中插入一个记录”这个操作。 间隙锁之间都不存在冲突关系 间隙锁是在可重复读隔离级别下才会生效的，不可重复读那没必要加锁了 间隙锁和行锁合称next-key lock:每个next-key lock是前开后闭区间。 也就是说，我们的表t初始化以后。如果用select * from t for update要把整个表所有记录锁起来， 就形成了7个next-key lock，分别是 (-∞,0]、(0,5]、(5,10]、(10,15]、(15,20]、(20, 25]、(25, +suprenum]。 可以看看之前文章间隙锁 间隙锁案例、规则、死锁 ","date":"2021-01-20","objectID":"/mysql-%E9%94%81%E7%9A%84%E5%AD%A6%E4%B9%A0/:0:2","tags":["MySQL","间隙锁","临键锁"],"title":"MySQL-锁的学习","uri":"/mysql-%E9%94%81%E7%9A%84%E5%AD%A6%E4%B9%A0/"},{"categories":["MySQL性能调优"],"content":"间隙加锁规则 原则1：加锁的基本单位是next-key lock。希望你还记得，next-key lock是前开后闭区间。 原则2：查找过程中访问到的对象才会加锁。 优化1：索引上的等值查询，给唯一索引加锁的时候，next-key lock退化为行锁。 优化2：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。 一个bug：唯一索引上的范围查询会访问到不满足条件的第一个值为止。 注意！有行才会加行锁。如果查询条件没有命中行，那就加next-key lock。当然，等值判断的时候，需要加上优化2 （即：索引上的等值查询，向右遍历时且最后一个值不满足等值条件的时候，next-key lock退化为间隙锁。） \u003c=到底是间隙锁还是行锁？其实，这个问题，你要跟“执行过程”配合起来分析。 在InnoDB要去找“第一个值”的时候，是按照等值去找的，用的是等值判断的规则； 找到第一个值以后，要在索引内找“下一个值”，对应于我们规则中说的范围查找。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:1:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例SQL 所有案例都是在可重复读隔离级别(repeatable-read)下验证的 CREATE TABLE `t` ( `id` int(11) NOT NULL, `c` int(11) DEFAULT NULL, `d` int(11) DEFAULT NULL, PRIMARY KEY (`id`), KEY `c` (`c`) ) ENGINE=InnoDB; insert into t values(0,0,0),(5,5,5), (10,10,10),(15,15,15),(20,20,20),(25,25,25); ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:2:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例1-等值查询间隙锁 由于表t中没有id=7的记录，所以用我们上面提到的加锁规则判断一下的话： 根据原则1，加锁单位是next-key lock，session A加锁范围就是(5,10]； 同时根据优化2，这是一个等值查询(id=7)，而id=10不满足查询条件，next-key lock退化成间隙锁，因此最终加锁的范围是(5,10)。 所以，session B要往这个间隙里面插入id=8的记录会被锁住，但是session C修改id=10这行是可以的。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:3:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例2-非唯一索引等值锁 这里session A要给索引c上c=5的这一行加上读锁。 根据原则1，加锁单位是next-key lock，因此会给(0,5]加上next-key lock。 要注意c是普通索引，因此仅访问c=5这一条记录是不能马上停下来的，需要向右遍历，查到c=10才放弃。根据原则2，访问到的都要加锁，因此要给(5,10]加next-key lock。 但是同时这个符合优化2：等值判断，向右遍历，最后一个值不满足c=5这个等值条件，因此退化成间隙锁(5,10)。 根据原则2 ，只有访问到的对象才会加锁，这个查询使用覆盖索引，并不需要访问主键索引，所以主键索引上没有加任何锁，这就是为什么session B的update语句可以执行完成。 但session C要插入一个(7,7,7)的记录，就会被session A的间隙锁(5,10)锁住。 在这个例子中，lock in share mode只锁覆盖索引，但是如果是for update就不一样了。 执行 for update时，系统会认为你接下来要更新数据，因此会顺便给主键索引上满足条件的行加上行锁。 这个例子说明,锁是加在索引上的, 同时，它给我们的指导是，如果你要用lock in share mode来给行加读锁避免数据被更新的话，就必须得绕过覆盖索引的优化，在查询字段中加入索引中不存在的字段。 所以，如果一个select * from … for update 语句，优化器决定使用全表扫描，那么就会把主键索引上next-key lock全加上。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:4:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例3：主键索引范围锁 mysql\u003e select * from t where id=10 for update; mysql\u003e select * from t where id\u003e=10 and id\u003c11 for update; 这两条查语句肯定是等价的，但是它们的加锁规则不太一样 开始执行的时候，要找到第一个id=10的行，因此本该是next-key lock(5,10]。 根据优化1， 主键id上的等值条件，退化成行锁，只加了id=10这一行的行锁。 范围查找就往后继续找，找到id=15这一行停下来，因此需要加next-key lock(10,15]。 session A这时候锁的范围就是主键索引上，行锁id=10和next-key lock(10,15]。这样，session B和session C的结果你就能理解了。 这里需要注意一点，首次session A定位查找id=10的行的时候，是当做等值查询来判断的，而向右扫描到id=15的时候，用的是范围查询判断。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:5:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例4：非唯一索引范围锁 session A用字段c来判断，加锁规则跟案例三唯一的不同是：在第一次用c=10定位记录的时候，索引c上加了(5,10]这个next-key lock后，由于索引c是非唯一索引，没有优化规则，也就是说不会蜕变为行锁，因此最终sesion A加的锁是，索引c上的(5,10] 和(10,15] 这两个next-key lock。 所以从结果上来看，sesson B要插入（8,8,8)的这个insert语句时就被堵住了。 这里需要扫描到c=15才停止扫描，是合理的，因为InnoDB要扫到c=15，才知道不需要继续往后找了。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:6:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例5：唯一索引范围锁bug session A是一个范围查询，按照原则1的话，应该是索引id上只加(10,15]这个next-key lock，并且因为id是唯一键，所以循环判断到id=15这一行就应该停止了。但是实现上，InnoDB会往前扫描到第一个不满足条件的行为止，也就是id=20。而且由于这是个范围扫描，因此索引id上的(15,20]这个next-key lock也会被锁上。 所以你看到了，session B要更新id=20这一行，是会被锁住的。同样地，session C要插入id=16的一行，也会被锁住。 照理说，这里锁住id=20这一行的行为，其实是没有必要的。因为扫描到id=15,就可以确定不用往后再找了,但实现上还是这么做了 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:7:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例6：非唯一索引上存在\"等值\"的例子 insert into t values(30,10,30); 这时，session A在遍历的时候，先访问第一个c=10的记录。同样地，根据原则1，这里加的是(c=5,id=5)到(c=10,id=10)这个next-key lock。然后，session A向右查找，直到碰到(c=15,id=15)这一行，循环才结束。根据优化2，这是一个等值查询，向右查找到了不满足条件的行，所以会退化成(c=10,id=10) 到 (c=15,id=15)的间隙锁。 所以最后加锁就是（5,5) 到（15,15）之间 即(c=5,id=5)和(c=15,id=15)这两行上都没有锁。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:8:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例7：limit 语句加锁 session A的delete语句加了 limit 2。你知道表t里c=10的记录其实只有两条，因此加不加limit 2，删除的效果都是一样的，但是加锁的效果却不同。可以看到，session B的insert语句执行通过了，跟案例六的结果不同。这是因为，案例七里的delete语句明确加了limit 2的限制，因此在遍历到(c=10, id=30)这一行之后，满足条件的语句已经有两条，循环就结束了。 因此，索引c上的加锁范围就变成了从（c=5,id=5)到（c=10,id=30)这个前开后闭区间； ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:9:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例8：一个死锁的例子 现在，我们按时间顺序来分析一下为什么是这样的结果。 session A 启动事务后执行查询语句加lock in share mode，在索引c上加了next-key lock(5,10] 和间隙锁(10,15)； session B 的update语句也要在索引c上加next-key lock(5,10] ，进入锁等待； 然后session A要再插入(8,8,8)这一行，被session B的间隙锁锁住。由于出现了死锁，InnoDB让session B回滚。 其实是这样的，session B的“加next-key lock(5,10] ”操作，实际上分成了两步，先是加(5,10)的间隙锁，加锁成功；然后加c=10的行锁，这时候才被锁住的。 也就是说，我们在分析加锁规则的时候可以用next-key lock来分析。但是要知道，具体执行的时候，是要分成间隙锁和行锁两段来执行的。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:10:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例9：order by改变加锁方向 由于是order by c desc，第一个要定位的是索引c上“最右边的”c=20的行，所以会加上间隙锁(20,25)和next-key lock (15,20]。 在索引c上向左遍历，要扫描到c=10才停下来，所以next-key lock会加到(5,10]，这正是阻塞session B的insert语句的原因。 在扫描过程中，c=20、c=15、c=10这三行都存在值，由于是select *，所以会在主键id上加三个行锁。 因此，session A 的select语句锁的范围就是：索引c上 (5, 25)；主键索引上id=10、15、20三个行锁。 这里在说明一下！锁就是加在索引上的，这是InnoDB的一个基础设定，需要你在分析问题的时候要一直记得。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:11:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"案例10：再次分析范围查询 begin; select * from t where id\u003e9 and id\u003c12 order by id desc for update; 我们知道这个语句的加锁范围是主键索引上的 (0,5]、(5,10]和(10, 15)。也就是说，id=15这一行，并没有被加上行锁 我们说加锁单位是next-key lock，都是前开后闭区间，但是这里用到了优化2，即索引上的等值查询，向右遍历的时候id=15不满足条件，所以next-key lock退化为了间隙锁 (10, 15)。 首先这个查询语句的语义是order by id desc，要拿到满足条件的所有行，优化器必须先找到“第一个id\u003c12的值”。 这个过程是通过索引树的搜索过程得到的，在引擎内部，其实是要找到id=12的这个值，只是最终没找到，但找到了(10,15)这个间隙。 然后向左遍历，在遍历过程中，就不是等值查询了，会扫描到id=5这一行，所以会加一个next-key lock (0,5]。 begin; select id from t where c in(5,20,10) lock in share mode; 在查找c=5的时候，先锁住了(0,5]。但是因为c不是唯一索引，为了确认还有没有别的记录c=5，就要向右遍历，找到c=10才确认没有了，这个过程满足优化2，所以加了间隙锁(5,10)。 同样的，执行c=10这个逻辑的时候，加锁的范围是(5,10] 和 (10,15)；执行c=20这个逻辑的时候，加锁的范围是(15,20] 和 (20,25)。4 通过这个分析，我们可以知道，这条语句在索引c上加的三个记录锁的顺序是：先加c=5的记录锁，再加c=10的记录锁，最后加c=20的记录锁。 需要注意的 select id from t where c in(5,20,10) order by c desc for update; 间隙锁虽然不互斥间隙锁，但是细化到记录上还是会出现互斥的。问题是in条件的加锁时一个个去扫描的。所以两个sql语句在并发时有可能会出现死锁 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:12:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"怎么查看死锁 下面是间隙锁案例10出现死锁情况 在出现死锁后，执行show engine innodb status命令得到的部分输出。这个命令会输出很多信息，有一节LATESTDETECTED DEADLOCK，就是记录的最后一次死锁信息。 这个结果分成三部分： TRANSACTION，是第一个事务的信息； TRANSACTION，是第二个事务的信息； WE ROLL BACK TRANSACTION (1)，是最终的处理结果，表示回滚了第一个事务。 第一个事务的信息中： WAITING FOR THIS LOCK TO BE GRANTED，表示的是这个事务在等待的锁信息； index c of table test.t，说明在等的是表t的索引c上面的锁； lock mode S waiting 表示这个语句要自己加一个读锁，当前的状态是等待中； Record lock说明这是一个记录锁； n_fields 2表示这个记录是两列，也就是字段c和主键字段id； 0: len 4; hex 0000000a; asc ;;是第一个字段，也就是c。值是十六进制a，也就是10； 1: len 4; hex 0000000a; asc ;;是第二个字段，也就是主键id，值也是10； 这两行里面的asc表示的是，接下来要打印出值里面的“可打印字符”，但10不是可打印字符，因此就显示空格。 第一个事务信息就只显示出了等锁的状态，在等待(c=10,id=10)这一行的锁。 当然你是知道的，既然出现死锁了，就表示这个事务也占有别的锁，但是没有显示出来。别着急，我们从第二个事务的信息中推导出来。 第二个事务显示的信息要多一些： “ HOLDS THE LOCK(S)”用来显示这个事务持有哪些锁； index c of table test.t 表示锁是在表t的索引c上； hex 0000000a和hex 00000014表示这个事务持有c=10和c=20这两个记录锁； WAITING FOR THIS LOCK TO BE GRANTED，表示在等(c=5,id=5)这个记录锁。 从上面这些信息中，我们就知道： “lock in share mode”的这条语句，持有c=5的记录锁，在等c=10的锁； “for update”这个语句，持有c=20和c=10的记录锁，在等c=5的记录锁。 因此导致了死锁。这里，我们可以得到两个结论： 由于锁是一个个加的，要避免死锁，对同一组资源，要按照尽量相同的顺序访问； 在发生死锁的时刻，for update 这条语句占有的资源更多，回滚成本更大，所以InnoDB选择了回滚成本更小的lock in share mode语句，来回滚。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:13:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"怎么看锁等待 可以看到，由于session A并没有锁住c=10这个记录，所以session B删除id=10这一行是可以的。但是之后，session B再想insert id=10这一行回去就不行了。 show engine innodb status index PRIMARY of table test.t ，表示这个语句被锁住是因为表t主键上的某个锁。 lock_mode X locks gap before rec insert intention waiting 这里有几个信息： insert intention表示当前线程准备插入一个记录，这是一个插入意向锁。为了便于理解，你可以认为它就是这个插入动作本身。 gap before rec 表示这是一个间隙锁，而不是记录锁。 那么这个gap是在哪个记录之前的呢？接下来的0~4这5行的内容就是这个记录的信息。 n_fields 5也表示了，这一个记录有5列： 0: len 4; hex 0000000f; asc ;;第一列是主键id字段，十六进制f就是id=15。所以，这时我们就知道了，这个间隙就是id=15之前的，因为id=10已经不存在了，它表示的就是(5,15)。 1: len 6; hex 000000000513; asc ;;第二列是长度为6字节的事务id，表示最后修改这一行的是trx id为1299的事务。 2: len 7; hex b0000001250134; asc % 4;; 第三列长度为7字节的回滚段信息。可以看到，这里的acs后面有显示内容(%和4)，这是因为刚好这个字节是可打印字符。 后面两列是c和d的值，都是15。 因此，我们就知道了，由于delete操作把id=10这一行删掉了，原来的两个间隙(5,10)、(10,15）变成了一个(5,15)。 session A执行完select语句后，什么都没做，但它加锁的范围突然“变大”了； 当我们执行select * from t where c\u003e=15 and c\u003c=20 order by c desc lock in share mode; 向左扫描到c=10的时候，要把(5, 10]锁起来。 也就是说，所谓“间隙”，其实根本就是由“这个间隙右边的那个记录”定义的。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:14:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"update间隙锁变大的例子 session A的加锁范围是索引c上的 (5,10]、(10,15]、(15,20]、(20,25]和(25,suprenum]。 之后session B的第一个update语句，要把c=5改成c=1，你可以理解为两步： 插入(c=1, id=5)这个记录； 删除(c=5, id=5)这个记录。 按照我们上面说的，索引c上(5,10)间隙是由这个间隙右边的记录。所以通过这个操作，session A间隙锁范围变成下层 接下来session B要执行 update t set c = 5 where c = 1这个语句了，一样地可以拆成两步： 插入(c=5, id=5)这个记录； 删除(c=1, id=5)这个记录。 第一步尝试在已经加了间隙锁的(1,10)中插入数据，所以就被堵住了。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:15:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"kill锁住的线程 select * from t where id=1; # 长时间没返回 分析语句之前 都应该执行show processlist;查看 后续操作找到了blocking_id kill掉 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:16:0","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"长时间没返回 如果:performance_schema=on 打开着可以通过select blocking_pid from sys.schema_table_lock_waits; ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:16:1","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"行锁 select * from t sys.innodb_lock_waits where locked_table=`'test'.'t' ## 查询指定表里面的情况 select * from t where c=5 for update 当级别为RR时，是可以解决幻读的，此时对于每条记录的间隙还要加上GAP锁。也就是说，表上每一条记录和每一个间隙都锁上了。 但是实际上 innodb先锁全表的所有行 返回server层,InnoDB就会把不满足条件的行行锁去掉。所以语句执行完只会锁c=5的行。 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:16:2","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["MySQL性能调优"],"content":"小总结 表锁是主动在SQL 上面加的 MDL锁时自动加比如改变表结构 如果在一个事务内，更新的数据条数过多。建议分开更新，因为在你事务开启时，其他session的查询都会查找你的undo log链条，如果你的链条太长会导致很慢。 (DML、DDL语句时都会申请MDL锁，DML操作需要MDL读锁，DDL操作需要MDL写锁) 增删改查，或者修改表之类的语句都会申请MDL锁，而增删改查 会自动加上MDL读锁。修改表会加上写锁 ","date":"2021-01-03","objectID":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/:16:3","tags":["MySQL","间隙锁案例"],"title":"MySQL-锁-间隙锁案例篇","uri":"/mysql-%E9%94%81-%E9%97%B4%E9%9A%99%E9%94%81%E6%A1%88%E4%BE%8B%E7%AF%87/"},{"categories":["并发编程学习"],"content":"LongAdder 更适合做全局计数统计。而AtomicLong适合做自增，因为他能累加并实时返回值，而LongAdder统计的值时近实时。 AtomicLong 由于一直通过cas自旋修改值，在高并发情况下会有很多线程处于自旋，而LongAdder 利用空间换时间的方式把要增加的值先放另一边。 ","date":"2020-09-29","objectID":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:0:0","tags":["LongAdder"],"title":"LongAdder源码学习","uri":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"分析 ","date":"2020-09-29","objectID":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:1:0","tags":["LongAdder"],"title":"LongAdder源码学习","uri":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"cell @sun.misc.Contended static final class Cell { // 由于数组对象volatile 修饰，将只对数组对象可见，所以在cell 里面增加volatile修饰value volatile long value; Cell(long x) { value = x; } final boolean cas(long cmp, long val) { return UNSAFE.compareAndSwapLong(this, valueOffset, cmp, val); } // Unsafe mechanics private static final sun.misc.Unsafe UNSAFE; private static final long valueOffset; static { try { // 返回对象的value在内存中的偏移量 UNSAFE = sun.misc.Unsafe.getUnsafe(); Class\u003c?\u003e ak = Cell.class; valueOffset = UNSAFE.objectFieldOffset (ak.getDeclaredField(\"value\")); } catch (Exception e) { throw new Error(e); } } } 因为由于驻留在数组中的原子对象往往彼此相邻,如果一个对象出现缓存行失效，将会影响另外的对象的缓存刷新。 @sun.misc.Contended注解 消除了伪共享，由于缓存行一般是64k 这个注解通过填充对象大小来让缓存行只存在一个cell对象。 伪共享指的是多个线程同时读写同一个缓存行的不同变量时导致的 CPU缓存失效。尽管这些变量之间没有任何关系，但由于在主内存中邻近，存在于同一个缓存行之中，它们的相互覆盖会导致频繁的缓存未命中，引发性能下降 ","date":"2020-09-29","objectID":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:1:1","tags":["LongAdder"],"title":"LongAdder源码学习","uri":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"add() public void add(long x) { Cell[] as; long b, v; int m; Cell a; // 判断cells是否等于null 如果不等于null 存在cells数组就会进入 // 如果前面条件不满足：会执行cas操作base 存在竞争会进入 if ((as = cells) != null || !casBase(b = base, b + x)) { // 用于标识是否存在竞争 false 标识有竞争 boolean uncontended = true; if (as == null || (m = as.length - 1) \u003c 0 || // 算下标 找个cell (a = as[getProbe() \u0026 m]) == null || // cas 写cell !(uncontended = a.cas(v = a.value, v + x))) // cells没初始化 || 数组对象下标不存在 || 初始化了更改数组对象值失败 longAccumulate(x, null, uncontended); } } ","date":"2020-09-29","objectID":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:1:2","tags":["LongAdder"],"title":"LongAdder源码学习","uri":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["并发编程学习"],"content":"longAccumulate() 如果cells 存在 那么尝试写入数组对象值 如果 不存在 就初始化 如果 初始化 加锁失败 就再次 写base。 final void longAccumulate(long x, LongBinaryOperator fn, boolean wasUncontended) { int h;//通过线程引用拿到一个hash值 主要用于找下标 if ((h = getProbe()) == 0) { ThreadLocalRandom.current(); // force initialization h = getProbe(); wasUncontended = true; } // 扩容意向 true 可能扩容，false不会扩容 boolean collide = false; // True if last slot nonempty for (;;) { Cell[] as; Cell a; int n; long v; // cells 存在 可以先尝试写到cells数组内 if ((as = cells) != null \u0026\u0026 (n = as.length) \u003e 0) { // 如果找到的下标为空 ，尝试增加一个cell对象到数组 if ((a = as[(n - 1) \u0026 h]) == null) { if (cellsBusy == 0) { // Try to attach new Cell Cell r = new Cell(x); // Optimistically create // 尝试获取标记锁 将cellsBusy改为1 if (cellsBusy == 0 \u0026\u0026 casCellsBusy()) { boolean created = false; try { // Recheck under lock Cell[] rs; int m, j; // 这里再次判断了下标值 是否存在 以防其他线程已经初始化了,因为上层if判断没有判断这个 if ((rs = cells) != null \u0026\u0026 (m = rs.length) \u003e 0 \u0026\u0026 rs[j = (m - 1) \u0026 h] == null) { rs[j] = r; created = true; } } finally { cellsBusy = 0; } if (created) break; continue; //如果失败 再次循环 // Slot is now non-empty } } // 扩容意向改为false // 因为 你的(a = as[(n - 1) \u0026 h]) == null 判断为真 说明你是能写数据进去的不需要扩容。 collide = false; } // 在外面cas cells数组元素值 失败进来的时候是false // 会执行到h = advanceProbe(h); 重置hash else if (!wasUncontended) // CAS already known to fail wasUncontended = true; // Continue after rehash // 重置 hash 过后 再来cas 数组元素一次 // !! 还有一点就是下面条件不会 break说明都会执行h = advanceProbe(h);都会来尝试cas else if (a.cas(v = a.value, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // 判断数组是否已经不相等了。说明已经扩容了 else if (n \u003e= NCPU || cells != as) collide = false; // At max size or stale // 扩容意向 改为true else if (!collide) collide = true; // 开始扩容 else if (cellsBusy == 0 \u0026\u0026 casCellsBusy()) { try { if (cells == as) { // Expand table unless stale Cell[] rs = new Cell[n \u003c\u003c 1]; // 两倍 for (int i = 0; i \u003c n; ++i) rs[i] = as[i]; cells = rs; } } finally { cellsBusy = 0; } collide = false; continue; // Retry with expanded table } h = advanceProbe(h); } // 初始化cells else if (cellsBusy == 0 \u0026\u0026 cells == as \u0026\u0026 casCellsBusy()) { boolean init = false; try { // Initialize table // 再次加判断 以免覆盖 类似双重检查 // 因为上层判断是分3部分的 并且 赋值cells与cellsBusy也是分开的，难免会出现问题 if (cells == as) { Cell[] rs = new Cell[2]; rs[h \u0026 1] = new Cell(x); cells = rs; // 初始化成功 init = true; } } finally { cellsBusy = 0; } // 成功退出 if (init) break; } // 都不成功上面加锁失败 再次尝试修改base else if (casBase(v = base, ((fn == null) ? v + x : fn.applyAsLong(v, x)))) break; // Fall back on using base } } ","date":"2020-09-29","objectID":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/:1:3","tags":["LongAdder"],"title":"LongAdder源码学习","uri":"/longadder%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"},{"categories":["数据结构与算法"],"content":"HashMap并发修改异常,或者循环时删除异常 ConcurrentModificationException基本都是modCount修改后与原modCount不相等而暴露的异常。很多集合都会有这个问题 编写者抛这个异常的是想 快速失败，暴露异常。让开发者自己解决 当我们在for循环map 时，会编译成迭代器的方式 去运行。而删除我们调用的是hashmap自身的remove方法。 HashMap\u003cInteger, String\u003e host = new HashMap(); host.put(1, \"2\"); host.put(2, \"2\"); host.put(3, \"2\"); Iterator var2 = host.keySet().iterator(); while(var2.hasNext()) { Integer integer = (Integer)var2.next(); if (integer.equals(1)) { host.remove(integer); } } var2.next() 这行代码执行时 会判断count而报错。 解决方法就是 用 迭代器的remove 方法,会在remove时重新给modCount赋值 public final void remove() { Node\u003cK,V\u003e p = current; if (p == null) throw new IllegalStateException(); if (modCount != expectedModCount) throw new ConcurrentModificationException(); current = null; removeNode(p.hash, p.key, null, false, false); expectedModCount = modCount; } 这个不是线程安全的，所以并发还是用ConcurrentHashMap ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:1","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap 底层结构 1.7:数组+链表 (由于是链表长度长了。查询效率不高。所以在设计初衷1.7的hash算法更复杂。数据也更散列) 1.8:数组+链表+红黑树（JDK8中即使用了单向链表，也使用了双向链表，双向链表主要是为了红黑树相关链表操作方便，应该在插入，扩容，链表转红黑树，红黑树转链表的过程中都要操作链表） ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:2","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap 转红黑条件 只有当链表中的元素个数大于8(此时 node有9个)，并且数组的长度大于等于64时才会将链表转为红黑树。 // putVal 片段 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { // 这里放入第九个元素 只有在binCount在第七次并且当前元素下一个元素为null时 p.next = newNode(hash, key, value, null); if (binCount \u003e= TREEIFY_THRESHOLD - 1) // TREEIFY_THRESHOLD=8 treeifyBin(tab, hash); break; } if (e.hash == hash \u0026\u0026 ((k = e.key) == key || (key != null \u0026\u0026 key.equals(k)))) break; p = e; } // 这是在treeifyBin方法 树化前的判断 MIN_TREEIFY_CAPACITY=64 if (tab == null || (n = tab.length) \u003c MIN_TREEIFY_CAPACITY) resize();// 利用扩容来缩减链表长度 ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:3","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap 扩容机制 当前容量大于等于阈值 或 在树化之前，当前数组的长度小于64，链表长度大于等于8 也会发生扩容。 默认 长度是16 阀值是16*0.75 每次扩容时阀值=旧阀值 \u003c\u003c 1 容量=旧容量\u003c\u003c1 也就是两倍 // 部分代码 else if ((newCap = oldCap \u003c\u003c 1) \u003c MAXIMUM_CAPACITY \u0026\u0026 oldCap \u003e= DEFAULT_INITIAL_CAPACITY) newThr = oldThr \u003c\u003c 1; // double threshold 在扩容时通过对新的数组进行与运算新下标 这是下标对应node节点只有一个时 如果node 节点是链表，都会把这个链表拆分成两个高低node节点的链表loHead和hiHead 拆分后在放入数组、这样只需要改变链表头结点的位置就完成转移。 如果是红黑树 也是类似链表 先拆分 在移动只改表头结点的位置。有可能拆分后树太小会转链表 ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:4","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap put过程 final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) { // 进来前已经把key的hash算好了 Node\u003cK,V\u003e[] tab; Node\u003cK,V\u003e p; int n, i; //判断当前hashmap对象是否为空，为空就初始化 if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // 通过与运算 算出下标 并判断这个下标是否存在值 不存在就赋值新值 存在就进入下面else if ((p = tab[i = (n - 1) \u0026 hash]) == null) tab[i] = newNode(hash, key, value, null); else { // 当通过与运算算出下标对应小标存在值 进入 Node\u003cK,V\u003e e; K k; // 如果key相等 说明就是一个值 则覆盖 if (p.hash == hash \u0026\u0026 ((k = p.key) == key || (key != null \u0026\u0026 key.equals(k)))) e = p; else if (p instanceof TreeNode) // 如果节点是树类型 那么直接向红黑树插入值 在putTreeVal的循环中第一次循环就赋值到树中。并且有覆盖的值会返回原有值 e = ((TreeNode\u003cK,V\u003e)p).putTreeVal(this, tab, hash, key, value); else { // 进入这里说明还是链表通过binCount计数从头遍历到尾结点,新kv会封装成node插入到链表尾部 条件满足就树化。 for (int binCount = 0; ; ++binCount) { if ((e = p.next) == null) { p.next = newNode(hash, key, value, null); if (binCount \u003e= TREEIFY_THRESHOLD - 1) // -1 for 1st treeifyBin(tab, hash); break; } if (e.hash == hash \u0026\u0026 ((k = e.key) == key || (key != null \u0026\u0026 key.equals(k)))) break; p = e; } } // ！注意 这里的赋值引用将会影响上面的e相关的赋值 if (e != null) { // existing mapping for key V oldValue = e.value; // 如果设置了onlyIfAbsent为false 说明要替换原有的值 会把旧值的vale赋值给e // 旧值为空 说明 没得覆盖的值将赋值并返回 if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); // 空方法 LinkedHashMap的方法 return oldValue; } } // 修改次数增加 ++modCount; // hashMap的元素个数size加1 如果size大于扩容的阈值，则进行扩容 if (++size \u003e threshold) resize(); afterNodeInsertion(evict);// 空方法 LinkedHashMap的方法 return null; } ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:5","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap 树化 简单分析 final void treeifyBin(Node\u003cK,V\u003e[] tab, int hash) { int n, index; Node\u003cK,V\u003e e; // 是否满足树化条件 if (tab == null || (n = tab.length) \u003c MIN_TREEIFY_CAPACITY) resize(); else if ((e = tab[index = (n - 1) \u0026 hash]) != null) { // 循环节点 把单向链表转为双向链表 TreeNode\u003cK,V\u003e hd = null, tl = null; do { TreeNode\u003cK,V\u003e p = replacementTreeNode(e, null); if (tl == null) hd = p; else { p.prev = tl; tl.next = p; } tl = p; } while ((e = e.next) != null); if ((tab[index] = hd) != null) // 把双向链表 转树 hd.treeify(tab); } } // treeify 具体逻辑 不分析代码了，有点乱 1. 将头结点作为root节点，然后依次将next节点插入到根节点，转变红黑树。 2. 再插入时候key比较 1. 如果key实现了comparable接口，通过实现方式比较 2. 否则比较key的hashCode 3. 否则比较key的class.getName 4. 否则比较key的System.identityHashCode比较 3. 最后树化后，取出root节点（TreeNode），放到entry下标位置 ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:6","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap remove值后退化链表的条件 有两个地方会判断并退化成链表的条件 remove时退化 // 在 removeTreeNode的方法中 // 在红黑树的root节点为空 或者root的右节点、root的左节点、root左节点的左节点为空时 说明树都比较小了 if (root == null || (movable \u0026\u0026 (root.right == null || (rl = root.left) == null|| rl.left == null))) { tab[index] = first.untreeify(map); // too small return; } 在扩容时 low、high 两个TreeNode 长度小于6时 会退化为树。 ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:7","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap 为什么选择红黑树 因为AVL树插入节点或者删除节点，整体的性能是不如红黑树的。AVL每个左右节点的高度是不能大于1的。所以维持这种结构比较消耗性能。主要还是左旋右旋改变与维护AVL高度问题，红黑树比他好的就是只需改变节点颜色就可以了。 二分查找树，他的左右节点不平衡，一开始就固定了root，那么极端的情况下会成为链表结构。 链表长度越长，那么他的插入和查询效率都很低。 而红黑树他的整体查找，增删节点的效率都是比较高的。 ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:8","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["数据结构与算法"],"content":"HashMap 的hash优化 static final int hash(Object key) { int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h \u003e\u003e\u003e 16); } 举个例子 key=\"ykx\" key的hashCode是=119718 这是个10进制数 原始hashCode对应二进制11101001110100110==00000000000000011101001110100110 然后我再向右位移16位后的二进制==00000000000000000000000000000001 异或运算 0000000000000001 1101001110100110 0000000000000000 0000000000000001 0000000000000001 1101001110100111==119719 这个就是最终的hash值 优化点在哪 HashMap在get方法去获取值时是这样的 hash(key)\u0026(n-1) 进行与运算而找到数组位置 0000000000000001 1101001110100111 上面优化后的hash值 hashMap默认数组长度16 0000000000000000 0000000000001111 \u0026 0000000000000000 0000000000000111 =7 这就是最终的位置下标 只要数组长度是2的n次方 你会发现：119719%16与119719\u0026(16-1)的值时相同的 因为与运算的性能会比取模效率高。 总结 其实这里最重要的几个点就是在算hash值时高是16位与低16进行了异或运算（因为很有可能有两个不同的值hash高16位不相同低16位相同）。这样就导致HashMap寻址运算时低16位已经包含了高16位与低16 的特征，因为在寻址的时候大多数都是低16位在运算，因为数组长度-1的数字大小一般情况都比较小。所以在get寻址时基本都是低16在运算，尽量避免hash冲突，寻址时用与运算代替取模运算也是比较大的优化，只要当HashMap的数组长度是2的n次方那么我们算出来的hash值取模这个长度等于与运算这个长度-1 的值。 ","date":"2020-09-05","objectID":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/:0:9","tags":["HashMap","集合","红黑树"],"title":"HashMap 常见问题Java8","uri":"/hashmap-%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98java8/"},{"categories":["JVM"],"content":"类加载过程 public class Test { public static void main(String[] args) { A a = new A(); } } 加载 -\u003e 验证 -\u003e 准备 -\u003e 解析 -\u003e 初始化 -\u003e 使用 -\u003e 卸载 验证：加载类时验证类语法问题，符合JVM规范,并且把需要用到的类同时加载 准备：给类分配内存空间，给变量分配默认值 比如int类型给0 对象给开辟空间 解析：对字节码进行符号转化，符号引用替换为直接引用 初始化：对类里面的值进行赋值，比如给对象赋值new A()-如果有父类还需要先初始化父类; 对静态块变量赋值。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:1","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(1)类加载与垃圾回收","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["JVM"],"content":"类加载器 上面这些过程都需要通过类加载器加载进来 比如我们应用程序要加载一个类：Application ClassLoader会问扩展类加载器 扩展类加载器会问启动类加载器，然后启动类加载器找了自己目录半天没找到这类就会告诉扩展类让他加载，扩展类找了没找到就会让应用程序类加载。也就是先问父类父类加载不了再压到子类 直到加载 类加载器本身就是做的父子关系模型 ,这也是代码设计思想，保证代码的可扩展性,各司其职！顶层类加载器就不会硬编码规定了，任何加载器都可以扩展。 双亲委派机制的一点重要性 双亲委派模型设计的出发点很重要，对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类名称空间。 也就是说:判断2个类是否 “相等”，只有在这2个类是由同一个类加载器加载的前提下才有意义，否则即使这2个类来源于同一个Class文件，被同一个虚拟机加载，只要加载它们的类加载器不同，这2个类必定不相等。 基于双亲委派模型设计，那么Java中基础的类，Object类似Object类重复多次的问题就不会存在了，因为经过层层传递，加载请求最终都会被Bootstrap ClassLoader所响应。加载的Object类也会只有一个，否则如果用户自己编写了一个java.lang.Object类，并把它放到了ClassPath中，会出现很多个Object类，这样Java类型体系中最最基础的行为都无法保证，应用程序也将一片混乱 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:2","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(1)类加载与垃圾回收","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["JVM"],"content":"tomcat的双亲委派机制 Tomcat自定义了Common、Catalina、Shared等类加载器，其实就是用来加载Tomcat自己的一些核心基础类库的。 然后Tomcat为每个部署在里面的Web应用都有一个对应的WebApp类加载器，负责加载我们部署的这个Web应用的类 至于Jsp类加载器，则是给每个JSP都准备了一个Jsp类加载器。 而且大家一定要记得，Tomcat是打破了双亲委派机制的 每个WebApp负责加载自己对应的那个Web应用的class文件，也就是我们写好的某个系统打包好的war包中的所有class文 件，不会传导给上层类加载器去加载。 打破双亲委派的原因 tomcat中的需要支持不同web应用依赖同一个第三方类库的不同版本，jar类库需要保证相互隔离； 同一个第三方类库的相同版本在不同web应用可以共享 tomcat自身依赖的类库需要与应用依赖的类库隔离 jsp需要支持修改后不用重启tomcat即可生效 为了上面类加载隔离 和类更新不用重启，定制开发各种的类加载器 怎么让别人不能反编译你的代码 在对字节码进行加密，然后自定义类加载器来解密 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:3","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(1)类加载与垃圾回收","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["JVM"],"content":"JVM内存划分 还有一个区域，是不属于JVM的，通过NIO中的allocateDirect这种API，可以在Java堆外分配内存空间。然后，通过Java虚拟机里的DirectByteBuffer来引用和操作堆外内存空间。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:4","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(1)类加载与垃圾回收","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["JVM"],"content":"怎么判定对象是否可以回收 可达性分析 在一个方法中创建了一个对象，然后有一个局部变量引用了这个对象，这种情况是最常见的 此时main()方法的栈帧入栈，然后调用\"loadReplicasFromDisk()“方法，栈帧入栈，接着让局部变量 “replicaManager\"引用堆内存里的\"ReplicaManager\"实例对象 当回收线程分析时 局部变量就是可以作为GC Roots的只要一个对象被局部变量引用了，那么就说明他有一个GC Roots，此时就不能被回收了。（其实就是栈中变量表引用的） 同理在JVM的规范里，静态变量也可以看做是一种GC Roots，此时只要一个对象被GC Roots引用了，就不会去回收他。(其实就是方法区引用的) Native方法引用的也可以 首先该类的所有实例（堆中）都已经被回收；其次该类的ClassLoader已经被回收；最后，对该类对应的Class对象 没有任何引用 为什么没有用引用计数器的问题 Step1：GcObject实例1的引用计数加1，实例1的引用计数=1； Step2：GcObject实例2的引用计数加1，实例2的引用计数=1； Step3：GcObject实例2的引用计数再加1，实例2的引用计数=2； Step4：GcObject实例1的引用计数再加1，实例1的引用计数=2； 执行到Step 4，则GcObject实例1和实例2的引用计数都等于2。 Step5：栈帧中obj1不再指向Java堆，GcObject实例1的引用计数减1，结果为1； Step6：栈帧中obj2不再指向Java堆，GcObject实例2的引用计数减1，结果为1。 到此，发现GcObject实例1和实例2的计数引用都不为0，那么如果采用的引用计数算法的话，那么这两个实例所占的内存将得不到释放，这便产生了内存泄露。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:5","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(1)类加载与垃圾回收","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["JVM"],"content":"Java中对象不同的引用类型 弱引用：就跟没引用是类似的，如果发生垃圾回收，就会把这个对象回收掉。 强引用：强引用就是代表绝对不能回收的对象， 软引用：就是说有的对象可有可无，如果内存实在不 够了，可以回收他。 虚引用，很少用，暂时忽略。 finalize() 假设有一个Kafka对象要被垃圾回收了，那么假如这个对象重写了Object类中的finialize()方法 此时会先尝试调用一下他的finalize()方法，看是否把自己这个实例对象给了某个GC Roots变量，比如说代码中就给了kafka类的静态变量。 如果重新让某个GC Roots变量引用了自己，那么就不用被垃圾回收了。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/:0:6","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(1)类加载与垃圾回收","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-1%E7%B1%BB%E5%8A%A0%E8%BD%BD%E4%B8%8E%E5%9E%83%E5%9C%BE%E5%9B%9E%E6%94%B6/"},{"categories":["JVM"],"content":"ParNew Serial 与 Serial old 单线程的就先不分析学习了 到底什么时候会尝试触发Minor GC 一般年轻代内部内存比例是8:1:1 当Eden区满了会触发minor gc把活下来的对象放入剩余的s区中。 触发Minor GC之前会如何检查老年代大小，涉及哪几个步骤和条件 判断老年代剩余空间是否大于新生代所有对象内存空间。 如果HandlePromotionFailure 这个参数设置后会进入下一步判断，老年代剩余大小是否大于之前minor gc进入老年代的平均大小(担保) 如果上面判断失败了或者这个参数没设置，会直接进入full gc 如果参数设置了，条件成立后，会冒险尝试minor gc ，如果年轻代放得下就放年轻代，不然就放老年代，如果都放不下 就会执行full gc ，这次full gc 会连带新生代一起进行垃圾回收，如果还不行就OOM了。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:1:0","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"什么时候在Minor GC之前就会提前触发一次Full GC？ 当老年代剩余大小小于年轻代大小，并且没有设置HandlePromotionFailure时会触发full gc，如果设置了，老年代剩余大小小于年轻代以往GC大小。 Full GC的算法是什么 标记后清理 然后整理 内存块。 Minor GC过后可能对应哪几种情况 剩余S区能放下就放，不能放下就进入老年代，老年代不能放下就full gc。 哪些情况下Minor GC后的对象会进入老年代 对象年龄大于15 对象大小大于PretenureSizeThreshold这个参数设置的单个对象大小 年龄1+年龄2+年龄n的多个年龄对象总和超过了Survivor区 域的50%，此时就会把年龄n及以上的对象都放入老年代 同龄对象大小超过 Survivor某区内存的一半,那就把这个年龄以上的对象送到老年代。 采用parnew+cms垃圾回收器如何只做ygc,零full gc 和垃圾收集器没有什么关系，不同垃圾收集器，差别只在于性能和吞吐量的区别。并不影响垃圾回收时机。 根据堆中对象生存周期特点，合理分配eden s0 s1 大小，尽量让对象在新生代就被回收，需要注意动态年龄判断、 内存担保1.6后默认开启 永久代 大家现在既然都知道了，Full GC有上述几个触发条件，同时触发Full GC的时候其实会带上针对新生代的Young GC，也会有针对老年 代的Full GC，还会有针对永久代的GC。所以假如存放类信息、常量池的永久代满了之后，就会触发一次Full GC。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:1:1","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"CMS老年代垃圾回收器 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:2:0","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"CMS 初始标记 这一阶段会停止一切工作线程，“Stop the World”，但是会很快，仅仅标记GC Roots直接引用的对象。 并发标记 这个阶段会很慢，最耗时，因为是对老年代所有的GC Roots进行直接或间接的跟踪标记，由于是并行的，不影响系统。 重新标记 为了保证重新标记阶段耗时尽可能的变短,再重新标记前增加个并发预清理阶段： 另外为了防止并发预清理阶段等太久都不发生young gc，提供了CMSMaxAbortablePrecleanTime 参数来设置等待多久没有等到young gc就强制remark。默认是5s 但是最终一劳永逸的办法是，添加参数CMSScavengeBeforeRemark，在重新标记前强制YGC 这一阶段会停止一切工作线程，“Stop the World”，但是也是会很快，因为仅仅标记前面两个阶段遗留下来的对象。 并发清理 这个阶段会很慢，因为需要进行对象的清理，但是他也是跟系统程序并发运行的，所以其实也不影响系统程序的执行。但是消耗cpu 浮动垃圾问题 当在并发清理的时候可能会出现minor gc 放入一些对象进入老年代。这些对象是没标记到的，所以清理不了。所以在CMS回收期间会预留一点内存空间出来，当老年代内存占用空间比例大于某个比例时就直接进行full GC,可以通过“-XX:CMSInitiatingOccupancyFraction” 这个设置比例。 JDK 1.6里面默认的值是 92%。 也就是说，老年代占用了92%空间了，就自动进行CMS垃圾回收，预留8%的空间给并发回收期间，系统程序把一些新对象放入老年代 中。 但是，当这8%都不够放入回收期间的对象时，会发生Concurrent Mode Failure，此时就会自动用“Serial Old”垃圾回收器替代CMS，就是直接强行把系统程序“Stop the World”，重新进行长时间的GC Roots追 踪，标记出来全部垃圾对象，不允许新的对象产生，然后回收再恢复。 如果还是放不下 就会OOM了。 所以在生产实践中，这个自动触发CMS垃圾回收的比例需要合理优化一下老年代在并发清理期间有多少对象进入老年代，避免“Concurrent Mode Failure”问题。 整理内存碎片阶段 由于标记清理过后会有很多内存碎片，内存碎片放不下新进来的对象，就对导致频繁的full gc。 CMS有一个参数是“-XX:+UseCMSCompactAtFullCollection”，默认就打开了，意思是每次full gc 过后再次进行STW,进行碎片整理， “-XX:CMSFullGCsBeforeCompaction“ 这个参数是多少次full gc 过后进行整理，默认为0，每次都会进行整理。 CMS为啥老年代的Full GC要比新生代的Minor GC慢很多倍，一般在10倍以上 minor gc年轻代，产生的对象一般GC roots引用都不长，好找，并且是一次性回收。存活对象少，迁移内存很快，然后一次性清理垃圾对象，这个速度就是快 full gc 并发标记阶段要追踪所有GCroots 对象的引用，并发清理的时候标记的对象分散在不同的地方，最后完事了还要把碎片整理在一块。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:2:1","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"总结老年代触发GC 的几次时机 老年代可用内存小于新生代全部对象的大小，如果没开启空间担保参数，会直接触发Full GC，所以一般空间担保参数都会打开； 开启空间担保情况下老年代剩余空间小于历次新生代进入老年代对象的平均大小。此时会体检full gc。 新生代minor gc后存活的大对象放不进 Survivor，会直接进入老年代，而老年代此时如果放不下，会进入full gc。 在CMS垃圾算法下：-XX:CMSInitiatingOccupancyFraction 这个参数设置的比例超出了，也会直接进入Full gc。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:2:2","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"总体应该怎么进行优化 预估系统需要多少内存，老年代新生代内存比例，结合业务预估对象存活时间。 根据minor gc多久运行一次，每次有多少对象进入s区，对象需要存活多久。来分配新生代区域。 每次回收新生代后活下来的对象要小于s区一半，这样合理规避动态年龄判断。 合理设置到老年代的晋升次数，让常驻对象尽快进入老年代，以免留在新生代占用空间。 注意空间担保打开。(1.6后默认打开了) 根据业务预估内存比例与对象存活时间，才能合理优化新生代gc次数，才能尽可能少老年代gc次数。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:2:3","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"频繁FGC的可能性 内存分配不合理，导致对象频繁进入老年代，进而引发频繁的Full GC； 存在内存泄漏等问题，就是内存里驻留了大量的大对象塞满了老年代，导致稍微有一些对象进入老年代就会引发Full GC； 永久代里的类太多，触发了Full GC System.gc() 导致 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/:2:4","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(2)ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-2parnew-cms/"},{"categories":["JVM"],"content":"场景 500万日活用户，每个用户平均访问20来次的一个系统，按10%的付费转化率来计算，每天应该产生50万订单，50万订单一般集中在4小时，平均下来每秒几十个订单。 当在高峰期的时候每秒有1000个下单请求：按3台机器算，每台机器抗300个请求左右。机器4核8G。 估算内存 每个订单按1kb，300个订单300kb 算上订单连带对象 订单库存等，放大20倍 查询对象再放大10倍 300kb*20*10=60mb 对象一秒后消失 系统给JVM内存是4G 我们分配给堆新生代1.5g 老年代1.5g 然后再给永久代256M内存,基本上这4G内存就差不多了,其他给JVM其他。 “-Xms3072M -Xmx3072M -Xmn1536M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M” Minor GC 一秒60M那么我们差不多在25秒的时候就会占满新生代： 因为JDK1.6以后默认开启空间担保了的。主要就是比较 “老年代可用空间大小”和“历次Minor GC后进入老年代对象的平均大小”，刚开始肯定这个检查是可以通过的 由于设置了新生代各比例8:1:1将会在20秒的时候进入minor gc 过后会存在100m左右垃圾 -Xms3072M -Xmx3072M -Xmn1536M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M - XX:SurvivorRatio=8 由于s区每个区域150m 每次垃圾回收会有100m进入过来 由于动态年龄规划问题，这些对象会直接进入老年代。 调整新生代2g 老年代1g 这时同年龄问题已经解决 “-Xms3072M -Xmx3072M -Xmn2048M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M - XX:SurvivorRatio=8” Full GC 由于有的对象还是会逃过15次年轻代GC进入老年代比如controller 得类等等需要长期存活的类 这个时候我们可以提前让这些对象进入老年代。让他们尽快不占用新生代内存。 “-Xms3072M -Xmx3072M -Xmn2048M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M - XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=5” 大对象问题 由于这系统大对象会比较少，我们设置1M “-Xms3072M -Xmx3072M -Xmn2048M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M - XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=5 -XX:PretenureSizeThreshold=1M” 大促时多久会执行一次Full GC 基本可以确认默认开启了空间担保参数。此外 现在 新生代 s1和s2区是200m 此外就是Minor GC过后可能存活的对象超过200MB放不下Survivor了，或者是一下子占到超过Surviovr的50%，此时 会有一些对象进入老年代中。 但是我们之前对新生代的JVM参数进行优化，就是为了避免这种情况，经过我们的测算，这种概率应该是很低的。 但是虽说是很低，也不能完全是是没有这种情况，比如某一次GC过后可能刚好机缘巧合有超过200MB对象，就会进入 老年代里。我们可以做一个假设，大概就是这个订单系统在大促期间，每隔5分钟会在Minor GC之后有一小批对象进入老年代， 大概200MB左右的大小。 每次Minor GC之前，都检查一下“老年代可用内存空间” \u003c “历次Minor GC后升入老年代的平均对象大小” 设置了“-XX:CMSInitiatingOccupancyFaction”参数，比如设定值为92%，那么此时可能前面几个条件都没满 足，但是刚好发现这个条件满足了，比如就是老年代空间使用超过92%了，此时就会自行触发Full GC 可以大概估算一哈 差不多40分钟左右才有可能会产生full GC 目前就先估算 经过前面的推算，我们基本可知道，老年代大概有900MB的对象了，剩余可用空间仅仅只有100MB了，此时就会触发一次Full GC，这个时候新生代正好有200M进入过来。这种可能 但是概率相对较少。 “-Xms3072M -Xmx3072M -Xmn2048M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=5 -XX:PretenureSizeThreshold=1M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFaction=92 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0” UseCMSCompactAtFullCollection 与 CMSFullGCsBeforeCompaction 是搭配使用的；前者目前默认就是true了CMSFullGCsBeforeCompaction 说的是，到底还要再执行多少次full GC才会做压缩。默认是0. 指定垃圾回收器 “-Xms3072M -Xmx3072M -Xmn2048M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M - XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=5 -XX:PretenureSizeThreshold=1M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC” 总结 根据业务合理分配老年代新生代比例。优化之前尽量去估算一下内存大概多少。根据新生代每次产生对象与GC后剩余对象来估算老年代存储大小。Full GC优化的前提是Minor GC的优化，Minor GC的优化的前提是合理分配内存空间，合理分 配内存空间的前提是对系统运行期间的内存使用模型进行预估 优化思路其实简单来说就一句话：尽量让每次Young GC后的存活对象小于Survivor区域的50%， 都留存在年轻代里。尽量别让对象进入老年代。尽量减少Full GC的频率，避免频繁Full GC对JVM性能的影响。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-3parnew-cms-%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%E5%AE%9E%E6%88%98/:0:0","tags":["jvm","垃圾回收","ParNew","CMS"],"title":"JVM调优学习之旅-(3)ParNew+CMS 案例模拟实战","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-3parnew-cms-%E6%A1%88%E4%BE%8B%E6%A8%A1%E6%8B%9F%E5%AE%9E%E6%88%98/"},{"categories":["JVM"],"content":"G1垃圾回收器介绍 G1垃圾回收器在我们设置堆内存大小后，JVM启动在默认情况下把堆内存拆分为大小均等的region具体大小是—堆内存/2048。region大小是2的倍数。 可以通过 “-XX:G1HeapRegionSize” 设置region大小。 region 数量是动态的。初始的时候新生代region只有5%,会随着新生代增加region跟着增加。 垃圾回收后也会减少。默认新生代的占比不会超过60%，可以通过**“- XX:G1MaxNewSizePercent”**设置 新生代 新生代也是有eden区 s1，s2 的区别的他们的数量也是根据region的增加而增加，也是根据一定比例来决定的。“-XX:SurvivorRatio=8” 什么时候进入老年代 当年龄到了会进入老年代，年龄设置**“-XX:MaxTenuringThreshold”** 默认15 动态年龄规划或某一次发生GC后存货对象大于s区一半。 大对象去哪了 G1 会有专门的region 存放大对象，甚至一个大对象可以放几个region。由于新生代region是逐渐变大，所以大对象是在还没分配新生代的时候，region 就是用来存放大对象的。新生代 老年代GC 都会来回收大对象。 G1垃圾回收的过程 初始标记：先STW,然后对各个线程栈内存中的局部变量代表的GC Roots，以及方法区中的类静态变量代表的GC Roots，进行扫描，标记出来他们直接引用的那些对象。 并发标记：通过gc roots 追踪全部对象。并且会标记在这期间有哪些对象修改。(比如说哪个对象被新建了，哪个对象失去了引用) 最终标记阶段：先STW，但是会根据并发标记 阶段记录的 那些对象修改，最终标记一下有哪些存活对象，有哪些是垃圾对象。 混合回收阶段：会STW,这个阶段会计算老年代中每个Region中的存活对象数量，存活对象的占比，还有执行垃圾回 收的预期性能和效率。然后在限定的时间范围内尽可能回收对象。会选择部分Region进行回收，因为必须让垃圾回收的停顿时间控制在我 们指定的范围内。 public class Kafka { public static ReplicaManager replicaManager = new ReplicaManager(); //第一阶段标记直接引用与对应的ReplicaManager类 } public class ReplicaManager { // 第二阶段标记通过gc roots replicaManager 追踪 replicaFetcher这个实例变量 发现引用了 ReplicaFetcher对象 public ReplicaFetcher replicaFetcher = new ReplicaFetcher(); } 什么时候自动触发新生代+老年代的混合垃圾回收 G1有一个参数，是“-XX:InitiatingHeapOccupancyPercent”，他的默认值是45%；当老年代占据堆内存45% 就会尝试触发一个新生代+老年代一起回收的混合回收阶段。 又称Mixed GC 混合回收过程 由于混合回收由于停顿时间的原因，会有多次。有一些参数可以控制这个，比如**“-XX:G1MixedGCCountTarget”**参数，就是在一次混合回收的过程中，最后一个阶段执行几次混合 回收，默认值是8次 因为你停止系统一会儿，回收掉一些Region，再让系统运行一会儿，然后再次停止系统一会儿，再次回收掉一些Region，这样可以尽 可能让系统不要停顿时间过长，可以在多次回收的间隙，也运行一下。 还有一个参数，就是**“-XX:G1HeapWastePercent”**，默认值是5% 他的意思就是说，在混合回收的时候，对Region回收都是基于复制算法进行的，都是把要回收的Region里的存活对象放入其他 Region，然后这个Region中的垃圾对象全部清理掉。一旦空闲出来的Region数量达到了堆内存的5%，此时就会 立即停止混合回收，意味着本次混合回收就结束了。 而且从这里也能看出来G1整体是基于复制算法进行Region垃圾回收的，不会出现内存碎片的问题，不需要像CMS那样标记-清理之 后，再进行内存碎片的整理。 还有一个参数，“-XX:G1MixedGCLiveThresholdPercent”，他的默认值是85%，意思就是确定要回收的Region的时候，必须是存 活对象低于85%的Region才可以进行回收 否则要是一个Region的存活对象多余85%，你还回收他干什么？这个时候要把85%的对象都拷贝到别的Region，这个成本是很高的。 回收失败 由于是给予复制算法进行回收，那么再拷贝的过程发现存活的对象没有地方放得下，那么会触发失败，会停止系统程序，然后采用单线程标记算法来回收。很慢。 从上面可以看出G1非常适合大内存的系统和延时要求较小的系统，例如实时通信，因为parnew垃圾回收器如果是大内存的话，在垃圾回收期间，会有很长的停顿时间，而G1可以控制每次回收停顿时间。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-4g1/:0:1","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(4)G1","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-4g1/"},{"categories":["JVM"],"content":"G1垃圾回收模拟分析 一个在线教育系统 高峰期在每天晚上 持续2-3小时 估算 3小时 差不多有60万的用户在线 平均一个用户差不多一个小时在线时长 那么每小时大概会有20万活跃用户同时在线学习。 由于有互动效果， 每个用户平均一分钟互动一次 20万用户一小时差不多1200万次互动 那么每秒差不多压力有3000 5台 4c8g的机器 平均每秒可以抗住600请求 所有大致估算一下，一次互动请求大致会连带创建几个对象，占据几KB的内存，比如我们就认为是5KB吧那么一秒600请求会占用3MB左右的内存。 我们采用G1垃圾回收器 4G堆内存 那么假设我们对机器上的JVM，分配4G给堆内存，其中新生代默认初始占比为5%，最大占比为60%，每个Java线程的栈内存为1MB，元数据区域（永久代）的内存为256M，此时JVM参数如下 -Xms4096M -Xmx4096M -Xss1M -XX:PermSize=256M -XX:MaxPermSize=256M -XX:+UseG1GC “-XX:G1NewSizePercent”参数是用来设置新生代初始占比的，不用设置，维持默认值为5%即可。 “-XX:G1MaxNewSizePercent”参数是用来设置新生代最大占比的，也不用设置，维持默认值为60%即可。 此时堆内存共4G，那么此时会除以2048，计算出每个Region的大小，此时每个Region的大小就是2MB， 刚开始新生代就占5%的Region，可以认为新生代就是只有100个Region，有200MB的内存空间 在G1垃圾回收器中有一个至关重要的参数会影响到GC的表现，就是“-XX:MaxGCPauseMills”，他的默认值是200 毫秒 也就是说咱们希望每次触发一次GC的时候导致的系统停顿时间（也就是“Stop the World”）不要超过200毫秒，避 免系统因为GC长时间卡死。 分析 G1会根据停顿时间的长度来估算回收时间，意思就是 每秒创建3M大的新生代 差不多一分钟就塞满200m 这时回收只需要几十毫秒 G1这时是不会回收的 G1可能会凑够差不多200ms才开始回收。 这个是无法精准预测的！只有具体拿工具查看。 停顿时间到底应该怎么设置 设置小了 频繁gc 虽然说时间短，但是会让对象更快进入老年代 会频繁 mixed GC 设置大了 此时可能一次GC停顿时间就会达到几百毫秒，但是GC的频率很低。比如说30分钟才触发一次新生代GC，但是每次停顿500毫秒。 所以这个参数到底如何设置，需要结合后续给大家讲解的系统压测工具、gc日志、内存分析工具结合起来进行考虑，尽量让系统的gc频率别太高，同时每次gc停顿时间也别太长，达到一个理想的合理值 G1与parnew+cms的区别点 g1和pn+cms调优原则都是尽可能ygc，不做老年代gc。 g1相对而言更加智能，也意味着jvm会用更多的资源去判断每个region的使用情况。 而pn+cms也更加纯粹和直接，虽然g1在gc时不会产生碎片，但是由于每个region存在存活率85%不清理的机制，会导致内存没有充分利用与释放问题。 因此，对于cpu性能高的，内存容量大的，对应用响应度高的系统推荐使用g1。 而内存小，cpu性能比较低下的系统也可以使用pn+cms会更合适 parallel scavenge与 parnew两者都是复制算法，都是并行处理，但是不同的是，parallel scavenge 可以设置最大gc停顿时间（-XX:MaxGCPauseMills）以及gc时间占比(-XX:GCTimeRatio) parallel scavenge更关注 吞吐量 ，不过目前系统喜欢吞吐量的基本都用G1了。 G1也可以做到 parallel scavenge+Parallel Old的功能 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-4g1/:0:2","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(4)G1","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-4g1/"},{"categories":["JVM"],"content":"模拟GC ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:1:0","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"JVM参数 -XX:NewSize=5242880 -XX:MaxNewSize=5242880 -XX:SurvivorRatio=8 -XX:InitialHeapSize=10485760 -XX:MaxHeapSize=10485760 -XX:PretenureSizeThreshold=10485760 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc.log 参数说明 -XX:NewSize 初始新生代大小 5M -XX:MaxNewSize 最大新生代大小 5M -XX:SurvivorRatio Eden区与s1 s2 区的比例 -XX:InitialHeapSize 初始堆大小 10M -XX:MaxHeapSize 最大堆大小 10M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC 新生代parnew 老年代 cms -XX:PretenureSizeThreshold=10485760 指定了大对象阈值是10MB -XX:+PrintGCDetils：打印详细的gc日志 -XX:+PrintGCTimeStamps：这个参数可以打印出来每次GC发生的时间 -Xloggc:gc.log：这个参数可以设置将gc日志写入一个磁盘文件 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:1:1","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"示例代码 public class Demo1 { public static void main(String[] args) { byte[] array1 = new byte[1024 * 1024]; array1 = new byte[1024 * 1024]; array1 = new byte[1024 * 1024]; array1 = null; byte[] array2 = new byte[2 * 1024 * 1024]; } } ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:1:2","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"gc日志 Java HotSpot(TM) 64-Bit Server VM (25.231-b11) for windows-amd64 JRE (1.8.0_231-b11), built on Oct 5 2019 03:11:30 by \"java_re\" with MS VC++ 10.0 (VS2010) Memory: 4k page, physical 33359028k(19834688k free), swap 35456180k(14483660k free) CommandLine flags: -XX:InitialHeapSize=10485760 -XX:MaxHeapSize=10485760 -XX:MaxNewSize=5242880 -XX:NewSize=5242880 -XX:OldPLABSize=16 -XX:PretenureSizeThreshold=10485760 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:SurvivorRatio=8 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:-UseLargePagesIndividualAllocation -XX:+UseParNewGC 0.103: [GC (Allocation Failure) 0.103: [ParNew: 3684K-\u003e512K(4608K), 0.0018922 secs] 3684K-\u003e1647K(9728K), 0.0020309 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap par new generation total 4608K, used 3746K [0x00000000ff600000, 0x00000000ffb00000, 0x00000000ffb00000) eden space 4096K, 78% used [0x00000000ff600000, 0x00000000ff928990, 0x00000000ffa00000) from space 512K, 100% used [0x00000000ffa80000, 0x00000000ffb00000, 0x00000000ffb00000) to space 512K, 0% used [0x00000000ffa00000, 0x00000000ffa00000, 0x00000000ffa80000) concurrent mark-sweep generation total 5120K, used 1135K [0x00000000ffb00000, 0x0000000100000000, 0x0000000100000000) Metaspace used 3143K, capacity 4496K, committed 4864K, reserved 1056768K class space used 343K, capacity 388K, committed 512K, reserved 1048576K 前面都是一目了然 我们从第六行开始看 0.103: [GC (Allocation Failure) 0.103: [ParNew: 3684K-\u003e512K(4608K), 0.0018922 secs] 3684K-\u003e1647K(9728K), 0.0020309 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 这是一次GC概要说明： GC (Allocation Failure) 由于分配失败 产生一次gc 系统运行到0.103秒时ParNew 产生一次年轻代gc 年轻代空间4.5M (Eden区是4MB，两个Survivor中只有一个是可以放存活对象的，另外一个是必须一致保持空闲的) 3684K 说明已经使用了多大空间 但是gc 过后 存活下来512k 3684K-\u003e1647K(9728K) 这是整个堆gc后整体情况 gc前占用 3684k gc后还占用 1647k Times: user=0.00 sys=0.00, real=0.00 secs 由于单位是秒 所以忽略为0 了 Heap par new generation total 4608K, used 3746K [0x00000000ff600000, 0x00000000ffb00000, 0x00000000ffb00000) eden space 4096K, 78% used [0x00000000ff600000, 0x00000000ff928990, 0x00000000ffa00000) from space 512K, 100% used [0x00000000ffa80000, 0x00000000ffb00000, 0x00000000ffb00000) to space 512K, 0% used [0x00000000ffa00000, 0x00000000ffa00000, 0x00000000ffa80000) concurrent mark-sweep generation total 5120K, used 1135K [0x00000000ffb00000, 0x0000000100000000, 0x0000000100000000) Metaspace used 3143K, capacity 4496K, committed 4864K, reserved 1056768K class space used 343K, capacity 388K, committed 512K, reserved 1048576K 这是内存结束后的情况 新生代总大小4608k目前还在使用3746k 年轻代目前占用78% 里面包含了2M数组 还有未知对象 s1也占用了一个 老年代也存在1135k对象 。 具体还存在哪些对象后续用工具看清楚,先了解个大概 Metaspace used 3143K, capacity 4496K, committed 4864K, reserved 1056768K class space used 343K, capacity 388K, committed 512K, reserved 1048576K 这里牵涉到了操作系统的虚拟内存的概念！ 首先 Jdk8开始把类的元数据 放到本地内存（native heap），称之为MetaSpace,理论上本地内存剩余多少，MetaSpace就有多大。 而 class space 是元空间内部的。不过设置了Metaspace大小后还是会有溢出情况。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:1:3","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"触发动态年龄判断 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:0","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"JVM参数 其他参数看前一篇解释 -XX:NewSize=10485760 -XX:MaxNewSize=10485760 -XX:InitialHeapSize=20971520 -XX:MaxHeapSize=20971520 -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=10485760 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc1.log -XX:MaxTenuringThreshold=15 年龄15 新生代我们通过“-XX:NewSize”设置为10MB了 然后其中Eden区是8MB，每个Survivor区是1MB，Java堆总大小是20MB，老年代是10MB，大对象必须超过10MB才会直接进入老年 代 但是我们通过“-XX:MaxTenuringThreshold=15”设置了，只要对象年龄达到15岁才会直接进入老年代。 一切准备就绪，先看看我们当前的内存分配情况 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:1","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"代码 public class Demo2 { public static void main(String[] args) { byte[] array1 = new byte[2*1024 * 1024]; array1 = new byte[2*1024 * 1024]; array1 = new byte[2*1024 * 1024]; array1 = null; byte[] array2 = new byte[128 * 1024]; byte[] array3 = new byte[2 * 1024 * 1024]; } } 当main 方法执行到第五行的时候 给array3分配时新生代肯定放不下，前面已经放了2m +2m+2m+128k 此时 eden区可分配剩余1m左右 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:2","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"gc日志 Java HotSpot(TM) 64-Bit Server VM (25.231-b11) for windows-amd64 JRE (1.8.0_231-b11), built on Oct 5 2019 03:11:30 by \"java_re\" with MS VC++ 10.0 (VS2010) Memory: 4k page, physical 33359028k(19075240k free), swap 35456180k(13369076k free) CommandLine flags: -XX:InitialHeapSize=20971520 -XX:MaxHeapSize=20971520 -XX:MaxNewSize=10485760 -XX:MaxTenuringThreshold=15 -XX:NewSize=10485760 -XX:OldPLABSize=16 -XX:PretenureSizeThreshold=10485760 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:SurvivorRatio=8 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:-UseLargePagesIndividualAllocation -XX:+UseParNewGC 0.100: [GC (Allocation Failure) 0.100: [ParNew: 8130K-\u003e642K(9216K), 0.0008885 secs] 8130K-\u003e642K(19456K), 0.0010790 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap par new generation total 9216K, used 3141K [0x00000000fec00000, 0x00000000ff600000, 0x00000000ff600000) eden space 8192K, 30% used [0x00000000fec00000, 0x00000000fee70c60, 0x00000000ff400000) from space 1024K, 62% used [0x00000000ff500000, 0x00000000ff5a08c8, 0x00000000ff600000) to space 1024K, 0% used [0x00000000ff400000, 0x00000000ff400000, 0x00000000ff500000) concurrent mark-sweep generation total 10240K, used 0K [0x00000000ff600000, 0x0000000100000000, 0x0000000100000000) Metaspace used 3230K, capacity 4496K, committed 4864K, reserved 1056768K class space used 350K, capacity 388K, committed 512K, reserved 1048576K 分析 0.100: [GC (Allocation Failure) 0.100: [ParNew: 8130K-\u003e642K(9216K), 0.0008885 secs] 8130K-\u003e642K(19456K), 0.0010790 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 可以看出gc前就已经占用了8130k 垃圾回收后还有642k存在 par new generation total 9216K, used 3141K [0x00000000fec00000, 0x00000000ff600000, 0x00000000ff600000) eden space 8192K, 30% used [0x00000000fec00000, 0x00000000fee70c60, 0x00000000ff400000) from space 1024K, 62% used [0x00000000ff500000, 0x00000000ff5a08c8, 0x00000000ff600000) to space 1024K, 0% used [0x00000000ff400000, 0x00000000ff400000, 0x00000000ff500000) 我们 正常剩余存在的对象应该就是2m+128k左右 由于在128k产生后 分配内存失败引起gc 所以 那一部分去了 s1 也就是日志中的from 后面分配的2048k 去了 eden 区。 此时array2的对象 应该是1岁 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:3","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"更改代码 public class Demo2 { public static void main(String[] args) { byte[] array1 = new byte[2*1024 * 1024]; array1 = new byte[2*1024 * 1024]; array1 = new byte[2*1024 * 1024]; array1 = null; byte[] array2 = new byte[128 * 1024]; byte[] array3 = new byte[2 * 1024 * 1024]; array3=new byte[2*1024*1024]; array3=new byte[2*1024*1024]; array3=new byte[128*1024]; array3=null; byte[] array4 = new byte[2 * 1024 * 1024]; } } 此时的内存分布图 还没执行最后一行 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:4","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"上面代码再次执行gc日志 0.090: [GC (Allocation Failure) 0.090: [ParNew: 8130K-\u003e637K(9216K), 0.0009362 secs] 8130K-\u003e637K(19456K), 0.0011031 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] 0.092: [GC (Allocation Failure) 0.092: [ParNew: 7150K-\u003e366K(9216K), 0.0033473 secs] 7150K-\u003e990K(19456K), 0.0033991 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap par new generation total 9216K, used 2552K [0x00000000fec00000, 0x00000000ff600000, 0x00000000ff600000) eden space 8192K, 26% used [0x00000000fec00000, 0x00000000fee225d0, 0x00000000ff400000) from space 1024K, 35% used [0x00000000ff400000, 0x00000000ff45bb38, 0x00000000ff500000) to space 1024K, 0% used [0x00000000ff500000, 0x00000000ff500000, 0x00000000ff600000) concurrent mark-sweep generation total 10240K, used 623K [0x00000000ff600000, 0x0000000100000000, 0x0000000100000000) Metaspace used 3230K, capacity 4496K, committed 4864K, reserved 1056768K class space used 350K, capacity 388K, committed 512K, reserved 1048576K 从这里可以看出 cms 已经有600多k的老年代了 这600多k其实就是第一次gc的时候进入s区的对象， CMS管理的老年代，此时使用空间刚好是600多k，证明此时Survivor里的对象触发了动态年龄判定规则，虽然没有达到15岁，但是全部进入老年代了。 接着其实此时会发现Survivor区域中的对象都是存活的，而且总大小超过s区50%了，之前对象年龄都是1岁 此时根据动态年龄判定规则：年龄1+年龄2+年龄n的对象总大小超过了Survivor区域的50%，年龄n及以上的对象进入老 年代。 当然这里的对象都是年龄1的，所以老年龄进入老年代了。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:5","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"再次改变代码 public class Demo2 { public static void main(String[] args) { byte[] array1 = new byte[2 * 1024 * 1024]; array1=new byte[2*1024*1024]; array1=new byte[2*1024*1024]; byte[] array2 = new byte[128 * 1024]; array2=null; byte[] array3=new byte[2*1024*1024]; } } ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:6","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"gc日志 CommandLine flags: -XX:InitialHeapSize=20971520 -XX:MaxHeapSize=20971520 -XX:MaxNewSize=10485760 -XX:MaxTenuringThreshold=15 -XX:NewSize=10485760 -XX:OldPLABSize=16 -XX:PretenureSizeThreshold=10485760 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:SurvivorRatio=8 -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:-UseLargePagesIndividualAllocation -XX:+UseParNewGC 0.119: [GC (Allocation Failure) 0.119: [ParNew: 8130K-\u003e659K(9216K), 0.0020862 secs] 8130K-\u003e2709K(19456K), 0.0022548 secs] [Times: user=0.00 sys=0.00, real=0.00 secs] Heap par new generation total 9216K, used 3158K [0x00000000fec00000, 0x00000000ff600000, 0x00000000ff600000) eden space 8192K, 30% used [0x00000000fec00000, 0x00000000fee70c60, 0x00000000ff400000) from space 1024K, 64% used [0x00000000ff500000, 0x00000000ff5a4cc0, 0x00000000ff600000) to space 1024K, 0% used [0x00000000ff400000, 0x00000000ff400000, 0x00000000ff500000) concurrent mark-sweep generation total 10240K, used 2050K [0x00000000ff600000, 0x0000000100000000, 0x0000000100000000) Metaspace used 3230K, capacity 4496K, committed 4864K, reserved 1056768K class space used 350K, capacity 388K, committed 512K, reserved 1048576K 这里出现一个特点 Young GC过后存活对象放不下Survivor区域，从而部分对象会进入老年代 在gc 的时候会发现 有659k未知对象和2M的数组要放进 Survivor 区 放不下 会有部分对象进入老年代。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:2:7","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"fullGc之老年代放不下 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:3:0","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"JVM参数 其他参数看前一篇解释 -XX:NewSize=10485760 -XX:MaxNewSize=10485760 -XX:InitialHeapSize=20971520 -XX:MaxHeapSize=20971520 -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=3145728 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:gc2.log 上面注意一点 大对象给的 3M ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:3:1","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"代码 public class Demo3 { public static void main(String[] args) { byte[] array1 = new byte[4 * 1024 * 1024]; array1=null; byte[] array2 = new byte[2 * 1024 * 1024]; byte[] array3 = new byte[2 * 1024 * 1024]; byte[] array4 = new byte[2 * 1024 * 1024]; byte[] array5 = new byte[128 * 1024]; byte[] array6 = new byte[2 * 1024 * 1024]; } } ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:3:2","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"gc 日志 0.130: [GC (Allocation Failure) 0.130: [ParNew (promotion failed): 8130K-\u003e8803K(9216K), 0.0034184 secs]0.134: [CMS: 8194K-\u003e6774K(10240K), 0.0030527 secs] 12226K-\u003e6774K(19456K), [Metaspace: 3223K-\u003e3223K(1056768K)], 0.0070386 secs] [Times: user=0.00 sys=0.00, real=0.01 secs] Heap par new generation total 9216K, used 2422K [0x00000000fec00000, 0x00000000ff600000, 0x00000000ff600000) eden space 8192K, 29% used [0x00000000fec00000, 0x00000000fee5d898, 0x00000000ff400000) from space 1024K, 0% used [0x00000000ff500000, 0x00000000ff500000, 0x00000000ff600000) to space 1024K, 0% used [0x00000000ff400000, 0x00000000ff400000, 0x00000000ff500000) concurrent mark-sweep generation total 10240K, used 6774K [0x00000000ff600000, 0x0000000100000000, 0x0000000100000000) Metaspace used 3230K, capacity 4496K, committed 4864K, reserved 1056768K class space used 350K, capacity 388K, committed 512K, reserved 1048576K 最后一句代码还没执行时 gc前 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:3:3","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"分析 接着会执行如下代码：byte[] array6 = new byte[2 * 1024 * 1024];。此时，Eden区 已经放不下了。因此此时会直接触发一次Young GC。 我们看下面的GC日志：0.130: [GC (Allocation Failure) 0.130: [ParNew (promotion failed): 8130K-\u003e8803K(9216K), 0.0034184 secs] 这行日志显示了，Eden区原来是有8130K的对象，但是回收之后发现一个都回收不掉，因为上述几个数组都被变 量引用了。 此时，一定会直接把这些对象放入到老年代里去，但是此时老年代里已经有一个4MB的数组了，明显放不下3个2MB的数组和1个128KB的数组； [CMS: 8194K-\u003e6774K(10240K), 0.0030527 secs] 12226K-\u003e6774K(19456K), [Metaspace: 3223K-\u003e3223K(1056768K)], 0.0070386 secs] 此时执行了CMS垃圾回收器的Full GC，Full GC其实就是会对老年代进行Old GC， 同时一般会跟一次Young GC关联，还会触发一次元数据区（永久代）的GC。 这里看到老年代从8MB左右的对象占用，变成了6MB左右的对象占用 为什么？？？ 首先 执行young gc后 放了两个2M的数组进入老年代 ，发现无法继续再次放数据了 这时 执行了 old gc 回收了4m的大对象 因为此时4m的大对象是没引用的,可以回收。 然后把剩余的对象放进老年代 差不多2M+2m+2m+128k和一些未知对象存活在老年代。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/:3:4","tags":["jvm","垃圾回收","G1"],"title":"JVM调优学习之旅-(5)初识gc日志 ParNew+CMS","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-5%E5%88%9D%E8%AF%86gc%E6%97%A5%E5%BF%97-parnew-cms/"},{"categories":["JVM"],"content":"工具介绍 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:1:0","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"jstat jstat -gc PID 参数说明： S0C：这是From Survivor区的大小 S1C：这是To Survivor区的大小 S0U：这是From Survivor区当前使用的内存大小 S1U：这是To Survivor区当前使用的内存大小 EC：这是Eden区的大小 EU：这是Eden区当前使用的内存大小 OC：这是老年代的大小 OU：这是老年代当前使用的内存大小 MC：这是方法区（永久代、元数据区）的大小 MU：这是方法区（永久代、元数据区）的当前使用的内存大小 YGC：这是系统运行迄今为止的Young GC次数 YGCT：这是Young GC的耗时 FGC：这是系统运行迄今为止的Full GC次数 FGCT：这是Full GC的耗时 GCT：这是所有GC的总耗时 常用 jstat -gccapacity PID：堆内存分析 jstat -gcnew PID：年轻代GC分析，这里的TT和MTT可以看到对象在年轻代存活的年龄和存活的最大年龄 jstat -gcnewcapacity PID：年轻代内存分析 jstat -gcold PID：老年代GC分析 jstat -gcoldcapacity PID：老年代内存分析 jstat -gcmetacapacity PID：元数据区内存分析 jstat -gc PID 1000 10 每隔一秒展示10次 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:1:1","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"jmap、jhat jmap -histo PID 可以看到对象内存分部从大到小排序 详细的 可以用 jmap -dump:live,format=b,file=dump.hprof PID 查看堆内存快照 jhat dump.hprof -port 7000 通过web界面查看默认7000端口 视图化分析工具我采用MAT ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:1:2","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"jstat分析优化 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:0","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"模拟代码 public class Demo2 { public static void main(String[] args) throws Exception { Thread.sleep(30000); while (true) { loadData(); } } private static void loadData() throws InterruptedException { byte[] data = null; for (int i = 0; i \u003c 4; i++) { data = new byte[10 * 1024 * 1024]; data = null; byte[] datal = new byte[10 * 1024 * 1024]; byte[] data2 = new byte[10 * 1024 * 1024]; byte[] data3 = new byte[10 * 1024 * 1024]; data3 = new byte[10 * 1024 * 1024]; Thread.sleep(1000); } } } ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:1","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"jvm参数 -XX:NewSize=104857600 -XX:MaxNewSize=104857600 -XX:InitialHeapSize=209715200 -XX:MaxHeapSize=209715200 -XX:SurvivorRatio=8 -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=20971520 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:jstat.log 大对象我设置的20m。避免我们程序里分配的 大对象直接进入老年代 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:2","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"分析 jstat -gc pid 1000 每隔一秒打印分析 日志 可以看到程序在第一次循环放了50M对象进入 此时EU是 57753 而随后执行了一次YGC ，从代码可以看出此时只有10M进入 老年代 第一次循环 50m 第二次循环时：data：10m data1：10m 这时已经 70m了 data2放不下了。执行YGC此时存活下来的就是data1还有引用 随后 data2 data3 总共30m 进入eden区 从这一点 就可以看出 分配很不对了。回收的对象根本放不进 s区 我们再看 FGC规律 老年代总共就100MB左右，已经占用了60MB了，此时如果发生一次Young GC，有30MB存活对象要放入老年代的话，你还要放30MB对象，明显老年代就要不够了，或者老年代存在碎片放不下新进来的对象了，此时进行了Full GC， 可以看到，按照这段代码，几乎是每秒新增80MB左右，触发每秒1次Young GC，每次Young GC后存活下来 20MB~30MB的对象，老年代每秒新增20MB~30MB的对象，触发老年代几乎三秒一次Full GC。 从上图也可以看出每次Full GC都是由Young GC触发的，因为Young GC过后存活对象太多要放入老年代，老年代内存不够了触 发Full GC，所以必须得等Full GC执行完毕了，Young GC才能把存活对象放入老年代，才算结束。这就导致Young GC也是速度非常慢 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:3","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"改进参数 -XX:NewSize=209715200 -XX:MaxNewSize=209715200 -XX:InitialHeapSize=314572800 -XX:MaxHeapSize=314572800 -XX:SurvivorRatio=2 -XX:MaxTenuringThreshold=15 -XX:PretenureSizeThreshold=20971520 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:jstat.log 我们把堆大小调大为了300MB，年轻代给了200MB，同时“-XX:SurvivorRatio=2”表明，Eden:Survivor:Survivor的比例为2:1:1， 所以Eden区是100MB，每个Survivor区是50MB，老年代也是100MB 可以看出我们除了一些未知对象，我们每次YGC的对象都完美进入s区。每次YGC时间基本很短 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/:2:4","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(6)分析工具使用","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-6%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/"},{"categories":["JVM"],"content":"模拟案例背景 很多社交APP,日活百万用户，在晚高峰时，QPS很高。 而流量最大的模块，无非就是陌生人的个人主页，类似朋友圈之类的功能。 我们都知道一般这种肯定设置的是缓存，但是问题是这些数据都是比较大的，比如一个人发布了一些感悟，等等之类的。 这些数据都是比较大的。个人信息页展示的信息也是比较完全的。这些数据我们先预估5M. 用户操作 用户在高峰期玩app的时候一般都会连续不断的点击自己感兴趣的人，然后去查看他的信息，说说之类的。这种可能持续半小时-1小时。 首先，这些服务数据的压力这边就不扯了，无非就是缓存架构。 JVM 首先这种场景，高并发下Eden区会迅速填满。频繁触发YGC，然而这时候太快了，导致很多数据还没处理完毕， 这时就会把大量的数据朝着S区去，但是大概率S区放不下，会直接进入老年代。这样就会导致老年代频繁触发FGC，从而导致app来时卡顿。 优化前参数 针对上述场景，最核心的优化点，主要应该是增加机器，尽量让每台机器承载更少的并发请求，减轻压力。 同时，给年轻代的Survivor区域更大的内存空间，让每次Young GC后的存活对象务必停留在Survior中，别进入老年代 -XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=5 这个JVM参数我们应该关注一下： 对象进入老年代，由于老年代没有进行碎片整理，放不下一些大对象，就会导致更快的FGC，而这两个参数就是在5次FGC后进行整理内存碎片。JVM叫这压缩，Compaction。 所以我们年轻代优化好了，还得根据FGC的次数。看老年代在高峰期FGC的次数,来进行优化这个次数。因为这个整理碎片的过程也是会STW的。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-7p-cms%E6%A8%A1%E6%8B%9F%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E6%80%9D%E6%83%B3-%E6%AF%8F%E7%A7%92%E5%8D%81%E4%B8%87%E7%9A%84qps%E7%A4%BE%E4%BA%A4/:0:1","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(7)P+CMS模拟案例优化分析思想-每秒十万的QPS社交","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-7p-cms%E6%A8%A1%E6%8B%9F%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E6%80%9D%E6%83%B3-%E6%AF%8F%E7%A7%92%E5%8D%81%E4%B8%87%E7%9A%84qps%E7%A4%BE%E4%BA%A4/"},{"categories":["JVM"],"content":"怎么优化 通过jstat分析一下各个机器上的jvm的运行状况，判断出来每次Young GC 后存活对象有多少，然后就是合理增加Survivor区的内存，避免对象快速进入老年代。 另外一个，在当时对那个系统优化之后，增加了年轻代和Survivor区的大小，但还是会慢慢的有对象进入老年代里，毕竟系统负载很高，彻底让对象不进入老年代也很难做到。 在降低了Full GC频率之后，务必设置如下参数“-XX:+UseCMSCompactAtFullCollection - XX:CMSFullGCsBeforeCompaction=0”，每次Full GC后都整理一下内存碎片。这个可以根据高峰期FGC次数权衡。 如果不合理修改这个参数就会导致：每次Full GC过后，都造成老年代里很多内存碎片，那么必然导致下一次Full GC更快到来，因为内存碎片会导致老年代可用内 存变少。也许第一次Full GC是一小时才有，第二次Full GC也许是40分钟之后，第三次Full GC可能就是20分钟之后，要是不解决CMS内存碎片 问题，必然导致Full GC慢慢变得越来越频繁 如何尽可能优化FGC 一个参数是“-XX:+CMSParallelInitialMarkEnabled”， 这个参数会在CMS垃圾回收器的“初始标记”阶段开启多线程并发执行。 大家应该还记得初始标记阶段，是会进行Stop the World的，会导致系统停顿，所以这个阶段开启多线程并发之后，可以尽可能优化 这个阶段的性能，减少Stop the World的时间。 另外一个参数是“-XX:+CMSScavengeBeforeRemark”， 这个参数会在CMS的重新标记阶段之前，先尽量执行一次Young GC。 也就是，CMS的重新标记也是会Stop the World的，所以如果在重新标记之前，先执行一次Young GC，就会回收掉一 些年轻代里没有人引用的对象。 所以如果先提前回收掉一些对象，那么在CMS的重新标记阶段就可以少扫描一些对象，因为老年代对象可能会和年轻代有引用关系，gc roots寻找扫描的路径就变短了，此时就可以提升CMS的重新标记阶段的性能， 减少他的耗时。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-7p-cms%E6%A8%A1%E6%8B%9F%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E6%80%9D%E6%83%B3-%E6%AF%8F%E7%A7%92%E5%8D%81%E4%B8%87%E7%9A%84qps%E7%A4%BE%E4%BA%A4/:0:2","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(7)P+CMS模拟案例优化分析思想-每秒十万的QPS社交","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-7p-cms%E6%A8%A1%E6%8B%9F%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E6%80%9D%E6%83%B3-%E6%AF%8F%E7%A7%92%E5%8D%81%E4%B8%87%E7%9A%84qps%E7%A4%BE%E4%BA%A4/"},{"categories":["JVM"],"content":"参数案例 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:1:0","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"案例1-线上频繁Metadata GC 错误参数 -XX:SoftRefLRUPolicyMSPerMB=0 由于参数设置错误，导致线上gc日志 【Full GC（Metadata GC Threshold）xxxxx, xxxxx】 频繁清空元数据区，而这个区是存放一些加载类的信息的。 通过jstat，或者其他可视化工具 看起来Metaspace区域的内存呈现一个波动的状态，他总是会先不断增加，达到一个顶点之后， 就会把Metaspace区域给占满，然后自然就会触发一次Full GC，Full GC会带着Metaspace区域的垃圾回收， 所以接下来Metaspace区域的内存占用又变得很小了。 怎么查看到底什么类加载进入 -XX:+TraceClassLoading -XX:+TraceClassUnloading 通过这两个参数看看加载和卸载类的情况 【Loaded sun.reflect.GeneratedSerializationConstructorAccessor from __JVM_Defined_Class】 在JVM运行期间不断地加载这个类 这是反射的时候会创建的一个类的信息。可以看出，JVM就是会动态的去生成一些类放入Metaspace区域里的 并且JVM自己创建的奇怪的类，他们的Class对象都是SoftReference，也就是软引用的 举个例子 Student student = new Student(） 这时Metaspace就会存在一个student类 而年轻代会存在一个student类对象并且 错误原因 clock - timestamp \u003c= freespace * SoftRefLRUPolicyMSPerMB。 这个公式的意思就是说，“clock - timestamp”代表了一个软引用对象他有多久没被访问过了freespace代表JVM中的空闲内存空间，SoftRefLRUPolicyMSPerMB代表每一MB空闲内存空间可以允许SoftReference对象存活多久。 举个例子，假如说现在JVM创建了一大堆的奇怪的类出来，这些类本身的Class对象都是被SoftReference软引用的。然后现在JVM里的空间内存空间有3000MB，SoftRefLRUPolicyMSPerMB的默认值是1000毫秒，那么就意味着，此时那些奇怪的 SoftReference软引用的Class对象，可以存活3000 * 1000 = 3000秒，就是50分钟左右。 当然上面都是举例而已，大家都知道，一般来说发生GC时，其实JVM内部或多或少总有一些空间内存的，所以基本上如果不是快要发生OOM内存溢出了，一般软引用也是等到内存实在放不下对象才开始回收。 所以大家就知道了，按理说JVM应该会随着反射代码的执行，动态的创建一些奇怪的类，他们的Class对象都是软引用的，正常情况下不会被回收，但是也不应该快速增长才对 但是如果 把这个参数改为0的话 直接导致clock - timestamp \u003c= freespace * SoftRefLRUPolicyMSPerMB这个公式的右半边是 0，就导致所有的软引用对象，比如JVM生成的那些奇怪的Class对象，刚创建出来就可能被一次Young GC给带着立马回收掉一些。接着在反射调用时又不断的创建类信息到元空间(因为JDK源码里的实现有一些问题,所以导致并发环境下会重复创建一些Class)。元空间满了就FGC。 这个参数一般设置大一些就可以了，没必要频繁的去对软引用的对象做回收。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:1:1","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"案例2-大对象分析 分析案例前先看运行参数 -Xms1536M -Xmx1536M -Xmn512M -Xss256K -XX:SurvivorRatio=5 -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=68 -XX:+UseCMSInitiatingOccupancyOnly -XX:+CMSParallelRemarkEnabled -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC 堆大小1536M 年轻代512M 线程的栈大小 256k 年轻代比例5:1:1 意思就是 365:70:70左右 老年代1000m左右 -XX:CMSInitiatingOccupancyFraction=68 -XX:+UseCMSInitiatingOccupancyOnly 老年代占用68% 是启用回收 在占用680M时 CMSParallelRemarkEnabled 这是老年代在初始标记时采用并发标记 当时这个系统运行差不多30分钟左右进行一次FGC 意思就是差不多30分钟有680m左右对象进入老年代。 我们通过jstat 在线上观察JVM运行数据 并不是每次Young GC后都有几十MB对象进入老年代的，而是偶尔一次Young GC才 会有几十MB对象进入老年代，记住，是偶尔一次！ 这也就是说600多M 进入老年代是比较困难的。 继续观察发现系统运行着，会有段时间会有几百M的数据直接进入老年代。 大对象问题 定位大对象 我是通过jstat 观察系统，一旦有大对象进入老年代。我就用jmap打印当时的内存快照。通过可视化工具 查看具体信息。分析对象来源。 这次大对象的来源是一个sql语句没加条件而把大量的数据查询出来造成的。 针对本次优化 让开发同学解决代码中的bug，避免一些极端情况下SQL语句里不拼接where条件，务必要拼接上where条件，不允许查询表 中全部数据。彻底解决那个时不时有几百MB对象进入老年代的问题。 年轻代明显过小，Survivor区域空间不够，因为每次Young GC后存活对象在几十MB左右，如果Survivor就70MB很容易触发 动态年龄判定，让对象进入老年代中。所以直接调整JVM参数如下： -Xms1536M -Xmx1536M -Xmn1024M -Xss256K -XX:SurvivorRatio=5 -XX:PermSize=256M -XX:MaxPermSize=256M -XX:+UseParNewGC -XX:+UseConcMarkSweepGC -XX:CMSInitiatingOccupancyFraction=92 -XX:+CMSParallelRemarkEnabled -XX:+UseCMSInitiatingOccupancyOnly -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:+PrintHeapAtGC 年轻代设置 直接把年轻代空间调整为700MB左右，每个Surivor是150MB左右，此时YGC过后就几十M存活对象，一般不会进入老年代 老年代设置 反之老年代就留500MB左右就足够了，因为一般不会有对象进入老年代。 而且调整了参数“XX:CMSInitiatingOccupancyFraction=92” 避免老年代仅仅占用68%就触发GC，现在必须要占用到92%才会触 发GC。(1.6后这个值时默认值92) ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:1:2","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"案例3 不要在业务代码使用System.gc() 他每次执行都会指挥JVM去尝试执行一次Full GC，连带年轻代、老年代、永久代都会去回收。都知道FGC卡顿时间长 针对这个问题，一方面大家平时写代码的时候，不要自己使用“System.gc()”去随便触发GC， 一方面可以在JVM参数中加入这 个参数：-XX:+DisableExplicitGC。这个参数的意思就是禁止显式执行GC，不允许你来通过代码触发GC。 所以推荐大家将“-XX:+DisableExplicitGC”参数加入到自己的系统的JVM参数中，或者是加入到公司的JVM参数模板中去。避免有 的开发工程师好心办坏事，代码中频繁触发GC就不好了。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:1:3","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"阶段性总结 系统承载高并发请求，或者处理数据量过大，导致Young GC很频繁，而且每次Young GC过后存活对象太多，内存分配不合理， Survivor区域过小，导致对象频繁进入老年代，频繁触发Full GC。 系统一次性加载过多数据进内存，搞出来很多大对象，导致频繁有大对象进入老年代，必然频繁触发Full GC 系统发生了内存泄漏， 莫名其妙创建大量的对象，始终无法回收，一直占用在老年代里，必然频繁触发Full GC Metaspace（永久代）因为加载类过多触发Full GC 误调用System.gc()触发Full GC - 如果jstat分析发现Full GC原因是第一种，那么就合理分配内存，调大Survivor区域即可。 - 如果jstat分析发现是第二种或第三种原因，也就是老年代一直有大量对象无法回收掉，年轻代升入老年代的对象并不多，那么就dump出来内存快照，然后用视图话工具分析。 - 通过分析，找出来什么对象占用内存过多，然后通过一些对象的引用和线程执行堆栈的分析，找到哪块代码弄出来那么多的对象的。接着优化代码即可。 - 如果jstat分析发现内存使用不多，还频繁触发Full GC，必然是第四种和第五种，此时对应的进行优化即可 公司最好所有jvm模板参数都加上 禁止System.gc()，打印出来GC日志 ：注意有些公司是禁止dump日志，因为可能会有几秒卡顿。 基本参数 -XX:+CMSParallelInitialMarkEnabled表示在初始标记的多线程执行，减少STW； -XX:+CMSScavengeBeforeRemark：在重新标记之前执行minorGC减少重新标记时间； -XX:+CMSParallelRemarkEnabled:在重新标记的时候多线程执行，降低STW； -XX：CMSInitiatingOccupancyFraction=92和-XX:+UseCMSInitiatingOccupancyOnly配套使用，如果不设置后者，jvm第一次会采用92%但是后续jvm会根据运行时采集的数据来进行GC周期，如果设置后者则jvm每次都会在92%的时候进行gc； -XX:+PrintHeapAtGC:在每次GC前都要GC堆的概况输出 -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/local/app/oom oom时自动dump文件 这个比较重要 分析很有用 XX:+UseCMSCompactAtFullCollection -XX:CMSFullGCsBeforeCompaction=0 多少次进行压缩 +DisableExplicitGC -XX:+PrintGCDetails -Xloggc:gc.log 禁用手动gc 打印gc日志 参数一定要自己设置，因为默认的参数会给予永久代 新生代的内存大小比较少。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:2:0","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"系统的排查问题体系 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:3:0","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"一种成熟的监控方案 如果公司拥有Zabbix、Open-Falcon之类的监控平台。当系统有异常出现是，通过钉钉、等能联系开发的方式发送提醒信息。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:3:1","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["JVM"],"content":"机器（CPU、磁盘、内存、网络）方面 cpu 负载高的问题，是否是GC太频繁导致 通过top看各方面指标 磁盘io问题 ,磁盘空间要有好的管控 ，这一块主要是怕其他操作把磁盘写满了。 内存这块，关注JVM 内存情况 gc频率。 代码异常这些捕获到的，都得上报到专门的分析、提醒平台。 实在监控方案没得, 就只能通过oom日志分析。能有当时的dump 最好。 ","date":"2020-08-19","objectID":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/:3:2","tags":["jvm","垃圾回收"],"title":"JVM调优学习之旅-(8)P+CMS案例优化分析记录与优化总结","uri":"/jvm%E8%B0%83%E4%BC%98%E5%AD%A6%E4%B9%A0%E4%B9%8B%E6%97%85-8p-cms%E6%A1%88%E4%BE%8B%E4%BC%98%E5%8C%96%E5%88%86%E6%9E%90%E8%AE%B0%E5%BD%95%E4%B8%8E%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"},{"categories":["运维"],"content":"编译 相关依赖 yum update yum install wget wget http://nginx.org/download/nginx-1.19.1.tar.gz ## nginx 源码包 wget https://ftp.pcre.org/pub/pcre/pcre-8.44.tar.gz ## 正则会用到 wget https://zlib.net/zlib-1.2.11.tar.gz ##gzip 模块 tar xf 指定文件解压 ./configure --help 查看编译帮助命令 --help print this message --prefix=PATH nginx总相对路径 下面都可以 默认以相对路径创建 --sbin-path=PATH set nginx binary pathname --modules-path=PATH set modules path --conf-path=PATH set nginx.conf pathname --error-log-path=PATH set error log pathname --pid-path=PATH set nginx.pid pathname --lock-path=PATH set nginx.lock pathname --user=USER nginx配置文件生效启动用户 --with开头都是可以单独指定需要使用的模块 --without 开头指定不需要的模块 还有一些指定模块路径的参数 我使用的配置参数 ./configure --prefix=/usr/local/nginx \\ --with-pcre=/usr/local/nginx_rely/rely/pcre-8.44 \\ --with-zlib=/usr/local/nginx_rely/rely/zlib-1.2.11 \\ --with-http_ssl_module \\ --with-http_image_filter_module \\ --with-http_stub_status_module 期间根据环境可能需要安装下面三个东西 yum -y install gcc gcc-c++ openssl openssl-devel gd gd-devel 出现这样 差不多就检测成功了 make 编译 生成Makefile make install 去安装目录启动 设置软连接 ln -s /usr/local/nginx/sbin/nginx /usr/bin/nginx nginx -h 查看具体命令 注意：请在运行 nginx 时，使用绝对路径。因为热部署USR2是通过绝对路径寻找 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:1:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"虚拟主机配置与热部署 由于 nginx运行时 一个master 和多个 worker进程以线程的方式运行 worker_processes 就代表worker进程数，我设置的是根据cpu核数。 user 代表配置文件启动生效用户 也可以加上用户组 worker_connections worker进程连接数 http://nginx.org/en/docs/ nginx 配置文件文档 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"基于多Ip的虚拟主机 location / ##说明映射的是本地nginx安装路径 root ## 路径下的文件夹 index ## 文件夹下的首页 前提主机是多网卡：不同ip对应不同路径 端口监听默认80 server { listen 192.168.1.110; server_name localhost; location / { root server1/html; index index.html index.htm; } } server { listen 192.168.1.111; server_name localhost; location / { root server2/html; index index.html index.htm; } } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:1","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"基于多端口的虚拟主机 server { listen 9011; server_name localhost; location / { root server3/html; index index.html index.htm; } } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:2","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"基于域名的虚拟主机 server { listen 80; server_name 域名; //匹配优先级 精确\u003e左侧通配符\u003e右侧通配符\u003e正则 location / { root server4/html; index index.html index.htm; } } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:3","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"热部署 当从老版本替换为新版本的 nginx 的时候，如果不热部署的话，会需要取消 nginx 服务并重启服务才能替换成功，这样的话会使正在访问的用户在断开连接，所以为了在不影响用户的体验下进行版本升级，就需要通过信号量热部署来升级版本。 注意一点：新的nginx编译后的 logs、sbin、conf这些目录名字不能变 先备份nginx二进制文件 并用新的二进制nginx文件覆盖 cp nginx nginx.bak 启动新的nginx 这里说明一下 nginx 启动时必须以绝对路径启动，不然SIGUSR2无法找到nginx kill -s SIGUSR2 21598 现在 新旧进程 都并存，并且在logs下存在了一个旧进程的master 进程pid 这时 可以优雅退出worker进程，具体用户超时时间连接参数 worker shutdown timeout time 在配置文件可以配置 kill -s SIGWINCH 21598 此时旧master 还存在的情况下可以开始验证新worker进程的nginx 是否满足需求 如果满足需求可以正式让就master退出了 kill -s SIGQUIT 21598 至此平滑升级就可以了;如果新版有不满足需求，未通过测试 在老版本的master未退出时可以回滚 kill -s SIGHUP 21598 此时新旧进程也是并存的 kill -s SIGQUIT 21603 退出新master进程 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:2:4","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"main、events段常用参数 # main段核心参数： ### user USERNAME [GROUP] 解释：指定运行nginx的worker子进程的属主和属组，其中属组可以不指定 示例： user nginx nginx; ### pid DIR 解释：指定运行nginx的master主进程的pid文件存放路径 示例： pid /usr/local/nginx/logs/nginx.pid; ### worker_rlimit_nofile number 解释：指定worker子进程可以打开的最大文件句柄数 示例： worker_rlimit_nofile 20480; 备注：linux 最大打开65535个句柄 一般每个worker*cpu核心数=总数-10000 ### worker_rlimit_core size 解释：指定worker子进程异常终止后的core文件，用于记录分析问题 示例： worker rlimit core 50M; working_directory /usr/local/nginx/tmp; 备注：当前worker所属用户需要有这个文件夹的写权限 ### worker_processes number|auto 解释：指定nginx启动的worker子进程数量 示例： worker processes 4; worker_processes auto; ### worker_cpul_affinity cpumask1 cpumask2 解释：将每个worker子进程与我们的CPU物理核心绑定 示例： worker_cpu_affinity 0001 0010 0100 1000; #4个物理核心，4个worker子进程 worker cpu affinity 00000001 00000010 00000100 00001000 00010000 00100000 01000000 10000000; #8物理核心，8个worker子进程 worker_cpu_affinity 01 10 01 10; #2个物理核心，4个子进程 备注：将每个worker子进程与特定CPU物理核心绑定，优势在于：避免同一个worker子进程 在不同的CPU核心上切换，缓存失效，降低性能；其并不能真正的避免进程切换, 主要还是cpu时间片切换开销大，又由于是多核，并不能充分使用cpu缓存，所以让nginx的worker进程绑定上固定的核心cpu 来切换。 ### worker_priority number 解释：指定worker子进程的nice值，以调整运行nginx的worker子进程的优先级，通常设定为负值，以优先调用nginx 示例： worker_priority -10; 备注：Linux默认进程的优先级值是120,值越小越优先；nice设定范围为-20到+19 如-20+100 ### worker_shutdown_timeout time 解释：指定worker子进程优雅退出时的超时时间(热部署时,nginx 切换pid进程时发送信号量关闭旧链接请求的强制条件) 示例： worker_shutdown_timeout 5s; ### timer_resolution time 解释：worker子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降，主要看系统对时间精度要求 示例： worker_resolution 100ms; ### daemon on|off 解释：设定nginx的运行方式，前台还是后台，前台用户调试，后台用于生产 示例： daemon off; 前台 默认on # events段核心参数 ### use 解释:nginx使用何种事件驱动模型 method可选值：select、poll、kqueue、epoll、/dev/poll、eventpor 默认配置：无 推荐配置：不指定，让nginx自己选择 ### worker_connections 解释：worker子进程能够处理的最大并发连接数 默认配置：worker_connections 1024 推荐配置：worker_connections 65535/worker_processes|65535 ### accept_mutex 解释:是否打开负载均衡互斥锁 （就是在worker子进程都加了个互斥锁，新请求进来只给其中一个处理） 可选值：on、off 默认配置：accept_mutex off 推荐配置：accept_mutext on ### accept_mutex_delay 200ms 解释：新连接分配给worker子进程的超时时间 当accept_mutex参数打开才有意义 ### lock file 解释：负载均衡互斥锁文件存放路径 默认配置：lock_file logs/nginx.lock 推荐配置：lock_file logs/nginx.lock ### muti_accept on|off 解释:子进程一次性可以接收的新连接个数 默认off ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:3:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"location学习与sub_status 模块 root与alias区别** /usr/local/nginx 这个路径是nginx主路径 server_name 匹对路径地址 location 后面是匹对监听端口后的路径地址 location /image { root /usr/local/nginx/image } 客户端请求www.test.com/image/1.jpg, 则对应磁盘映射路径/usr/local/nginx/image/image/1.jpg location /image { root image 相对地址 } 则对应磁盘映射路径/usr/local/nginx/image/image/1.jpg location /picture{ alias /opt/nginx/html/picture; } 客户端请求www.test.com/picture/1.jpg,则对应磁盘映射 则对应磁盘映射路径/opt/nginx/html/picture/1.jpg alias只能放在 location中 root能用在 http server location if中 类似全局参数一样 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"location 匹配规则 规则 含义 (优先级从高到低) 示例 = 精确匹配 location = /images/ {…} ^~ 匹配到即停止搜索 location ^~ /images/ {…} ~ 正则匹配 区分大小写 location ~ . (jpg l gif)$ { … } ~* 正则匹配 不区分大小写 loaction ~*.(jpg l gif)$ {…} 这个用得少 不带任何符号 location / {..} 结尾反斜线区别 /test 与 /test/ /test 先找目录 找不到 会尝试找test文件 /test/ 只会当做文件目录 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:1","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"sub_status 模块 location /uri{ stub_status; } 也可以写在server层级 状态项 含义 Active Connections 活跃的连接数量 accepts 接受的客户端连接总数量 handled 处理的客户端连接总数量 requests 客户端总的请求数量 Reading 读取客户端的连接数 Writing 响应数据到客户端的连接数 Waiting 空闲客户端请求连接数量 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:4:2","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"请求、链接、权限模块 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:5:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"请求与连接区别 connection是连接:即常说的tcp连接，三次握手，状态机 request是请求:例如http请求，无状态的协议 request是必须建立在connection之上 connection限制客户端并发连接数 Nginx默认是有这个模块，可以通过--without-http_limit_conn_module 禁用 由于限制客户端需要每个worker子进程对客户端有标识，所以需要用到共享内存。 常用指令 limit_conn_zone 上下文 http 写法 limit_conn_zone $binary_remote_addr zone=addr:10m 通常1m共享内存空间大小都能维护3万多并发连接 10m已经很大了 addr是名称 binary_remote_addr 相比 remote_addr 空间使用更少 前者4 后面7-8字节 limit_conn_status 上下文：http、server、location limit_conn_status code 默认状态码 limit_conn_status 503 超过连接数后返回的状态码 limit_conn_log_level 上下文：http、server、location limit_conn_ log_level info|notice|warn|error 默认error 当限速行为发生时会产生日志 limit_conn limit_conn 上面的名称 限速多少个客户端并发连接 限制的只是并发连接才会出现。 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:5:1","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"request限制客户端请求平均速率 写法跟conn一样 Nginx默认是有这个模块，可以通过--without-http_limit_req_module 禁用 由于限制客户端需要每个worker子进程对客户端有标识，所以需要用到共享内存。 使用 限流算法是leaky_bucket算法，漏桶算法，使流量平滑进入 limit_req_zone $binary_remote_addr zone=addr:10m rate=2r/m 一分钟处理两个请求 limit_req_log_level 限速日志级别 limit_conn_status 状态码 limit_req zone=addr burst=7 nodelay; 设置了 rate 可以不设置burst=7 nodelay 连续请求 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:5:2","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"限定指定ip 访问 allow 允许 deny 拒绝 会直接返回403 forbidden http、server、location、limit_except 上下文 可以组合编写,从上到下范围筛选 location/{ deny 192.168.1.1; allow 192.168.1.0/24; allow 10.1.1.0/16; allow 2001:0db8::/32; deny all; } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:5:3","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"限制特定用户访问auth_basic模块 默认已经有了的模块：可以通过--without-http_auth_basic_module禁用 语法：auth_basic 浏览器提示语|off; 默认值：auth_basic off; 上下文：http、server、location、limit_except 语法：auth_basic_user_file 配置的密钥文件绝对地址; 上下文：http、server、location、limit_except 制作用户名密码 可执行程序：htpasswd 所属软件包：httpd-tools 生成新的密码文件：htpasswd -bc encrypt_pass jack 123456 添加新用户密码：htpasswd -b encrypt_pass mike 123456 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:5:4","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"给予HTTP响应码做权限 需要增加模块--with-http_auth_request_module 语法：auth_request uri|off; 默认值：auth_request off; 上下文：http、server、location location /private/{ auth_request /auth; } location /auth { //授权服务器，根据返回状态码做后续操作 成功状态200 proxy_pass http://127.0.0.1:8080/verify; // 带上请求信息可以 proxy_pass_request_body off; proxy_set_header Content-Length \"\"; proxy_set_header X-Original-URI $request_uri; } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:5:5","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"tcp与http相关变量学习 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:6:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"TCP相关变量 变量名 含义 remote_addr 客户端IP地址 remote_port 客户端端口 server addr 服务端IP地址 server_port 服务端端口 server protocol 服务端协议 binary_remote_addr 二进制格式的客户端IP地址 固定4字节 connection TCP连接的序号，递增 connection_request TCP连接当前的请求数量 proxy_protocol_addr 若使用了proxy_protocol协议则返回协议中地址 否则返回空 proxy_protocol_port 若使用了proxy_protocol协议则返回协议中端口 否则返回空 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:6:1","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"HTTP 请求过程中相关变量 变量名 含义 uri 请求的URL,不包含参数 request_uri 请求的URL,包含参数 scheme 协议名，http或https request_method 请求方法 request_length 全部请求的长度，包括请求行、请求头、请求体 args 全部参数字符串 arg_参数名 特定参数值 is_args URL中有参数，则返回？否则返回空 query_string 与args相同 remote_user 由HTTP Basic Authentication协议传入的用户名 特殊变量 host 先看请求行，再看请求头，最后找server_name http_user_agent 用户浏览器 http_referer 从哪些链接过来的请求 http_via 经过一层代理服务器，添加对应代理服务器的信息 http_x_forwarded_for 获取用户真实IP http_cookie 用户cookie ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:6:2","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"处理http请求中的变量 变量名 含义 request_time 处理请求已耗费的时间 request_completion 请求处理完成返回OK,否则返回空 server_name 匹配上请求的server_name值 https 若开启https,则返回on,否则返回空 request_filename 磁盘文件系统待访问文件的完整路径 document_root 由URI和root/alias规则生成的文件夹路径 realpath_root 将document_root中的软链接换成真实路径 limit_rate 返回响应时的速度上限值 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:6:3","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"rewrite模块中的return指令 ### return指令 `return 执行后 后续的执行是不会执行的` 语法：return code [text]; return code URL; return URL; 上下文：server、location、if 1XX:消息类 2XX:成功 3XX:重定向 4XX:客户端错误 5XX:服务器错误 ### rewrite指令 语法：rewrite regex replacement [flag]; 上下文：server、location、if 示例：rewrite /images/(.*.jpg)$/pic/$1; last：这是讲images下面.jpg的文件 都重定到pic路径去查看对应的location break ：这是讲images下面.jpg的文件 都重定到pic路径去查看对应的文件路径 **flag说明** - last 重写后的URL发起新请求，再此进入server段，尝试location中的匹配 - break 直接使用重写后的URL,不再匹配其他location中语句 - redirect 返回302临时重定向 - permanent 返回301永久重定向 示例 ​```nginx location /search { rewrite ^/(.*)http://www.cctv.com permanent; location /images { rewrite /images/(.*) /pics/$1 break; location /pics { rewrite /pics/(.*)/photos/$1 ; location /photos { } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:7:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"if语句 $variable 仅为变量时，值为空或以0开头字符串都会被当做false处理 类似布尔值 =或！= 相等或不等比较 ~或！~正则匹配或非正则匹配 ~*正则匹配，不区分大小写 -f或！-f检查文件存在或不存在 -d或！-d检查目录存在或不存在 -e或！-e检查文件、目录、符号链接等存在或不存在 -x或！-X检查文件可执行或不可执行 示例 location /search/ { if ($remote_addr = \"192.168.184.1\" ) { return 200 \"test if OK in URL /search/\"; } location / { if ($uri = \"/images /\") { rewrite (.*) /pics/ break; } return 200\"test if failed } ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:7:1","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"autoindex autoindex on|off 打开关闭 默认off autoindex_exact_size on|off 文件大小是否精确到字节 默认on autoindex_format json|html|xml|jsonp 默认html autoindex_localtime 文件时间格式 默认off 示例 location /download/{ root /opt/source; index a.html; autoindex on; autoindex exact size on; autoindex format html; autoindex_localtime off; } 访问 /opt/source/download/下面的文件 如果存在a.html直接访问这个页面 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:7:2","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"反向代理服务器 反向代理服务器介于用户和真实服务器之间，提供请求和响应的中转服务 对于用户而言，访问反向代理服务器就是访问真实服务器 反向代理可以有效降低服务器的负载消耗，提升效率 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:8:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"upstream模块 定义上游服务器的连接信息，请求控制之类的基本信息。 指令 含议 upstream 段名，以{开始, }结束，中间定义上游服务URL server 定义上游服务地址 zone 定义共享内存，用于跨worker子进程 keepalive 对上游服务启用长连接 keepalive_ requests 一个长连接最多请求个数 keepalive_timeout 空闲情形下,一个长连接的超时时长 hash 哈希负载均衡算法 ip_hash 依据IP进行哈希计算的负载均衡算法 least_conn 最少连接数负载均衡算法 least_time 最短响应时间负载均衡算法 random 随机负载均衡算法 上游服务器server 配置参数 weight=number 权重值，默认为1 max_conns= number 上游服务器的最大并发连接数 fail_timeout= time 服务器不可用的判定时间 max_fails= number 服务器不可用的检查次数 backup 备份服务器仅当其他服务 器都不可用时 down 标记服务器长期不可用,离线维护 默认已被编译进Nginx 禁用须通过--without-http_upstream_module 配置示例 就是说在10秒内有两次请求失败 就会认为服务器不可用 upstream 名称{ server 代理地址 weight=3 max_conns= 1000 fail_timeout=10s max_fails=2; keepalive 32; # 上游空闲长连接最大数量 keepalive_requests 50; # 单个长连接可以处理的最多http请求个数 keepalive_timeout 30s; # 空闲长连接超过这个时间没请求就销毁 } upstream 与 server一个层级 负载均衡配置示例 nginx默认是轮询server upstream 名称{ server 代理地址; server 代理地址; #------不考虑上游服务器处理能力来负载 hash $变量名;# 这个就是单纯客户端配置以什么算hash值来负载 ip_hash; #单纯以ip算hash值 #------考虑上游服务器能力 怎么维护服务器的信息,是多个worker子进程通过共享内存 zone 内存名称 10M; # 通过共享内存维护上游服务器信息 least_conn; #挑选服务器处理连接数最少的，如果都一样 退化轮询； least_time; #最短响应时间 } 当上游服务器错误 当 upstream上游服务器出错后请求转发到 proxy_next_upstream 语法: proxy_next_upstream error| timeout| invalid_header|http_500| http_502|http_503|http_504|http_403| http_404| http_429|no_idempotent|off # no_idempotent: 对非幂等请求失败是否需要转发 加上就会重试转发 默认值: proxy_next_upstream error timeout ; 上下文:http、server、 location 需要配置 proxy_next_upstream_timeout 等待时间 proxy_next_upstream_tries 重试转发次数 两个参数 默认都是0 proxy_intercept_errors on|off #上游返回响应码大于300 是直接响应还是按照error_page处理 默认on打开会使用error_page 关闭会直接返回上游信息 需要增加配置 error_page 503 /503.html 也可以error_page 503 =200 /503.html 这样用户看到的就是200状态码 proxy_pass指令 默认已被编译进Nginx 禁用须通过--without-http_proxy_module 语法: proxy_ pass URL url必须http或者https开头 上下文: location、 if、limit_except upsteam back_end { server 192.168.184.20:8080 weight=2 max_conns=1000 fail_timeout=10s max_fails=3; keepalive 32; keepalive requests 80; keepalive timeout 20s; } server{ listen 80; server_name proxy.kutian.edu; location /proxy/ { proxy_pass http://back_end/proxy; } ## proxy_pass http://back_end/proxy/ ## 尾部带反斜线nginx会更改url删除location后面地址 ## 不带反斜线会原封不动请求到上游服务器 客户端请求代理服务器请求信息 客户端超过这些规则 会返回413 location /proxy/ { proxy_pass http://back_end/proxy; #上游服务器 proxy_request_buffering on; # 打开缓冲默认打开。请求接收完在转发：nginx处理比较快，上有服务器处理比较低 利用缓存来提升吞吐量 client_max_body_size 250k; #最大body大小 client_body_buffer_size 100k;#客户端缓冲大小 client_body_temp_path test_body_path;#当大于缓冲大小 小于body大小 会把请求持久化到磁盘的位置 client_body_in_file_only on; #不管请求体多大 都会把请求持久化到磁盘 默认off处理完成会删除 client_body_in_single_buffer on;#连续存储磁盘空间 默认off不连续 client_body_timeout 30; #链接超时时间 } 代理更改请求上游服务器 location /proxy/ { proxy_pass http://back_end/proxy; #上游服务器 proxy_method PUT; #请求方式修改 proxy_http_version 1.1;# http版本 要支持长连接 必须是1.1 并且要与上游服务器使用长连接还需要设置header proxy_pass_request_headers off; #默认打开 全部转发上游服务器 proxy_pass_request_body off; #默认打开 proxy_set_body \"body信息\"; # 上面关闭不影响 proxy_set_header test \"header参数信息\"; # 向头部插入信息 上面关闭不影响 } 注意 proxy_set_header 默认重定义两个 Header 头字段， proxy_set_header Host $proxy_host; proxy_set_header Connection close; Host 初始值 $proxy_host，这是因为 HTTP/1.1 必须包含 Host 字段以指定主机；至于 $proxy_host 跟 $host 的区别，前者是 backend 即后端的主机名，后者是 frontend 即自身的主机名。该字段要不要改成 $host 或 $http_host，视后端会不会校验域名和端口而定 Connection 初始值 close，也是因为在 HTTP/1.1 中所有连接都是长连接 (keep-alive)，除非声明 close 表示不需要，而后端的 Web 应用程序 (HTTP/1.0 或更古早的) 未必支持连接复用，所以统统设成 close 以免出错。跟后端通讯真需要用到 keep-alive，先配置一组 upstream (上游服务器)，回头再清空 Connection 字段即可。即 proxy_set_header Connection \"\"; 连接指令 proxy_connect_timeout 60s; # 连接超时时间，比如3次握手 适用于http、server、location proxy_socket_keepalive on|off;#默认off;通过socket 替换长连接 适用于http、server、location proxy_send_timeout time;#默认值60s; 发送超时时间的超时时间 适用于http、server、location proxy_ignore_client_bort on|off; #是否忽略客户端请求退出，on表示如果客户端断开连接，nginx到上游服务器也会断开连接；off表示如果客户端断开连接，nginx到上游服务器会到请求响应结束后再断开 默认关闭 ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:8:1","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["运维"],"content":"缓存 缓存文件-以便启动恢复 语法：proxy_cache_path path keys_zone=name:size #文件路径与缓存名称大小 默认值：proxy_cache_path off; 上下文：http 可选参数 含义 level path的目录层级 use_ temp_path off直接使用path路径; on使用proxy_temp_path路径 inactive 在指定时间内没有被访问缓存会被清理;默认10分钟 max_size 设定最大的缓存文件大小,超过将由CM清理 manager_files CM清理一次缓存文件，最大清理文件数;默认100 manager_sleep CM清理一次后进程的休眠时间;默认200毫秒 manager_threshold CM清理一 次最长耗时;默认50毫秒 loader_files CL载入文件到共享内存，每批最多文件数;默认100 loader_sleep CL加载缓存文件到内存后,进程休眠时间;默认200毫秒 loader_threshold CL每次载入文件到共享内存的最大耗时;默认50毫秒 CM:处理进程 缓存key设置 语法：proxy_cache_key string; 默认值：proxy_cache_key $scheme $proxy_host$request_uri ; # http://host/uri 上下文：http、server、location 语法:proxy_cache_valid [code]time; # 对指定状态码进行缓存 上下文: http、 server、location 默认配置示例: proxy_cache_valid 60m; #只对200、301、 302响应码缓存 缓存状态 upstream_ cache_status MISS :未命中缓存 HIT :命中缓存 EXPIRED :缓存过期 STALE:命中了陈|旧缓存; REVALIDDATED : Nginx验证陈旧缓存依然有效 UPDATING :内容陈旧，但正在更新 BYPASS :响应从原始服务器获取 配置示例 proxy_cache_path /opt/nginx/cache_temp levels=2:2 keys_zone=cache_zone:30m max_size=32g inactive=60m use_temp_path=off; upstream cache_server { server 192.168.184.20:1010; server 192.168.184.20: 1011; } server { listen 80; server_name cache.kutian.edu; location / { proxy_cache cache_zone ; proxy_cache_valid 200 5m; proxy_cache_key $scheme$proxy_host$request_uri; add_header Nginx-Cache-Status \"$upstream_cache_status\"; # 显示缓存状态 proxy_pass http://cache_server; } } # 上游服务器可以在响应时修改header X-Accel-Expires 定义缓存时间。失效 状态会变为 EXPIRED 对于 特定key 不缓存 proxy_no_cache #表明用户访问login和search两个url的时候，变量$nocache 设置值 if ($request_uri ~ ^/(login|search)){ set $nocache 1; } location / { #当变量$nocache 有值，不缓存。 proxy_no_cache $nocache; } proxy_cache_bypass #该指令的每个参数都指定了一个条件，只有请求满足其中的任何一个条件，并且参数的值不是0，则Nginx会把请求转发到后端的服务而不会使用缓存。 缓存大量失效解决 限制请求 proxy_cache_lock on|off 默认off #限制相同路径同时请求 单个请求返回响应，后续相同路径才能再次请求 proxy_cache_lock_timeout time 默认5s # 超时时间后 剩余请求同时请求 proxy_cache_lock_age time 默认5s #限制超时未返回 一个一个请求 启用陈旧缓存 proxy_cache_use_stale error|timeout|invalid_header|updating|http_500|http_502|http_503|http_504|http_403|http_404|off 默认值: proxy_cache_use_stale off ; proxy_cache_background_update on|off 默认off # 由nginx请求更新缓存，让客户端先用旧缓存 可选参数 含义 error 与上游建立连接、发送请求、读取响应头出错时 timeout 与上游建立连接、发送请求、读取响应头超时时 invalid_header 无效头部时 updating 缓存过期,正在更新时 http_状态码 缓存清除操作 第三方nginx缓存清除模块。使用可以加–add-module ngx_cache_purge文件路径编译新版 location ~ /cache_purge(/.*) { proxy_cache_purge cache_zone $host$1; } # cache_zone 对应缓存 keys_zone 这个的名称 # $host$1; 对应缓存key ","date":"2020-08-17","objectID":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/:9:0","tags":["Nginx","反向代理","负载均衡"],"title":"Nginx学习笔记","uri":"/nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"},{"categories":["Redis"],"content":"Redis 线程模型 客户端 socket01 向 redis 进程的 server socket 请求建立连接，此时 server socket 会产生一个 AE_READABLE 事件，IO 多路复用程序监听到 server socket 产生的事件后，将该 socket 压入队列中。文件事件分派器从队列中获取 socket，交给连接应答处理器。连接应答处理器会创建一个能与客户端通信的 socket01，并将该 socket01 的 AE_READABLE 事件与命令请求处理器关联。 假设此时客户端发送了一个 set key value 请求，此时 redis 中的 socket01 会产生 AE_READABLE 事件，IO 多路复用程序将 socket01 压入队列，此时事件分派器从队列中获取到 socket01 产生的 AE_READABLE 事件，由于前面 socket01 的 AE_READABLE 事件已经与命令请求处理器关联，因此事件分派器将事件交给命令请求处理器来处理。命令请求处理器读取 socket01 的 key value 并在自己内存中完成 key value 的设置。操作完成后，它会将 socket01 的 AE_WRITABLE 事件与命令回复处理器关联。 如果此时客户端准备好接收返回结果了，那么 redis 中的 socket01 会产生一个 AE_WRITABLE 事件，同样压入队列中，事件分派器找到相关联的命令回复处理器，由命令回复处理器对 socket01 输入本次操作的一个结果，比如 ok，之后解除 socket01 的 AE_WRITABLE 事件与命令回复处理器的关联。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:1","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"为啥 redis 单线程模型也能效率这么高？ 纯内存操作。 核心是基于非阻塞的 IO 多路复用机制。处理事件是纯内存操作。 C 语言实现，一般来说，C 语言实现的程序“距离”操作系统更近，执行速度相对会更快。 单线程反而避免了多线程的频繁上下文切换问题，预防了多线程可能产生的竞争问题。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:2","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"redis 过期策略 定期删除+惰性删除 所谓定期删除，指的是 redis 默认是每隔 100ms 就随机抽取一些设置了过期时间的 key，检查其是否过期，如果过期就删除。 所谓惰性删除在你获取某个 key 的时候，redis 会检查一下 ，这个 key 如果设置了过期时间那么是否过期了？如果过期了此时就会删除，不会给你返回任何东西。 但是这样：当有大量key过期时 这个策略往往是不够的。只有走走内存淘汰机制。 这个看具体业务设置适合的淘汰机制 noeviction: 当内存不足以容纳新写入数据时，新写入操作会报错，这个一般没人用吧，实在是太恶心了。 allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的 key（这个是最常用的）。 allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个 key，这个一般没人用吧，为啥要随机，肯定是把最近最少使用的 key 给干掉啊。 volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的 key（这个一般不太合适）。 volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个 key。 volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的 key 优先移除。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:3","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"手写一个 LRU 算法 利用已有的 JDK 数据结构实现一个 Java 版的 LRU。 class LRUCache\u003cK, V\u003e extends LinkedHashMap\u003cK, V\u003e { private final int CACHE_SIZE; /** * 传递进来最多能缓存多少数据 * * @param cacheSize 缓存大小 */ public LRUCache(int cacheSize) { // true 表示让 linkedHashMap 按照访问顺序来进行排序，最近访问的放在头部，最老访问的放在尾部。 super((int) Math.ceil(cacheSize / 0.75) + 1, 0.75f, true); CACHE_SIZE = cacheSize; } @Override protected boolean removeEldestEntry(Map.Entry\u003cK, V\u003e eldest) { // 当 map中的数据量大于指定的缓存个数的时候，就自动删除最老的数据。 return size() \u003e CACHE_SIZE; } } ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:4","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"Redis 主从架构 与哨兵机制保证高并发 高可用 sentinel，中文名是哨兵。哨兵是 redis 集群机构中非常重要的一个组件，主要有以下功能： 集群监控：负责监控 redis master 和 slave 进程是否正常工作。 消息通知：如果某个 redis 实例有故障，那么哨兵负责发送消息作为报警通知给管理员。 故障转移：如果 master node 挂掉了，会自动转移到 slave node 上。 配置中心：如果故障转移发生了，通知 client 客户端新的 master 地址。 哨兵用于实现 redis 集群的高可用，本身也是分布式的，作为一个哨兵集群去运行，互相协同工作。 故障转移时，判断一个 master node 是否宕机了，需要大部分的哨兵都同意才行，涉及到了分布式选举的问题。 即使部分哨兵节点挂掉了，哨兵集群还是能正常工作的，因为如果一个作为高可用机制重要组成部分的故障转移系统本身是单点的，那就很坑爹了。 哨兵至少需要 3 个实例，来保证自己的健壮性。 哨兵 + redis 主从的部署架构，是不保证数据零丢失的，只能保证 redis 集群的高可用性。 对于哨兵 + redis 主从这种复杂的部署架构，尽量在测试环境和生产环境，都进行充足的测试和演练。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:5","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"redis 哨兵主备切换的数据丢失问题 异步复制导致的数据丢失：意思就是master还没来得及同步数据到slave 的时候master 就宕机了。 脑裂，也就是说，某个 master 所在机器突然脱离了正常的网络，跟其他 slave 机器不能连接，但是实际上 master 还运行着。此时哨兵可能就会认为 master 宕机了，然后开启选举，将其他 slave 切换成了 master。这个时候，集群里就会有两个 master ，也就是所谓的脑裂。 怎么解决 min-slaves-to-write 1 min-slaves-max-lag 10 表示，要求至少有 1 个 slave，数据复制和同步的延迟不能超过 10 秒。 如果说一旦所有的 slave，数据复制和同步的延迟都超过了 10 秒钟，那么这个时候，master 就不会再接收任何请求了。 这是为了减少数据丢失。 sdown 是主观宕机，就一个哨兵如果自己觉得一个 master 宕机了，那么就是主观宕机 odown 是客观宕机，如果 quorum 数量的哨兵都觉得一个 master 宕机了，那么就是客观宕机 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:6","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"redis 持久化的两种方式 RDB：RDB 持久化机制，是对 redis 中的数据执行周期性的持久化。保存的是实际的数据 AOF：AOF 机制对每条写入命令作为日志（resp协议命令），以 append-only 的模式写入一个日志文件中，在 redis 重启的时候，可以通过回放 AOF 日志中的写入指令来重新构建整个数据集。 如果同时使用 RDB 和 AOF 两种持久化机制，那么在 redis 重启的时候，会使用 AOF 来重新构建数据，因为 AOF 中的数据更加完整。 RDB 优缺点 RDB 对 redis 对外提供的读写服务，影响非常小，可以让 redis 保持高性能，因为 redis 主进程只需要 fork 一个子进程，让子进程执行磁盘 IO 操作来进行 RDB 持久化即可。 由于RDB是保存的数据，恢复就比较快，但是RDB都是每隔一段时间保存一次数据，所以在周期内可能会有数据丢失。 RDB 每次在 fork 子进程来执行 RDB 快照数据文件生成的时候，如果数据文件特别大，可能会导致对客户端提供的服务暂停数毫秒，或者甚至数秒。 AOF优缺点 AOF 可以更好的保护数据不丢失，一般 AOF 会每隔 1 秒，通过一个后台线程执行一次fsync操作，最多丢失 1 秒钟的数据。 AOF 日志文件以 append-only 模式写入，所以没有任何磁盘寻址的开销，写入性能非常高，而且文件不容易破损，即使文件尾部破损，也很容易修复。 适合做适合做灾难性的误删除的紧急恢复 比如某人不小心用 flushall 命令清空了所有数据，只要这个时候后台 rewrite 还没有发生，那么就可以立即拷贝 AOF 文件，将最后一条 flushall 命令给删了，然后再将该 AOF 文件放回去，就可以通过恢复机制，自动恢复所有数据 相对来说性能会比RDB低，因为AOF 一般会配置成每秒 fsync 一次日志文件。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:7","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"Redis cluster 分布式寻址算法 hash 算法（大量缓存重建） 来了一个 key，首先计算 hash 值，然后对节点数取模。然后打在不同的 master 节点上。一旦某一个 master 节点宕机，所有请求过来，都会基于最新的剩余 master 节点数去取模，尝试去取数据。这会导致大部分的请求过来，全部无法拿到有效的缓存，导致大量的流量涌入数据库 一致性 hash 算法+虚拟节点 一致性 hash 算法将整个 hash 值空间组织成一个虚拟的圆环，整个空间按顺时针方向组织，下一步将各个 master 节点（使用服务器的 ip 或主机名）进行 hash。这样就能确定每个节点在其哈希环上的位置。 对key首先计算 hash 值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，遇到的第一个 master 节点就是 key 所在位置。 在一致性哈希算法中，如果一个节点挂了，受影响的数据仅仅是此节点到环空间前一个节点（沿着逆时针方向行走遇到的第一个节点）之间的数据，其它不受影响。增加一个节点也同理。 燃鹅，一致性哈希算法在节点太少时，容易因为节点分布不均匀而造成缓存热点的问题。为了解决这种热点问题，一致性 hash 算法引入了虚拟节点机制，即对每一个节点计算多个 hash，每个计算结果位置都放置一个虚拟节点。这样就实现了数据的均匀分布，负载均衡。 Redis cluster 的 hash slot 算法 Redis cluster 有固定的 16384 个 hash slot，对每个 key 计算 CRC16 值，然后对 16384 取模，可以获取 key 对应的 hash slot。 redis cluster 中每个 master 都会持有部分 slot，比如有 3 个 master，那么可能每个 master 持有 5000 多个 hash slot。hash slot 让 node 的增加和移除很简单，增加一个 master，就将其他 master 的 hash slot 移动部分过去，减少一个 master，就将它的 hash slot 移动到其他 master 上去。移动 hash slot 的成本是非常低的。客户端的 api，可以对指定的数据，让他们走同一个 hash slot，通过 hash tag 来实现。 任何一台机器宕机，另外两个节点，不影响的。因为 key 找的是 hash slot，不是机器。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:8","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["Redis"],"content":"缓存雪崩 击穿 穿透 雪崩 事前：redis 高可用，主从+哨兵，redis cluster，避免全盘崩溃。 事中：本地 ehcache 缓存 + hystrix 限流\u0026降级，避免 MySQL 被打死。 事后：redis 持久化，一旦重启，自动从磁盘上加载数据，快速恢复缓存数据。 穿透 用户大量请求库中不存在的数据 每次系统 从数据库中只要没查到，就写一个空值到缓存里去，比如 set -999 UNKNOWN。然后设置一个过期时间，这样的话，下次有相同的 key 来访问的时候，在缓存失效之前，都可以直接从缓存中取数据。 击穿 某个 key 非常热点，访问非常频繁，处于集中式高并发访问的情况，当这个 key 在失效的瞬间，大量的请求就击穿了缓存，直接请求数据库，就像是在一道屏障上凿开了一个洞 可以将热点数据设置为永远不过期；或者基于 redis or zookeeper 实现互斥锁，等待第一个请求构建完缓存之后，再释放锁，进而其它请求才能通过该 key 访问数据。 ","date":"2020-05-26","objectID":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/:0:9","tags":["面试"],"title":"总结Redis问题","uri":"/%E6%80%BB%E7%BB%93redis%E9%97%AE%E9%A2%98/"},{"categories":["数据结构与算法"],"content":" 学习数据结构 -动态数组 模仿 ArrayList public class ArrayList\u003cE\u003e { /** * 大小 */ private int size; /** * 原始数组 */ private Object[] elementData; /** * 空数组 */ private static final Object[] EMPTY_DATA = {}; /** * 默认数组长度 */ private static final int DEFAULT_CAPACITY = 10; public ArrayList(int initialCapacity) { // 大于0 设置为用户大小 等于0 给空数组 否则设置错误 if (initialCapacity \u003e 0) { this.elementData = new Object[initialCapacity]; } else if (initialCapacity == 0) { this.elementData = EMPTY_DATA; } else { throw new IllegalArgumentException(\"长度设置错误\" + initialCapacity); } } public ArrayList() { // 没设置给个空数组 this.elementData = EMPTY_DATA; } public int size() { return size; } public boolean isEmpty() { return size == 0; } /** * 添加数据 * * @param e 数据 * @return 添加成功 */ public boolean add(Object e) { // 确保我的数组长度是当前实际大小+1 ensureCapacityInternal(size + 1); elementData[size++] = e; return true; } /** * 添加方法 带下标位置 * * @param index 下标 * @param e 数据 * @return 添加成功 */ public boolean add(int index, Object e) { // 校验位置 rangeCheckForAdd(index); // 确保数组长度我能放进去 ensureCapacityInternal(size + 1); //添加指定位置并移动指定位置后数据 系统方法 复制数组很有趣 可以实现自己复制给自己 // 比如 0 1 2 3 4 5 我插入下标为3 的数据 变成 0 1 2 e 3 4 5 意思就是 把源数组 下标为3起始位置 开始 copy 到目标数组index+1 起始位置后的数据 // copy长度size-index System.arraycopy(elementData, index, elementData, index + 1, size - index); elementData[index] = e; size++; return true; } /** * 获取指定下标数据 * * @param index 下标 * @return 数据 */ public E get(int index) { // 验证下标是否在正常范围 rangeCheck(index); return (E) elementData[index]; } /** * 设置 指定下标数据 * * @param index 下标 * @param e 修改数据 */ public E set(int index, int e) { // 验证下标是否在正常范围 rangeCheck(index); // 返回旧值 E oldElementData = (E) elementData[index]; elementData[index] = e; return oldElementData; } /** * 删除指定下标数据 * * @param index 下标 * @return 旧数据 */ public E remove(int index) { // 检测下标是否越界 rangeCheck(index); //返回旧值 E oldElementData = (E) elementData[index]; // 上面index能过检测 最大的大小就是size -1 就是数组最后一个值的下标 int move = size - index - 1; if (move \u003e 0) { // 说明需要移动下标 不是最后一位 比如 我有 1,2,3,4 数组 我要修改 数组为2下标的数据 我就要移动4 移动到 3 的位置 长度为 size-2-1 System.arraycopy(elementData, index + 1, elementData, index, move); } // 移动过后最后一位需要清空 不然数据不会消失 elementData[--size] = null; return oldElementData; } /** * 返回数据指定下标 * * @param e 数据 * @return 下标 */ public int indexOf(Object e) { if (e != null) { for (int i = 0; i \u003c size; i++) { if (e.equals(elementData[i])) { return i; } } } else { return -1; } return -1; } @Override public String toString() { StringJoiner stringJoiner = new StringJoiner(\", \", \"[\", \"]\"); for (Object element : elementData) { if (element != null) { stringJoiner.add(String.valueOf(element)); } } return stringJoiner.toString(); } /** * 下标长度位置是否合法判断 添加 * * @param index 下标 */ private void rangeCheckForAdd(int index) { // 只能添加下边小于等于 当前list长度的 可以添加等于size下标位置的数据 if (index \u003e size || index \u003c 0) { throw new IndexOutOfBoundsException(\"index 越界(添加时)\" + index); } } /** * 验证下标是否越界 获取 * * @param index 下标 */ private void rangeCheck(int index) { // 获取仅存在的下标 if (index \u003e= size) { throw new IndexOutOfBoundsException(\"下标越界异常\" + index); } } /** * 确保容量有多大 * * @param minLength 最小长度 */ private void ensureCapacityInternal(int minLength) { // 判断 大小 给予 默认大小 if (elementData == EMPTY_DATA) { //表示是空数组给个默认长度 minLength = DEFAULT_CAPACITY; } // 如果 数组长度 已经小于我们需要的最小长度 就扩容 if (minLength \u003e elementData.length) { // 旧长度 int oldLength = elementData.length; // 新长度等于旧长度 的 1.5倍 旧长度+旧长度*0.5 int newLength = oldLength + (oldLength \u003e\u003e 1); Object[] copy = new Object[newLength]; // 把旧数组 赋值给新数组 System.arraycopy(elementData, 0, copy, 0, oldLength); log.warn(\"扩容原始长度{},扩容后长度{}\", oldLength, newLength); elementData = copy; } } ","date":"2020-05-13","objectID":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/:0:0","tags":["动态数组"],"title":"数据结构学习-动态数组","uri":"/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%AD%A6%E4%B9%A0-%E5%8A%A8%E6%80%81%E6%95%B0%E7%BB%84/"},{"categories":["MySQL性能调优"],"content":" 复习时自己产生的问题 ","date":"2020-01-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/:0:0","tags":["MySQL"],"title":"MySQL性能调优(7)复习补充","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/"},{"categories":["MySQL性能调优"],"content":"log 缓冲池 Buffer Pool 首先，InnnoDB 的数据都是放在磁盘上的，InnoDB 操作数据有一个最小的逻辑单位，叫做页（索引页和数据页）。我们对于数据的操作，不是每次都直接操作磁盘，因为磁盘的速度太慢了。InnoDB 使用了一种缓冲池的技术，也就是把磁盘读到的页放到一 块内存区域里面。这个内存区域就叫 Buffer Pool，下一次读取相同的页，先判断是不是在缓冲池里面，如果是，就直接读取，不用再 次访问磁盘。 修改数据的时候，先修改缓冲池里面的页。内存的数据页和磁盘数据不一致的时候， 我们把它叫做脏页。InnoDB 里面有专门的后台线程把 Buffer Pool 的数据写入到磁盘， 每隔一段时间就一次性地把多个修改写入磁盘，这个动作就叫做刷脏。 Buffer Pool 是 InnoDB 里面非常重要的一个结构，它的内部又分成几块区域。这里 我们趁机到官网来认识一下 InnoDB 的内存结构和磁盘结构。 Buffer Pool SHOW STATUS LIKE '%innodb_buffer_pool%'; 官网链接 Buffer Pool 默认大小是 128M（134217728 字节），可以调整。 这个可以作为优化，根据业务把缓冲池调大有利于更多的索引热点数据查询 https://dev.MySQL.com/doc/refman/5.7/en/server-status-variables.html 内存的缓冲池写满了InnoDB 用 LRU算法来管理缓冲池（链表实现，不是传统的 LRU，分成了 young 和 old），经过淘汰的数据就是热点数据。这个策略跟redis一样。 Change Buffer 写缓冲 如果这个数据页不是唯一索引，不存在数据重复的情况，也就不需要从磁盘加载索 引页判断数据是不是重复（唯一性检查）。这种情况下可以先把修改记录在内存的缓冲 池中，从而提升更新语句（Insert、Delete、Update）的执行速度。把 Change Buffer 记录到数据页的操作叫做 merge。在访问这个数据页的时候，或者通过后台线程、或者数据库 shut down、 redo log 写满时触发merge。 如果数据库大部分索引都是非唯一索引，并且业务是写多读少，不会在写数据后立 刻读取，就可以使用 Change Buffer（写缓冲）。写多读少的业务，调大这个值。 Change Buffer 占 Buffer Pool 的比例，默认 25% Adaptive Hash Index 索引应该是放在磁盘的。InnoDB本身不支持哈希索引，所有索引检索都走B树，Adaptive Hash index可以认为是“索引的索引”。哈希索引就是采用一定的哈希算法，把键值换算成新的哈希值，检索时不需要类似B+树那样从根节点到叶子节点逐级查找，只需一次哈希算法即可立刻定位到相应的位置，速度非常快。 (redo)Log Buffer 如果 Buffer Pool 里面的脏页还没有刷入磁盘时，数据库宕机或者重 启，这些数据丢失。如果写操作写到一半，甚至可能会破坏数据文件导致数据库不可用。为了避免这个问题，InnoDB 把所有对页面的修改操作专门写入一个日志文件，并且 在数据库启动时从这个文件进行恢复操作（实现 crash-safe）——用它来实现事务的持久性。这 种 日 志 和 磁 盘 配 合 的 整 个 过 程 ， 其 实 就 是 MySQL 里 的 WAL 技 术 （Write-Ahead Logging），它的关键点就是先写日志，再写磁盘。 同样是写磁盘，为什么不直接写到 db file 里面去？为什么先写日志再写磁盘？ 我们先来了解一下随机 I/O 和顺序 I/O 的概念。 磁盘的最小组成单元是扇区，通常是 512 个字节。 操作系统和内存打交道，最小的单位是页 Page。 操作系统和磁盘打交道，读写磁盘，最小的单位是块 Block。 如果我们所需要的数据是随机分散在不同页的不同扇区中，那么找到相应的数据需要等到磁臂旋转到指定的页，然后盘片寻找到对应的扇区，才能找到我们所需要的一块数据，一次进行此过程直到找完所有数据，这个就是随机 IO，读取数据速度较慢。 假设我们已经找到了第一块数据，并且其他所需的数据就在这一块数据后边，那么就不需要重新寻址，可以依次拿到我们所需的数据，这个就叫顺序 IO。 刷盘是随机 I/O，而记录日志是顺序 I/O，顺序 I/O 效率更高。因此先把修改写入日志，可以延迟刷盘时机，进而提升系统吞吐。 当然 redo log 也不是每一次都直接写入磁盘，在 Buffer Pool 里面有一块内存区域（Log Buffer）专门用来保存即将要写入日志文件的数据，默认 16M，它一样可以节省磁盘 IO。 需要注意：redo log 的内容主要是用于崩溃恢复。磁盘的数据文件，数据来自 buffer pool。redo log写入磁盘，不是写入数据文件。那么，Log Buffer 什么时候写入 log file？ 在我们写入数据到磁盘的时候，操作系统本身是有缓存的。flush 就是把操作系统缓冲区写入到磁盘 log buffer 写入磁盘的时机，由一个参数控制，默认是 1。 SHOW VARIABLES LIKE 'innodb_flush_log_at_trx_commit'; 0（延迟写） log buffer 将每秒一次地写入 log file 中，并且 log file 的 flush 操作同时进行。 该模式下，在事务提交的时候，不会主动触发写入磁盘的操作。 1（默认，实时写，实时刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file，并且刷到磁盘 中去。 2（实时写，延迟刷） 每次事务提交时 MySQL 都会把 log buffer 的数据写入 log file。但是 flush 操 作并不会同时进行。该模式下，MySQL 会每秒执行一次 flush 操作 问题:Redo log 和db文件都是在磁盘上的，为什么写redo log 是顺序io 写db文件就是随机IO呢？ buffer pool中有很多数据等待刷脏的时候，写入redo log是顺序写入的。 而这些数据在磁盘中的位置不是连续的，每次都要重新寻址，所以是随机I/O。 数据写入到redo log里面就有了保障，刷盘就不需要那么频繁了，提升了系统的吞吐量。MySQL很多地方都利用的这一点 双写缓冲问题 InnoDB 的页和操作系统的页大小不一致，InnoDB 页大小一般为 16K，操作系统页 大小为 4K，InnoDB 的页写入到磁盘时，一个页需要分 4 次写。如果存储引擎正在写入页的数据到磁盘时发生了宕机，可能出现页只写了一部分的 情况，比如只写了 4K，就宕机了，这种情况叫做部分写失效（partial page write），可能会导致数据丢失，如果这个页本身已经损坏了，用redo log来做崩溃恢复是没有意义的。所以在对于应用 redo log 之前，需要一个页的副本。如果出现了写入失效，就用页的副本来还原这个页，然后再应用 redo log。这个页的副本就是 double write，InnoDB 的双写技术。通过它实现了数据页的可靠性。 跟redo log一样，double write 由两部分组成，一部分是内存的 double write，一个部分是磁盘上的 double write。因为 double write 是顺序写入的，不会带来很大的开销 Binlog binlog 以事件的形式记录了所有的 DDL 和 DML 语句（因为它记录的是操作而不是 数据值，属于逻辑日志），可以用来做主从复制和数据恢复。 跟 redo log 不一样，它的文件内容是可以追加的，没有固定大小限制。 在开启了 binlog 功能的情况下，我们可以把 binlog 导出成 SQL 语句，把所有的操 作重放一遍，来实现数据的恢复。 binlog 的另一个功能就是用来实现主从复制，它的原理就是从服务器读取主服务器 的 binlog，然后执行一遍。 例如一条语句：update teacher set name='盆鱼宴' where id=1; 1、先查询到这条数据，如果有缓存，也会用到缓存。 2、把 name 改成盆鱼宴，然后调用引擎的 API 接口，写入这一行数据到内存，同时记录 redo log。这时 redo log 进入 prepare 状态，然后告诉执行器，执行完成了，可以随时提交。 3、执行器收到通知后记录 binlog，然后调用存储引擎接口，设置 redo log为 commit状态。 4、更新完成。 先记录到内存，再写日志文件。 记录 redo log 分为两个阶段。 存储引擎和 Server 记录不同的日志。 先记录 redo，再记录 binlog。 ","date":"2020-01-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/:0:1","tags":["MySQL"],"title":"MySQL性能调优(7)复习补充","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/"},{"categories":["MySQL性能调优"],"content":"index 页分裂 InnoDB在叶子上存储数据。一个节点就是一页。一个页可以存储多行数据，按照主键的顺序存储。当数据是顺序插入的时候，一页写满了，就申请一个新的页。如果是随机插入，在指定位置的页已经写满了（或者到达了分裂阈值）的时候，就会发生页结构的调整（即B+Tree的节点的分裂） 页的内部原理 页可以空或者填充满（100%），行记录会按照主键顺序来排列。这里有个重要的属性：MERGE_THRESHOLD。该参数的默认值是50%页的大小，它在InnoDB的合并操作中扮演了很重要的角色 当你插入数据时，如果数据（大小）能够放的进页中的话，那他们是按顺序将页填满的。根据B树的特性，它可以自顶向下遍历，但也可以在各叶子节点水平遍历。因为每个叶子节点都有着一个指向包含下一条（顺序）记录的页的指针。那么如果我突然执行删除，页会出现什么情况？ 页合并 当你删了一行记录时，实际上记录并没有被物理删除，记录被标记（flaged）为删除并且它的空间变得允许被其他记录声明使用。当页中删除的记录达到MERGE_THRESHOLD（默认页体积的50%），InnoDB会开始寻找最靠近的页（前或后）看看是否可以将两个页合并以优化空间使用。 页分裂 当前页有空间但是容纳不下我要插入的数据时。下一页又是满的无法插入数据时。 1. 创建新页 2. 判断当前页（页#10）可以从哪里进行分裂（记录行层面） 3. 移动记录行 4. 重新定义页之间的关系 页分裂会发生在插入或更新，并且造成页的错位（dislocation，落入不同的区）论id设计的重要性 感谢这篇文章 索引用到文件排序 Using filesort Using filresort代表：不能使用索引来排序，用到了额外的排序。 例如：查询用到了a字段上的idx_a，但是后面有order by b。 一些优化方式： 1.建立联合索引idx(a,b) 2.不使用MySQL的排序，改成在代码中排序 为什么要固定Page大小，而不是需要多少数据，读取多少数据（按需读取）为什么16k 设置页的大小与磁盘的预读取特性有关系。局部性原理认为：当一个数据被用到时，其附近的数据也通常会马上被使用。所以顺序读取附近的数据，可以提升I/O效率。每次都至少读取一页 默认的16KB大小，在涉及表扫描和批量更新的业务场景中更实用，效率已经很高了 官网解释 https://dev.MySQL.com/doc/refman/5.7/en/innodb-parameters.html#sysvar_innodb_page_size 表没主键索引问题 如果我们定义了主键(PRIMARY KEY)，那么 InnoDB 会选择主键作为聚集索引。 如果没有显式定义主键，则 InnoDB 会选择第一个不包含有 NULL 值的唯一索引作为主键索引、 如果也没有这样的唯一索引，则 InnoDB 会选择内置 6 字节长的 ROWID 作为隐 藏的聚集索引，它会随着行记录的写入而主键递增。 辅助索引什么情况下查询需要回表，什么情况下不需要回表 例如索引是index(a,b,c），select 索引里面的列 (包括select a，select b，select c，select a,b select a,c select b,c，select a,b,c)， 都不需要到主键索引的叶子节点获取完整数据，此时不需要回表。 ","date":"2020-01-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/:0:2","tags":["MySQL"],"title":"MySQL性能调优(7)复习补充","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/"},{"categories":["MySQL性能调优"],"content":"事务与锁 共享锁 有什么意义呢 举个例子，order_info 和 order_detail 有逻辑的主外键关系。 在操作order_detail的时候，不希望order_info被其他的事务修改，可以用共享锁，阻塞其他的事务修改 innodb有了MVCC为什么还需要LBCC 快照读（普通的select）用MVCC保证读一致性。 加锁的读和增删改，用lock保证读一致性 MySQL ACID怎么保证 原子性（Atomicity，或称不可分割性） 通过undo log 实现 一致性（Consistency）通过AID特性与应用保证一致性 隔离性（Isolation）MVCC 持久性（Durability）通过redo log实现 ","date":"2020-01-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/:0:3","tags":["MySQL"],"title":"MySQL性能调优(7)复习补充","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/"},{"categories":["MySQL性能调优"],"content":"优化 连接池大小应该设置多大 我们不妨想一下，为啥 Nginx 内部仅仅使用了 4 个线程，其性能就大大超越了 100 个进程的 Apache HTTPD 呢？ 在一核 CPU 的机器上，顺序执行A和B永远比通过时间分片切换“同时”执行A和B要快，其中原因，学过操作系统这门课程的童鞋应该很清楚。一旦线程的数量超过了 CPU 核心的数量，再增加线程数系统就只会更慢，而不是更快，因为这里涉及到上下文切换耗费的额外的性能。 在加上现在的磁盘又基本是SSD，无需寻址和没有旋回耗时的确意味着更少的阻塞所以更少的线程（更接近于CPU核心数）会发挥出更高的性能，只有当阻塞密集时，更多的线程数才能发挥出更好的性能。 连接数 = ((核心数 * 2) + 有效磁盘数) 当然，这取决于线上环境，开发还是设置大一点，因为大家都在联调测试。 Count问题 MyISAM之所以可以把表中的总行数记录下来供COUNT(*)查询使用，那是因为MyISAM数据库是表级锁，不会有并发的数据库行数修改，所以查询得到的行数是准确的。 但是，对于InnoDB来说，就不能做这种缓存操作了，因为InnoDB支持事务，其中大部分操作都是行级锁，所以可能表的行数可能会被并发修改，那么缓存记录下来的总行数就不准确了。 我们知道，InnoDB中索引分为聚簇索引（主键索引）和非聚簇索引（非主键索引），聚簇索引的叶子节点中保存的是整行记录，而非聚簇索引的叶子节点中保存的是该行记录的主键的值。 所以，相比之下，非聚簇索引要比聚簇索引小很多，所以MySQL会优先选择最小的非聚簇索引来扫表。所以，当我们建表的时候，除了主键索引以外，创建一个非主键索引还是有必要的。 COUNT(*)和COUNT(1)和count(字段)区别 InnoDB handles SELECT COUNT(*) and SELECT COUNT(1) operations in the same way. There is no performance difference. 官方发话 画重点：same way,no performance difference。所以，对于COUNT(1)和COUNT(*)，MySQL的优化是完全一样的，根本不存在谁比谁快！ 建议使用COUNT(*)！因为这个是SQL92定义的标准统计行数的语法。 COUNT(字段)多了一个步骤就是判断所查询的字段是否为NULL所以他的性能要比COUNT(*)慢。 ","date":"2020-01-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/:0:4","tags":["MySQL"],"title":"MySQL性能调优(7)复习补充","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/"},{"categories":["MySQL性能调优"],"content":"存储引擎 InnoDB和MyISAM区别 InnoDB 支持事务，MyISAM 不支持事务。这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一 InnoDB 支持外键，而 MyISAM 不支持。对一个包含外键的 InnoDB 表转为 MYISAM 会失败 InnoDB 是聚集索引数据跟索引存放在一起，MyISAM 是非聚集索引数据和索引分开。 InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁。 由于索引分开的MyISAM 在内存中存储了row_count 值的 meta 信息可以直接获取到总行数 底层使用B+树不用B树 AVL(平衡二叉树)树解决了索引频繁的修改，和查询效率不高的问题。 B树相对AVL树，每个节点存储的数据更多，路数更多，树的深度减少，减少I/O次数，提升效率。 B+树相对B树，效率更稳定，因为数据存在叶子节点；排序能力更强，因为叶子节点有下一个数据区的指针；读写能力更强，因为根节点和枝节点不用存数据，所以可以保存更多的关键字。扫库扫表能力更强，因为数据都存在叶子节点，而且叶子节点都有下一个数据区的指针，所以遍历起来很方便。 B+树数据存放量问题 假设我们的树深度为2 我们id用bigint数据类型占8字节 我们一条数据1k大小 指针在InnoDB中占6字节 一个页为16k=16384字节 我们一页就可以存放16384/14=1170个指向记录 注意！这是根节点 每个指向记录下面存(16k/1k)*1170=18720记录 如果深度为3那么就是1170*1170*16=21902400条记录 所以说我们2000万的数据如果通过id搜索也就3次io。 ","date":"2020-01-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/:0:5","tags":["MySQL"],"title":"MySQL性能调优(7)复习补充","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%987%E5%A4%8D%E4%B9%A0%E8%A1%A5%E5%85%85/"},{"categories":["运维"],"content":"镜像地址 基本命令 vagrant init # 初始化 vagrant up # 启动虚拟机 vagrant halt # 关闭虚拟机 vagrant reload # 重启虚拟机 vagrant ssh # SSH 至虚拟机 vagrant status # 查看虚拟机运行状态 vagrant destroy # 销毁当前虚拟机 vagrant suspend # 挂起当前虚拟机 vagrant resume # 恢复被挂起的vm vagrant box list # 列出所有box列表 vagrant box remove {base name} # 删除 vagrant box add {title} {url} # 添加镜像 vagrant destroy # 停止当前正在运行的虚拟机并销毁所有创建的资源 vagrant package # 把当前的运行的虚拟机环境进行打包，可用于分发开发环境 vagrant plugin # 安装卸载插件 vagrant provision # 设置基本的环境，进一步设置可以使用Chef/Puppet进行搭建 vagrant ssh-config # 输出ssh连接的一些信息 vagrant status # 获取虚拟机状态 vagrant version # 获取vagrant的版本 ","date":"2020-01-06","objectID":"/vagrant%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/:0:0","tags":["虚拟机","Vagrant"],"title":"Vagrant基本命令","uri":"/vagrant%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4/"},{"categories":["运维"],"content":" 从公网ip访问自己的机器-以前一直用的花生壳、NATAPP之类的;要么付费要么随机变化域名要么速度慢。frp可以解决这一类问题。 搭建frp服务器进行内网穿透，可用且推荐，可以达到不错的速度，且理论上可以开放任何想要的端口，可以实现的功能远不止远程桌面或者文件共享 准备: 有公网ip的服务器(有域名可以配合ip映射访问如果要映射80端口需要配合nginx) frp地址 服务端配置 在 github 找到你电脑架构对应的版本下载 wget https://github.com/fatedier/frp/releases/download/v0.31.0/frp_0.31.0_linux_amd64.tar.gz 解压 tar -zxvf frp_0.31.0_linux_amd64.tar.gz 重命名文件夹 mv frp_0.31.0_linux_amd64 frp cd frp ls 这里服务端，我们只需要关注 frps 、frps.ini 编辑 frps.ini 文件 [common] bind_port = 7000 dashboard_port = 7500 token = 12345678 dashboard_user = admin dashboard_pwd = admin bind_port表示用于客户端和服务端连接的端口，这个端口号我们之后在配置客户端的时候要用到。 dashboard_port是服务端仪表板的端口，若使用7500端口，在配置完成服务启动后可以通过浏览器访问 公网ip:7500 查看frp服务运行信息。 token是用于客户端和服务端连接的口令，请自行设置并记录，稍后会用到。 dashboard_user和dashboard_pwd表示打开仪表板页面登录的用户名和密码，自行设置即可。 运行 ./frps -c frps.ini 运行 nohup ./frps -c frps.ini \u0026 后台运行 ps -ef|grep frp 找frp应用进程 客户端设置 frp的客户端就是我们想要真正进行访问的那台设备，大多数情况下应该会是一台Windows主机，这里使用Windows主机做例子；Linux配置方法类似。 在 github 找到你电脑架构对应的版本下载 解压 https://github.com/fatedier/frp/releases/download/v0.31.0/frp_0.31.0_windows_amd64.zip 编辑 frpc.ini 这个文件 [common] server_addr = 服务器地址 server_port = 7000 token = 12345678 [rdp] 规则名称自定义 type = tcp 用什么协议 local_ip = 127.0.0.1 local_port = 8521 映射本地端口 remote_port = 8521 服务器端口 映射本地端口 运行 ./frpc -c frpc.ini 完整配置 frps的完整配置文件（服务器） frpc的完整配置文件（客户端） ","date":"2020-01-06","objectID":"/%E4%BD%BF%E7%94%A8frp%E8%BF%9B%E8%A1%8C%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/:0:0","tags":["内网穿透"],"title":"使用frp进行内网穿透","uri":"/%E4%BD%BF%E7%94%A8frp%E8%BF%9B%E8%A1%8C%E5%86%85%E7%BD%91%E7%A9%BF%E9%80%8F/"},{"categories":["Redis"],"content":"慢查询 Redis慢查询分析 与MySQL一样:当执行时间超过极大值时，会将发生时间、耗时、 命令记录； redis命令生命周期：发送 排队 执行 返回,慢查询只统计第3个执行步骤的时间 Redis如何设置 动态设置6379:\u003e config set slowlog-log-slower-than 10000 //10毫秒 使用config set完后,若想将配置持久化保存到redis.conf，要执行config rewrite ;前提是你根据redis.conf 执行 redis.conf修改：找到slowlog-log-slower-than 10000 ，修改保存即可 注意：slowlog-log-slower-than =0记录所有命令 -1命令都不记录 Redis慢查询原理 慢查询记录也是存在队列里的，slow-max-len 存放的记录最大条数，比如设置的slow-max-len＝10，当有第11条慢查询命令插入时，队列的第一条命令就会出列，第11条入列到慢查询队列中， 可以config set动态设置，也可以修改redis.conf 获取队列里慢查询的命令：slowlog get 获取慢查询列表当前的长度：slowlog len 对慢查询列表清理（重置）：slowlog reset 对于线上slow-max-len配置的建议：线上可加大slow-max-len的值，记录慢查询存 长命令时redis会做截断，不会占用大量内存，线上可设置1000以上 对于线上slowlog-log-slower-than配置的建议：默认为10毫秒，根据redis并发量来调整，对于高并发比建议为1毫秒 慢查询是先进先出的队列，访问日志记录出列丢失，需定期执行slowlog get,将结果 存储到其它设备中 Redis性能测试(在外部请求) redis-benchmark -h 192.168.42.111 -p 6379 -c 100 -n 10000 100个并发连接，10000个请求，检测服务器性能 redis-benchmark -h 192.168.42.111 -p 6379 -q -d 100 测试存取大小为100字节的数据包的性能 redis-benchmark -h 192.168.42.111 -p 6379 -t set,get -n 100000 -q 只测试 set,lpush操作的性能 redis-benchmark -h 192.168.42.111 -p 6379 -n 100000 -q script load “redis.call(‘set’,‘foo’,‘bar’)” 只测试某些数值存取的性能 ","date":"2019-09-29","objectID":"/redis%E5%AD%A6%E4%B9%A04-%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90%E4%B8%8Eresp%E5%8D%8F%E8%AE%AE/:0:1","tags":["Redis","RESP协议"],"title":"Redis学习(4)-慢查询分析与RESP协议","uri":"/redis%E5%AD%A6%E4%B9%A04-%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90%E4%B8%8Eresp%E5%8D%8F%E8%AE%AE/"},{"categories":["Redis"],"content":"RESP协议 Redis服务器与客户端通过RESP（Redis Protocol specification）协议通信 (aof 文件就是通过RESP存储的) Simple to implement. Fast to parse. Human readable. 好实现、解析快、易于理解 set test 5 我执行了这个命令 产生的RESP 可以在aof文件里面查看 *3 -----表示后面有几组数据 set test 5 所以就是3 $3 -----set 的长度 为3 set -----执行的命令 $4 -----test的长度 为4 test ----执行的命令 $1 ----5的长度 5 ----执行的命令 应用 我们可以通过这个在客户端产生成这样的数据通过socket连接redis 发送给他(jedis客户端的原理就是这样) 我们也可以通过把数据库数据查询出来格式化成resp的格式请求redis(就做到了备份数据库) mysql -utest -ptest stress --default-character-set=utf8 --skip-column-names --raw \u003c order.sql | redis-cli -h 192.168.42.111 -p 6379 -a 12345678 --pipe 1. 使用用户名和密码登陆连接数据库 2. 登陆成功后执行order.sql的select语句得到查询结果集result 3. 使用密码登陆Redis 4. Redis登陆成功后, 使用PIPE管道将result导入Redis. PIPELINE操作流程 大多数情况下，我们都会通过请求-相应机制去操作redis。只用这种模式的一般的步骤是，先获得jedis实例，然后通过jedis的get/put方法与redis交互。由于redis是单线程的，下一次请求必须等待上一次请求执行完成后才能继续执行。然而使用Pipeline模式，客户端可以一次性的发送多个命令，无需等待服务端返回。这样就大大的减少了网络往返时间，提高了系统性能。但是要用好、用对还是得深度学习 流程 使用PIPELINE可以解决网络开销的问题 原理也非常简单,流程如下, 将多个指令打包后,一次性提交到Redis, 网络通信只有一次 ","date":"2019-09-29","objectID":"/redis%E5%AD%A6%E4%B9%A04-%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90%E4%B8%8Eresp%E5%8D%8F%E8%AE%AE/:0:2","tags":["Redis","RESP协议"],"title":"Redis学习(4)-慢查询分析与RESP协议","uri":"/redis%E5%AD%A6%E4%B9%A04-%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90%E4%B8%8Eresp%E5%8D%8F%E8%AE%AE/"},{"categories":["Redis"],"content":"设计一个缓存系统，不得不要考虑的问题就是：缓存穿透、缓存击穿与失效时的雪崩效应。 这里不涉及一级缓存还是二级缓存，主要是讲述使用缓存的时候可能会遇到的一些问题以及一些解决办法 我们使用缓存的时候流程一般是这样： 当我们查询一条数据时，先去查询缓存，如果缓存有就直接返回，如果没有就去查询数据库，然后返回并缓存。 这种情况下就可能会出现一些现象。 缓存穿透 正常情况下，我们去查询数据都是存在。 那么请求查询一条压根儿数据库中根本就不存在的数据，也就是缓存和数据库都查询不到这条数据，但是请求每次都会打到数据库上面去。 这种查询不存在数据的现象我们称为缓存穿透。在流量大时，DB压力很大，要是有人利用不存在的key频繁攻击我们的应用，这就是漏洞。 解决方案 缓存空值 之所以会发生穿透，就是因为缓存中没有存储这些空数据的key。从而导致每次查询都到数据库去了。那么我们就可以为这些key对应的值设置为null 丢到缓存里面去。后面再出现查询这个key 的请求的时候，直接返回null 。这样就不用在到数据库中去走一圈了，但是别忘了设置过期时间 BloomFilter布隆过滤器 BloomFilter 类似于一个hbase set 用来判断某个元素（key）是否存在于某个集合中。这种方式在大数据场景应用比较多，比如 Hbase 中使用它去判断数据是否在磁盘上。还有在爬虫场景判断url是否已经被爬取过。这种方案可以加在上面方案中，在缓存之前在加一层 BloomFilter，在查询的时候先去 BloomFilter 去查询 key 是否存在，如果不存在就直接返回，存在再走查缓存 -\u003e 查 DB。 如何选择 针对于一些恶意攻击，攻击带过来的大量key 是不存在的，那么我们采用第一种方案就会缓存大量不存在key的数据。此时我们采用第一种方案就不合适了，我们完全可以先对使用第二种方案进行过滤掉这些key。 针对这种key异常多、请求重复率比较低的数据，我们就没有必要进行缓存，使用第二种方案直接过滤掉。 对于空数据的key有限的，重复率比较高的，我们则可以采用第一种方式进行缓存。 缓存击穿 缓存击穿是我们可能遇到布隆过滤器缓存方案可能遇到的问题。 在平常高并发的系统中，大量的请求同时查询一个 key 时，此时这个key正好失效了，就会导致大量的请求都打到数据库上面去。这种现象我们称为缓存击穿。 解决方案 上面的现象是多个线程同时去查询数据库的这条数据，那么我们可以在第一个查询数据的请求上使用一个 互斥锁来锁住它。其他的线程走到这一步拿不到锁就等着(自旋)，等第一个线程查询到了数据，然后做缓存。后面的线程进来发现已经有缓存了，就直接走缓存。 缓存雪崩 缓存雪崩的情况是说，当某一时刻发生大规模的缓存失效的情况，比如你的缓存服务宕机了，会有大量的请求进来直接打到DB上面。结果就是DB 称不住，挂掉。 解决方案 方案转自乔二爷 针对缓存服务器宕机的情况分三步 事前-使用集群缓存，保证缓存服务的高可用 这种方案就是在发生雪崩前对缓存集群实现高可用，如果是使用 Redis，可以使用 主从+哨兵 ，Redis Cluster 来避免 Redis 全盘崩溃的情况。 事中-ehcache本地缓存 + Hystrix限流\u0026降级,避免MySQL被打死 使用 ehcache 本地缓存的目的也是考虑在 Redis Cluster 完全不可用的时候，ehcache 本地缓存还能够支撑一阵。可以不用。 使用 Hystrix进行限流 \u0026 降级 ，比如一秒来了5000个请求，我们可以设置假设只能有一秒 2000个请求能通过这个组件，那么其他剩余的 3000 请求就会走限流逻辑。然后去调用我们自己开发的降级组件（降级），比如设置的一些默认值呀之类的。以此来保护最后的 MySQL 不会被大量的请求给打死。 事后开启Redis持久化机制，尽快恢复缓存集群(AOF、RDB) 解决热点数据集中失效问题-也属于雪崩 我们设置缓存时采用了相同的过期时间，导致缓存在某一时刻同时失效，请求全部转发到DB，DB瞬时压力过重雪崩。 解决方案 为了避免这些热点的数据集中失效，那么我们在设置缓存过期时间的时候，我们让他们失效的时间错开。比如在一个基础的时间上加上或者减去一个范围内的随机值 结合上面的击穿的情况，在第一个请求去查询数据库的时候对他加一个互斥锁，其余的查询请求都会被阻塞住，直到锁被释放，从而保护数据库。但是也是由于它会阻塞其他的线程，此时系统吞吐量会下降。需要结合实际的业务去考虑是否要这么做 ","date":"2019-09-27","objectID":"/redis%E5%AD%A6%E4%B9%A02-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E7%A9%BF%E9%80%8F%E9%9B%AA%E5%B4%A9%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE/:0:0","tags":["Redis","缓存击穿","缓存穿透","缓存雪崩","热点数据"],"title":"Redis学习(2)-缓存击穿、穿透、雪崩、热点数据","uri":"/redis%E5%AD%A6%E4%B9%A02-%E7%BC%93%E5%AD%98%E5%87%BB%E7%A9%BF%E7%A9%BF%E9%80%8F%E9%9B%AA%E5%B4%A9%E7%83%AD%E7%82%B9%E6%95%B0%E6%8D%AE/"},{"categories":["Redis"],"content":"Redis的常用数据类型 String 使用场景 Key的设计注意事项 一般以业务功能模块： 比如购物车key: cart:001,表示1号用户的购物车,简短明了以主,节约内存。 简单字符缓存 set key value get key 结构体或对象的存储 set user:1 value //value为XML或Json格式 mset user:1:name deer user:1:age 18 mget user1:name user:1:age Redis分布式锁 计数功能-点赞 INCR article:001 GET article:001 集群环境下的Session共享 使用spring session与redis完成session共享 Hash 使用场景 购物车之类–表结构的数据都可以用hash 全选功能-获取所有该用户的所有购物车商品 商品数量-购物车图标上要显示购物车里商品的总数 删除-要能移除购物车里某个商品 增加或减少某个商品的数量 如何设计实现？ hmset cart:001 prod:01 1 prod:02 1 指令说明：当前登录用户ID号做为KEY，商品ID号为Field, 加入购物车数量为value List 使用场景 利用List实现栈、队列 先进后出：栈 =LPUSH+LPOP-\u003eFILO 先进先出：队列=LPUSH + RPOP-\u003eFIFO Blocking MQ（阻塞队列）=LPUSH +BRPOP 订阅号发布消息之类的 Set 使用场景 抽奖功能 微信有一个活动，活动ID为001，如何实现微信抽奖功能，基于Redis设计实现？ userId: 004 当Lison点击参与抽奖时，数据放入set集合 sadd act:001 004 开始抽奖2名中奖者 srandmember act:001 2 取出两个但是不移除 spop act:001 2 取出两个并移除 查看有多少用户参加了本次抽奖 smembers act:001 用户关系设计 set的特殊命令 setA={A,B,C} setB={B, C} 集合与集合之间的交集 sinter setA setB－－\u003e得到集合{B,C} 集合与集合之间的并集 sunion setA setB －－\u003e得到集合{A,B,C} 集合与集合之间的差集 sdiff setA setB－－\u003e得到集合{A} 1）James老师关注的人 sadd jamesCares lison,peter,king,av 2) Lison老师关注的人 sadd lisonCares james,av,cjk,king 3) av老师关注的人 sadd avCares deer,cjk,king －－－－－－－－－－－－－－ 1）James和lison共同关注的人 sinter jamesCares lisonCares , 计算结果为 {av, king} 2) 我关注的人也关注他(king) sismember lisonCares king lisonCares关注king没 sismember avCares king avCares关注king没 3）我可能认识的人 SDIFF lisonCares jamesCares-\u003e {james.cjk} Zset 有序集合 常用于排行榜，如视频网站需要对用户上传视频做排行榜，或点赞数与集合有联系，不能有重复的成员 新闻话题榜 话题榜Redis设计实现, 以日期做为Key 1）点击话题增加1 zincrby topic:20190912 1 军嫂怒怼张馨予 2) 排行实现,展示今日前9排名 从大到小 zrevrange topic:20190912 0 9 withscores 3）统计近3日点击数据 zunionstore 新key 统计的key数量 topic:20190910 topic:20190911 topic:20190912 可以加WEIGHTS改变数值 4) 展示近3日的排行前9名 zrevrange topic:20190910-20190912 0 9 withscores ","date":"2019-09-27","objectID":"/redis%E5%AD%A6%E4%B9%A03-%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/:0:0","tags":["Redis","使用场景"],"title":"Redis学习(3)-使用场景","uri":"/redis%E5%AD%A6%E4%B9%A03-%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/"},{"categories":["Redis"],"content":"docker简单安装设置密码并开启持久化 docker run -d --name myredis -p 6379:6379 redis --requirepass \"156967\" --appendonly yes ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:1","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"文档 文档学习 ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:2","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"特性 速度快 数据放内存中是速度快的主要原因、C语言实现，与操作系统距离近、使用了单线程架构，预防多线程可能产生的竞争问题 丰富的功能：value可以为string、hash、list、set、zset等多种数据结构，可以满足很多应用场景。还提供了键过期，发布订阅，事务，流水线。(流水线: Redis 的流水线功能允许客户端一次将多个命令请求发送给服务器， 并将被执行的多个命令请求的结果在一个命令回复中全部返回给客户端， 使用这个功能可以有效地减少客户端在执行多个命令时需要与服务器进行通信的次数) 高可用和分布式：哨兵机制实现高可用，保证redis节点故障发现和自动转移 键值对的数据结构服务器 持久化：发生断电或机器故障，数据可能会丢失，可以持久化到硬盘通过(aof、rdb) 主从复制：实现多个相同数据的redis副本 ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:3","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"使用场景 缓存：合理使用缓存加快数据访问速度，降低后端数据源压力 排行榜：按照热度排名，按照发布时间排行，主要用到列表和有序集合 计数器应用：视频网站播放数，网站浏览数，使用redis计数 社交网络：赞、踩、粉丝、下拉刷新 消息队列：发布和订阅 ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:4","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"工具说明 可执行文件 作用 redis-server 启动redis redis-cli redis命令行客户端 redis-benchmark 基准测试工具 redis-check-aof AOF持久化文件检测和修复工具 redis-check-dump RDB持久化文件检测和修复工具 redis-sentinel 启动哨兵 ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:5","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"数据类型 数据类型(String) 设值命令： //10秒后过期 px 10000 毫秒过期 set age 23 ex 10 //不存在键name时，返回1设置成功；存在的话失败0 setnx name test //存在键age时，返回1成功 set age 25 xx //存在则返回value, 不存在返回nil 获值命令：get age // mset k v k v 批量设值：mset country china city beijing //返回china beigjin, address为nil 批量获取：mget country city address 常用命令–字符串计数 incr age //必须为整数自加1，非整数返回错误，无age键从0自增返回1 decr age //整数age减1 incrby age 2 //整数age+2 decrby age 2//整数age -2 incrbyfloat score 1.1 //浮点型score+1.1 常用命令–字符串追加 append追加指令： set name hello; append name world //追加后成helloworld 字符串长度： set hello “世界”；strlen hello //结果6，每个中文占3个字节 截取字符串： set name helloworld ; getrange name 2 4//返回 llo 数据类型(Hash) 哈希hash是一个String类型的field和value的映射表，hash特适合用于存储对象 hmset user:1 name yakax age 18 设值 命令 hset key field value 设值：hset user:1 name yakax //成功返回1，失败返回0 取值：hget user:1 name //返回yakax 删值：hdel user:1 age //返回删除的个数 计算个数：hset user:1 name yakax; hset user:1 age 23; hlen user:1 //返回2，user:1有两个属性值 批量设值：hmset user:2 name yakax age 23 sex boy //返回OK 批量取值：hmget user:2 name age sex //返回三行：yakax 23 boy 判断field是否存在：hexists user:2 name //若存在返回1，不存在返回0 获取所有field: hkeys user:2 // 返回name age sex三个field 获取user:2所有value：hvals user:2 // 返回yakax 23 boy 获取user:2所有field与value：hgetall user:2 //name age sex yakax 23 boy值 增加1：hincrby user:2 age 1 //age+1 hincrbyfloat user:2 age 2 //浮点型加2 三种方案实现用户信息存储优缺点 原生： set user:1:name yakax; set user:1:age 23; set user:1:sex boy; 优点：简单直观，每个键对应一个值 缺点：键数过多，占用内存多，用户信息过于分散，不用于生产环境 将对象序列化存入redis set user:1 serialize(userInfo); 优点：编程简单，若使用序列化合理内存使用率高 缺点：序列化与反序列化有一定开销，更新属性时需要把userInfo全取出来进行反序列化，更新后再序列化到redis 使用hash类型： hmset user:1 name yakax age 23 sex boy 优点：简单直观，使用合理可减少内存空间消耗 缺点：要控制ziplist与hashtable两种编码转换，且hashtable会消耗更多内存serialize(userInfo); 内部编码：ziplist\u003c压缩列表\u003e和hashtable\u003c哈希表\u003e 当field个数少且没有大的value时，内部编码为ziplist 如：hmset user:3 name yakax age 24; object encoding user:3 //返回ziplist 当value大于64字节，内部编码由ziplist变成hashtable 如：hset user:4 address “fsgst64字节”; object encoding user:3 //返回hashtable 数据类型(List) 用来存储多个有序的字符串，一个列表最多可存2的32次方减1个元素 添加命令： rpush yakax c b a //从右向左插入cba, 返回值3 lrange yakax 0 -1 //从左到右获取列表所有元素 返回 c b a lpush key c b a //从左向右插入cba linsert yakax before b teacher //在b之前插入teacher, after为之后，使 用lrange yakax 0 -1 查看：c teacher b a 查找命令： lrange key start end //索引下标特点：从左到右为0到N-1 lindex yakax -1 //返回最右末尾a，-2返回b llen yakax //返回当前列表长度 lpop yakax //把最左边的第一个元素c删除 rpop yakax //把最右边的元素a删除 数据类型(Set) 用户标签，社交，查询有共同兴趣爱好的人,智能推荐,保存多元素，与列表不一样的是不允许有重复元素，且集合是无序，一个集合最多可存2的32次方减1个元素，除了支持增删改查，还支持集合交集、并集、差集； exists user //检查user键值是否存在 sadd user a b c//向user插入3个元素，返回3 sadd user a b //若再加入相同的元素，则重复无效，返回0 smember user //获取user的所有元素,返回结果无序 srem user a //返回1，删除a元素 scard user //返回2，计算元素个数 场景 标签，社交，查询有共同兴趣爱好的人,智能推荐 使用方式： 给用户添加标签： sadd user:1:fav basball fball pq sadd user:2:fav basball fball 或给标签添加用户 sadd basball:users user:1 user:2 sadd fball:users user:1 user:2 计算出共同感兴趣的人： sinter user:1:fav user:2:fav 数据类型－有序集合(ZSet) 常用于排行榜，如视频网站需要对用户上传视频做排行榜，或点赞数与集合有联系，不能有重复的成员。 添加 键 分数 key ZADD page_rank 15 1223 指令： zadd key score member [score member......] zadd user:zan 200 yakax //yakax的点赞数1, 返回操作成功的条数1 zadd user:zan 200 yakax 120 mike 100 lee // 返回3 zadd test:1 nx 100 yakax //键test:1必须不存在，主用于添加 zadd test:1 xx incr 200 yakax //键test:1必须存在，主用于修改,此时为300 zadd test:1 xx ch incr -299 yakax //返回操作结果1，300-299=1 zrange test:1 0 -1 withscores //查看点赞（分数）与成员名 zcard test:1 //计算成员个数， 返回1 排名场景： zadd user:3 200 yakax 120 mike 100 lee //先插入数据 zrange user:3 0 -1 withscores //查看分数与成员 zrank user:3 yakax //返回名次：第3名返回2，从0开始到2，共3名 zrevrank user:3 yakax //返回0， 反排序，点赞数越高，排名越前 差别 数据结构 是否允许元素重复 是否有序 有序实现方式 应用场景 列表 是 是 索引下标 时间轴，消息队列 集合 否 否 无 标签，社交 有序集合 否 是 分值 排行榜，点赞数 位图-BitMaps 基于字符串的位操作的数据类型 set k1 a 获取 value 在 offset 处的值（a 对应的 ASCII 码是 97，转换为二进制数据是 01100001） getbit k1 0 修改二进制数据（b 对应的 ASCII 码是 98，转换为二进制数据是 01100010） setbit k1 6 1 setbit k1 7 0 get k1 统计二进制位中 1 的个数 bitcount k1 获取第一个 1 或者 0 的位置 bitpos k1 1 bitpos k1 0 BITOP 命令支持 AND 、 OR 、 NOT 、 XOR 这四种操作中的任意一种参数： BITOP AND destkey srckey1 … srckeyN ，对一个或多个 key 求逻辑与，并将结果保存到 destkey BITOP OR destkey srckey1 … srckeyN，对一个或","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:6","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"全局命令 查看所有键： keys * 键总数 ： dbsize //2个键，如果存在大量键，线上禁止使用此指令 检查键是否存在： exists key //存在返回1，不存在返回0 删除键： del key //del hello school, 返回删除键个数，删除不存在键返回0 键过期： expire key seconds //set name test expire name 10,表示10秒过期 ttl key // 查看剩余的过期时间 键的数据结构类型： type key //返回string,键不存在返回none ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:7","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"数据库命令 redis数据库管理方式 select 0 切换数据库 flushdb 清空当前库 flushall 清空全部 dbsize 数据库大小 ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:8","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["Redis"],"content":"持久化机制 redis是一个支持持久化的内存数据库,也就是说redis需要经常将内存中的数据同步到磁盘来保证持久化，持久化可以避免因进程退出而造成数据丢失； RDB持久化把当前进程数据生成快照（.rdb）文件保存到硬盘的过程，有手动触发和自动触发 手动触发有save和bgsave两命令 save命令：阻塞当前Redis，直到RDB持久化过程完成为止，若内存实例比较大会造成长时间阻塞，线上环境不建议用它 bgsave命令：redis进程执行fork操作创建子线程，由子线程完成持久化，阻塞时间很短（微秒级），是save的优化,在执行redis-cli shutdown关闭redis服务时，如果没有开启AOF持久化，自动执行bgsave; RDB 命令：config set dir /usr/local //设置rdb文件保存路径 备份：bgsave //将dump.rdb保存到usr/local下 恢复：将dump.rdb放到redis安装目录与redis.conf同级目录，重启redis即可 优点： 压缩后的二进制文，适用于备份、全量复制，用于灾难恢复 加载RDB恢复数据远快于AOF方式 缺点： 无法做到实时持久化，每次都要创建子进程，频繁操作成本过高 保存后的二进制文件，存在老版本不兼容新版本rdb文件的问题 AOF 针对RDB不适合实时持久化，redis提供了AOF持久化方式来解决 开启：redis.conf设置：appendonly yes (默认不开启，为no) 默认文件名：appendfilename “appendonly.aof” 所有的写入命令(set hset)会append追加到aof_buf缓冲区中 AOF缓冲区向硬盘做sync同步 随着AOF文件越来越大，需定期对AOF文件rewrite重写，达到压缩 当redis服务重启，可load加载AOF文件进行恢复 配置详解 appendonly yes //启用aof持久化方式 # appendfsync always //每收到写命令就立即强制写入磁盘，最慢的，但是保证完全的持久化，不推荐使用 appendfsync everysec //每秒强制写入磁盘一次，性能和持久化方面做了折中，推荐(默认) no-appendfsync-on-rewrite yes //正在导出rdb快照的过程中,要不要停止同步aof auto-aof-rewrite-percentage 100 //aof文件大小比起上次重写时的大小,增长率100%时,重写 auto-aof-rewrite-min-size 64mb //aof文件,至少超过64M时,重写 aof如何恢复----------------------------------------- 1. 设置appendonly yes； 2. 将appendonly.aof放到dir参数指定的目录； 3. 启动Redis，Redis会自动加载appendonly.aof文件。 因为AOF的特性的原因，aof文件会越来越大，每次命令都是单独记录的所有redis提供了一个重写操作 BGREWRITEAOF 执行一个 AOF文件 重写操作。重写会创建一个当前 AOF 文件的体积优化版本。 即使 BGREWRITEAOF 执行失败，也不会有任何数据丢失，因为旧的 AOF 文件在 BGREWRITEAOF 成功之前不会被修改。 如果 AOF 文件出错了，怎么办 服务器可能在程序正在对 AOF 文件进行写入时停机， 如果停机造成了 AOF 文件出错（corrupt）， 那么 Redis 在重启时会拒绝载入这个 AOF 文件， 从而确保数据的一致性不会被破坏。 为现有的 AOF 文件创建一个备份。 使用 Redis 附带的 redis-check-aof 程序，对原来的 AOF 文件进行修复。 $ redis-check-aof --fix 可以使用 diff -u 对比修复后的 AOF 文件和原始 AOF 文件的备份，查看两个文件之间的不同之处。 重启 Redis 服务器，等待服务器载入修复后的 AOF 文件，并进行数据恢复 持久化顺序 ","date":"2019-09-26","objectID":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/:0:9","tags":["Redis","Redis命令","Redis持久化"],"title":"Redis学习(1)-基本命令与持久化机制","uri":"/redis%E5%AD%A6%E4%B9%A01-%E5%9F%BA%E6%9C%AC%E5%91%BD%E4%BB%A4%E4%B8%8E%E6%8C%81%E4%B9%85%E5%8C%96%E6%9C%BA%E5%88%B6/"},{"categories":["并发编程学习"],"content":"CountDownLatch countdownlatch 是一个同步工具类，它允许一个或多个线程一直等待，直到其他线程的操作执行完毕再执行,countdownlatch 提供了两个方法，一个是 countDown，一个是 await。countdownlatch 初始化的时候需要传入一个整数，在这个整数倒数到 0 之前，调用了 await 方法的程序都必须要等待，然后通过 countDown 来倒数 示例代码 public static void main(String[] args) throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(5); new Thread(() -\u003e { System.out.println(\"Thread1\"); countDownLatch.countDown(); //3-1=2 System.out.println(\"Thread1执行完毕\"); }).start(); new Thread(() -\u003e { System.out.println(\"Thread2\"); countDownLatch.countDown();//2-1=1 System.out.println(\"Thread2执行完毕\"); }).start(); new Thread(() -\u003e { System.out.println(\"Thread3\"); countDownLatch.countDown();//1-1=0 System.out.println(\"Thread3执行完毕\"); }).start(); countDownLatch.await(); } 输出-------------------- Thread1 Thread2 Thread2执行完毕 Thread1执行完毕 Thread3 Thread3执行完毕 -----------不会结束 模拟高并发 public class CountDownLatchDemo extends Thread { static CountDownLatch countDownLatch = new CountDownLatch(1); public static void main(String[] args) { for(int i=0;i\u003c10;i++){ new CountDownLatchDemo().start(); } countDownLatch.countDown(); } @Override public void run() { try { countDownLatch.await(); //阻塞 10个线程 Thread.currentThread } catch (InterruptedException e) { e.printStackTrace(); } //TODO 业务代码 System.out.println(\"ThreadName:\" + Thread.currentThread().getName()); } } 输出----------------- ThreadName:Thread-2 ThreadName:Thread-4 ThreadName:Thread-3 ThreadName:Thread-8 ThreadName:Thread-0 ThreadName:Thread-1 ThreadName:Thread-6 ThreadName:Thread-5 ThreadName:Thread-9 ThreadName:Thread-7 CountDownLatch 分析 我们只需要关系两个方法，一个是 countDown() 方法，另一个是 await() 方法，countDown() 方法每次调用都会将 state 减 1，直到state 的值为 0；而 await 是一个阻塞方法，当 state 减为 0 的时候，await 方法才会返回。await 可以被多个线程调用，大家在这个时候脑子里要有个图：所有调用了await 方法的线程阻塞在 AQS 的阻塞队列中，等待条件满足（state == 0），将线程从队列中一个个唤醒过来 await await() public void await() throws InterruptedException { // 可中断的共享锁 sync.acquireSharedInterruptibly(1); } acquireSharedInterruptibly public final void acquireSharedInterruptibly(int arg) throws InterruptedException { if (Thread.interrupted()) throw new InterruptedException(); if (tryAcquireShared(arg) \u003c 0) //state 如果不等于 0，说明当前线程需要加入到共享锁队列中 doAcquireSharedInterruptibly(arg); } doAcquireSharedInterruptibly private void doAcquireSharedInterruptibly(int arg) throws InterruptedException { //创建一个共享模式的节点添加到队列中 final Node node = addWaiter(Node.SHARED); boolean failed = true; try { // 通过自选不断判断 for (;;) { final Node p = node.predecessor(); if (p == head) { // 就判断尝试获取锁 int r = tryAcquireShared(arg); //r\u003e=0 表示获取到了执行权限，这个时候因为 state!=0，所以不会执行这段代码 if (r \u003e= 0) { setHeadAndPropagate(node, r); p.next = null; // help GC failed = false; return; } } //在阻塞线程，这也就是为什么会捕获这个异常 if (shouldParkAfterFailedAcquire(p, node) \u0026\u0026 parkAndCheckInterrupt()) throw new InterruptedException(); } } finally { if (failed) cancelAcquire(node); } } setHeadAndPropagate 通过doReleaseShared()来解决唤醒 把全部节点改为head头结点 private void setHeadAndPropagate(Node node, int propagate) { Node h = head; // Record old head for check below setHead(node); if (propagate \u003e 0 || h == null || h.waitStatus \u003c 0 || (h = head) == null || h.waitStatus \u003c 0) { Node s = node.next; if (s == null || s.isShared()) doReleaseShared(); } } countDown 由于线程被 await 方法阻塞了，所以只有等到countdown 方法使得 state=0 的时候才会被唤醒 只有当 state 减为 0 的时候，tryReleaseShared 才返回 true, 否则只是简单的 state = state - 1 如果 state=0, 则调用 doReleaseShared唤醒处于 await 状态下的线程 releaseShared public final boolean releaseShared(int arg) { if (tryReleaseShared(arg)) { doReleaseShared(); return true; } return false; } protected boolean tryReleaseShared(int releases) { // 递减计数;转换为零时发出信号 for (;;) { int c = getState(); if (c == 0) return false; int nextc = c-1; if (compareAndSetState(c, nextc)) return nextc == 0; } } doReleaseShared 共享锁的释放和独占锁的释放有一定的差别前面唤醒锁的逻辑和独占锁是一样，先判断头结点是不是SIGNAL 状态，如果是，则修改为 0，并且唤醒头结点的下一个节点 PROPAGATE： 标识为 PROPAGATE 状态的节点，是共享锁模式下的节点状态，处于这个状态下的节点，会对线程的唤醒进行传播 private void doReleaseShared() { for (;;) { Node h = head; if (h != null \u0026\u0026 h != tail) { int ws = h.waitStatus; if (ws =","date":"2019-07-30","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A06countdownlatchsemaphorecyclicbarrier/:0:1","tags":["CountDownLatch","CyclicBarrier","Semaphore"],"title":"并发编程学习(6)CountDownLatch、Semaphore、CyclicBarrier","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A06countdownlatchsemaphorecyclicbarrier/"},{"categories":["并发编程学习"],"content":"Semaphore semaphore 也就是我们常说的信号灯，semaphore 可以控制同时访问的线程个数，通过 acquire 获取一个许可，如果没有就等待，通过 release 释放一个许可。有点类似限流的作用。叫信号灯的原因也和他的用处有关，比如某商场就 5 个停车位，每个停车位只能停一辆车，如果这个时候来了 10 辆车，必须要等前面有空的车位才能进入；比较常见的就是做限流操作； 案例 public class SemaphoreDemo { //限流（AQS） //permits; 令牌(5) //公平和非公平 static class Car extends Thread{ private int num; private Semaphore semaphore; public Car(int num, Semaphore semaphore) { this.num = num; this.semaphore = semaphore; } public void run(){ try { semaphore.acquire(); //获得一个令牌, 如果拿不到令牌，就会阻塞 System.out.println(\"第\"+num+\" 抢占一个车位\"); Thread.sleep(2000); System.out.println(\"第\"+num+\" 开走喽\"); semaphore.release(); } catch (InterruptedException e) { e.printStackTrace(); } } } public static void main(String[] args) { // 可以基于公平与非公平锁 Semaphore semaphore=new Semaphore(5); for(int i=0;i\u003c10;i++){ new Car(i,semaphore).start(); } } } 创建 Semaphore 实例的时候，需要一个参数 permits，这个基本上可以确定是设置给 AQS 的 state 的，然后每个线程调用 acquire 的时候，执行 state = state - 1，release 的时候执行 state = state + 1，当然，acquire 的时候，如果 state = 0，说明没有资源了，需要等待其他线程 release。 ","date":"2019-07-30","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A06countdownlatchsemaphorecyclicbarrier/:0:2","tags":["CountDownLatch","CyclicBarrier","Semaphore"],"title":"并发编程学习(6)CountDownLatch、Semaphore、CyclicBarrier","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A06countdownlatchsemaphorecyclicbarrier/"},{"categories":["并发编程学习"],"content":"CyclicBarrier CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续工作。CyclicBarrier 默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用 await 方法告诉 CyclicBarrier 当前线程已经到达了屏障，然后当前线程被阻塞 案例 public class DataImportThread extends Thread{ private CyclicBarrier cyclicBarrier; private String path; public DataImportThread(CyclicBarrier cyclicBarrier, String path) { this.cyclicBarrier = cyclicBarrier; this.path = path; } @Override public void run() { System.out.println(\"开始导入：\"+path+\" 数据\"); //TODO 可以写业务 try { cyclicBarrier.await(); //阻塞 condition.await() } catch (InterruptedException e) { e.printStackTrace(); } catch (BrokenBarrierException e) { e.printStackTrace(); } } } public class CycliBarrierDemo extends Thread{ @Override public void run() { System.out.println(\"开始进行数据分析\"); } //循环屏障 //可以使得一组线程达到一个同步点之前阻塞. public static void main(String[] args) { CyclicBarrier cyclicBarrier=new CyclicBarrier (3,new CycliBarrierDemo()); new Thread(new DataImportThread(cyclicBarrier,\"file1\")).start(); new Thread(new DataImportThread(cyclicBarrier,\"file2\")).start(); new Thread(new DataImportThread(cyclicBarrier,\"file3\")).start(); } 输出-------------------------- 开始导入：file3 数据 开始导入：file2 数据 开始导入：file1 数据 开始进行数据分析 } 对于指定计数值 parties，若由于某种原因，没有足够的线程调用 CyclicBarrier 的 await，则所有调用 await 的线程都会被阻塞 同样的 CyclicBarrier 也可以调用 await(timeout, unit)，设置超时时间，在设定时间内，如果没有足够线程到达，则解除阻塞状态，继续工作 通过 reset 重置计数，会使得进入 await 的线程出现BrokenBarrierException 如果采用是 CyclicBarrier(int parties, RunnablebarrierAction) 构造方法，执行 barrierAction 操作的是最后一个到达的线程 CyclicBarrier 相比 CountDownLatch 来说，要简单很多，源码实现是基于 ReentrantLock 和 Condition 的组合使用;CyclicBarrier 和 CountDownLatch 很像，只是 CyclicBarrier 可以有不止一个栅栏，因为它的栅栏（Barrier）可以重复使用（Cyclic） ","date":"2019-07-30","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A06countdownlatchsemaphorecyclicbarrier/:0:3","tags":["CountDownLatch","CyclicBarrier","Semaphore"],"title":"并发编程学习(6)CountDownLatch、Semaphore、CyclicBarrier","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A06countdownlatchsemaphorecyclicbarrier/"},{"categories":["并发编程学习"],"content":"Condition 在前面学习 synchronized 的时候，有wait/notify 的基本使用，结合 synchronized 可以实现对线程的通信。那么，既然 J.U.C 里面提供了锁的实现机制，那 J.U.C 里面应该也有提供线程通信的机制,Condition 是一个多线程协调通信的工具类，可以让某些线程一起等待某个条件(condition)，只有满足条件时，线程才会被唤醒。 ","date":"2019-07-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/:1:0","tags":["Condition"],"title":"并发编程学习(5)Condition","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/"},{"categories":["并发编程学习"],"content":"测试代码 Signal public class Signal implements Runnable{ private Lock lock; private Condition condition; public Signal(Lock lock, Condition condition) { this.lock = lock; this.condition = condition; } @Override public void run() { lock.lock(); try { System.out.println(\"Signal--开始\"); condition.signal(); System.out.println(\"Signal--结束\"); }finally { lock.unlock(); } } } Wait public class Wait implements Runnable{ private Lock lock; private Condition condition; public Wait(Lock lock, Condition condition) { this.lock = lock; this.condition = condition; } @Override public void run() { lock.lock(); try { try { System.out.println(\"wait--开始\"); condition.await(); System.out.println(\"wait--结束\"); } catch (InterruptedException e) { e.printStackTrace(); } }finally { lock.unlock(); } } } 测试 public static void main(String[] args) { Lock lock = new ReentrantLock(); Condition condition = lock.newCondition(); new Thread(new Wait(lock, condition)).start(); new Thread(new Conditions(lock, condition)).start(); } ------------------------输出 wait--开始 Signal--开始 Signal--结束 wait--结束 ","date":"2019-07-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/:1:1","tags":["Condition"],"title":"并发编程学习(5)Condition","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/"},{"categories":["并发编程学习"],"content":"condition分析 调用 Condition，需要获得 Lock 锁，所以意味着会存在一个 AQS 同步队列。先进入await方法分析 public final void await() throws InterruptedException { if (Thread.interrupted()) //这也就是表示await的线程允许被中断 这是lock的一大特性 throw new InterruptedException(); //如果当前线程被中断，则抛出InterruptedException Node node = addConditionWaiter(); //创建一个新的节点，节点状态为 condition，采用的数据结构仍然是链表 int savedState = fullyRelease(node); //释放当前的锁，得到锁的状态，并唤醒 AQS 队列中的一个线程 int interruptMode = 0; //如果当前节点没有在同步队列上，即还没有被 signal，则将当前线程阻塞 while (!isOnSyncQueue(node)) { //判断这个节点是否在 AQS 队列上，第一次判断的是 false，因为前面已经释放锁了 LockSupport.park(this); //通过 park 挂起当前线程 if ((interruptMode = checkInterruptWhileWaiting(node)) != 0) break; } // 当这个线程醒来,会尝试拿锁, 当 acquireQueued返回 false 就是拿到锁了. // interruptMode != THROW_IE -\u003e 表示这个线程没有成功将 node 入队,但 signal 执行了 enq 方法让其入队了. if (acquireQueued(node, savedState) \u0026\u0026 interruptMode != THROW_IE) // 将这个变量设置成 REINTERRUPT. interruptMode = REINTERRUPT; // 如果 node 的下一个等待者不是 null, 则进行清理,清理 Condition 队列上的节点 如果是 null ,就没有什么好清理了. if (node.nextWaiter != null) // clean up if cancelled unlinkCancelledWaiters(); // 如果线程被中断了,需要抛出异常.或者什么都不做 if (interruptMode != 0) reportInterruptAfterWait(interruptMode); } addConditionWaiter 这个方法的主要作用是把当前线程封装成 Node，添加到等待队列。这里的队列不再是双向链表，而是单向链表 private Node addConditionWaiter() { //如 果 lastWaiter 不 等 于 空 并 且waitStatus 不等于 CONDITION 时，把冲好这个节点从链表中移除 Node t = lastWaiter; // If lastWaiter is cancelled, clean out. if (t != null \u0026\u0026 t.waitStatus != Node.CONDITION) { unlinkCancelledWaiters(); t = lastWaiter; } //构建一个 Node，waitStatus=CONDITION。这里的链表是一个单向的，所以相比 AQS 来说会简单很多 Node node = new Node(Thread.currentThread(), Node.CONDITION); if (t == null) firstWaiter = node; else t.nextWaiter = node; lastWaiter = node; return node; } fullyRelease 彻底的释放锁，什么叫彻底呢，就是如果当前锁存在多次重入，那么在这个方法中只需要释放一次就会把所有的重入次数归零。 final int fullyRelease(Node node) { boolean failed = true; try { // 获得重入的次数 int savedState = getState(); // 释放锁并且唤醒下一个同步队列中的线程 if (release(savedState)) { failed = false; return savedState; } else { throw new IllegalMonitorStateException(); } } finally { if (failed) node.waitStatus = Node.CANCELLED; } } isOnSyncQueue 判断当前节点是否在同步队列中，返回 false 表示不在，返回 true 表示在 如果不在 AQS 同步队列，说明当前节点没有唤醒去争抢同步锁，所以需要把当前线程阻塞起来，直到其他的线程调用 signal 唤醒 如果在 AQS 同步队列，意味着它需要去竞争同步锁去获得执行程序执行权限为什么要做这个判断呢？原因是在 condition 队列中的节 点会重新加入到 AQS 队列去竞争锁。也就是当调用 signal的时候，会把当前节点从 condition 队列转移到 AQS 队列 如果 ThreadA 的 waitStatus 的状态为 CONDITION，说明它存在于 condition 队列中，不在 AQS 队列。因为AQS 队列的状态一定不可能有 CONDITION 如果 node.prev 为空，说明也不存在于 AQS 队列，原因是 prev=null 在 AQS 队列中只有一种可能性，就是它是head 节点，head 节点意味着它是获得锁的节点。 如果 node.next 不等于空，说明一定存在于 AQS 队列中，因为只有 AQS 队列才会存在 next 和 prev 的关系 findNodeFromTail，表示从 tail 节点往前扫描 AQS 队列，一旦发现 AQS 队列的节点和当前节点相等，说明节点一定存在于 AQS 队列中 final boolean isOnSyncQueue(Node node) { if (node.waitStatus == Node.CONDITION || node.prev == null) return false; if (node.next != null) // If has successor, it must be on queue return true; /* * node.prev可以是非空的，但尚未在队列中， * 因为将CAS放在队列中的CAS可能会失败。因此，我们必须从尾部进行遍历， * 以确保它实际成功。在调用这种方法时，总是接近尾部，除非CAS失败（这不太可能）， * 它将在那里，所以我们几乎不会遍历很多 */ return findNodeFromTail(node); } Condition.signal await 方法会阻塞 wait，然后 Signal 抢占到了锁获得了执行权限，这个时候在 Signal 中调用了 Condition的 signal()方法，将会唤醒在等待队列中节点 public final void signal() { //先判断当前线程是否获得了锁，这个判断比较简单，直接用获得锁的线程和当前线程相比即可 所以你如果没获得锁来调用这个方法是不对的，会抛这个异常 if (!isHeldExclusively()) throw new IllegalMonitorStateException(); //拿到 Condition队列上第一个节点 Node first = firstWaiter; if (first != null) doSignal(first); } doSignal 对 condition 队列中从首部开始的第一个 condition 状态的节点，执行 transferForSignal 操作，将 node 从 condition队列中转换到 AQS 队列中，同时修改 AQS 队列中原先尾节点的状态 private void doSignal(Node first) { do { // 从 Condition 队列中删除 first 节点 if ( (firstWaiter = first.nextWaiter) == null) lastWaiter = null; // 将 next 节点设置成 null first.nextWaiter = null; } while (!transferForSignal(first) \u0026\u0026 (first = firstWaiter) != null); } AQS.transferForSignal final boolean transferForSignal(Node node) { //更新节点的状态为 0，如果更新失败，只有一种可能就是节点被 CANCELLED(取消) 了 if (!compareAndSetWaitStatus(node, Node.CONDITION, 0)) return false; //调用 enq--这是aqs的方法了，把当前节点添加到AQS 队列。并且返回返回","date":"2019-07-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/:1:2","tags":["Condition"],"title":"并发编程学习(5)Condition","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/"},{"categories":["并发编程学习"],"content":"图 ","date":"2019-07-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/:1:3","tags":["Condition"],"title":"并发编程学习(5)Condition","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A05condition/"},{"categories":["云原生"],"content":"准备 3台机器 安装docker HOST1: centos7.5 :172.16.217.135 zk1 HOST2: centos7.5 :172.16.217.136 zk2 HOST3: centos7.5 :172.16.217.137 zk3 先把防火墙开放三个端口 sudo firewall-cmd --zone=public --add-port=2181/tcp --permanent sudo firewall-cmd --zone=public --add-port=2888/tcp --permanent sudo firewall-cmd --zone=public --add-port=3888/tcp --permanent sudo firewall-cmd --reload 2181：对cline端提供服务 3888：选举leader使用 2888：集群内机器通讯使用（Leader监听此端口） 检查防火墙规则 firewall-cmd --list-all 防火墙的一些命令 //临时关闭防火墙,重启后会重新自动打开 systemctl restart firewalld //检查防火墙状态 firewall-cmd --state firewall-cmd --list-all //Disable firewall systemctl disable firewalld systemctl stop firewalld systemctl status firewalld //Enable firewall systemctl enable firewalld systemctl start firewalld systemctl status firewalld 分别安装容器 并跟随docker启动 HOST1 docker run -d --restart=always --net=host --name=zookeeper1 zookeeper HOST2 docker run -d --restart=always --net=host --name=zookeeper2 zookeeper HOST3 docker run -d --restart=always --net=host --name=zookeeper3 zookeeper 编辑zoo.cfg 与 myid 我这里是先编辑其中一个然后copy到容器里面 三个容器 zoo.cfg是一样的 文件是在/conf/下面 docker cp 文件 容器id:文件 //copy文件到容器 docker cp 容器id:文件 文件 //copy容器文件到宿主机 dataDir=/data dataLogDir=/datalog tickTime=2000 initLimit=5 syncLimit=2 autopurge.snapRetainCount=3 autopurge.purgeInterval=0 maxClientCnxns=60 standaloneEnabled=true admin.enableServer=true clientPort=2181 server.1=172.16.217.135:2888:3888 server.2=172.16.217.136:2888:3888 server.3=172.16.217.137:2888:3888 myid修改为对应server后面的id 重启 docker restart 容器id 查看日志 docker logs -f 容器id 注意myid是否是正确就说明编辑的东西是否生效了日志也有打印 观察状态 进入容器 docker exec -it 容器id bash bin/zkServer.sh status 最后效果 ","date":"2019-07-24","objectID":"/docker%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2zookeeper%E9%9B%86%E7%BE%A4/:0:0","tags":["ZooKeeper"],"title":"Docker分布式部署zookeeper集群","uri":"/docker%E5%88%86%E5%B8%83%E5%BC%8F%E9%83%A8%E7%BD%B2zookeeper%E9%9B%86%E7%BE%A4/"},{"categories":["面试"],"content":"Java开发手册下载链接 1-0.9=0.1是天经地义的，但在计算机的世界里，0.1恰恰是无法精确表示的一个小数，只有2的幂次倍小数才能够精确表示，如：0.5、0.25、0.125等。由于0.1是近似表达，在各种情形中的计算存在数位的取舍精度不一样，所以1-0.9未必等于0.9-0.8，所以浮点数之间的等值判断，基本数据类型不能用==来比较，包装数据类型不能用equals来判断。包装类型运算也是调用的浮点类型的计算 public class FloatPrimitiveTest { public static void main(String[] args) { float a = 1.0f - 0.9f; float b = 0.9f - 0.8f; if (a == b) { System.out.println(\"true\"); } else { System.out.println(\"false\"); } } } 输出------------false public class FloatWrapperTest { public static void main(String[] args) { Float a = Float.valueOf(1.0f - 0.9f); Float b = Float.valueOf(0.9f - 0.8f); if (a.equals(b)) { System.out.println(\"true\"); } else { System.out.println(\"false\"); } } } 输出--------false 正例-------------------------------- (1) 指定一个误差范围，两个浮点数的差值在此范围之内，则认为是相等的。 float a = 1.0f - 0.9f; float b = 0.9f - 0.8f; float diff = 1e-6f; if (Math.abs(a - b) \u003c diff) { System.out.println(\"true\"); } (2) 使用 BigDecimal 来定义值，再进行浮点数的运算操作。 BigDecimal a = new BigDecimal(\"1.0\"); BigDecimal b = new BigDecimal(\"0.9\"); BigDecimal c = new BigDecimal(\"0.8\"); BigDecimal x = a.subtract(b); BigDecimal y = b.subtract(c); if (x.equals(y)) { System.out.println(\"true\"); } 当 switch 括号内的变量类型为 String 并且此变量为外部参数时，必须先进行 null判断。 public class SwitchTest { public static void main(String[] args) { String param = null; switch (param) { case \"null\": System.out.println(\"null\"); break; default: System.out.println(\"default\"); } } } 抛出NPE异常 为了防止精度损失，禁止使用构造方法 BigDecimal(double)的方式把 double 值转化为 BigDecimal 对象。 BigDecimal(double)存在精度损失风险，在精确计算或值比较的场景中可能会导致业务逻辑异常。 如：BigDecimal g = new BigDecimal(0.1f); 实际的存储值为：0.10000000149 优先推荐入参为 String 的构造方法，或使用 BigDecimal 的 valueOf 方法，此方法内部其实执行了Double 的 toString，而 Double 的 toString 按 double 的实际能表达的精度对尾数进行了截断 推荐使用的方式 BigDecimal recommend1 = new BigDecimal(\"0.1\"); BigDecimal recommend2 = BigDecimal.valueOf(0.1); 在使用阻塞等待获取锁的方式中，必须在 try 代码块之外，并且在加锁方法与 try 代码块之间没有任何可能抛出异常的方法调用，避免加锁成功后，在 finally 中无法解锁。 如果 lock 方法在 try 代码块之内，可能由于其它方法抛出异常，导致在 finally代码块中，unlock 对未加锁的对象解锁，它会调用 AQS 的 tryRelease 方法（取决于具体实现类），抛出 IllegalMonitorStateException 异常。在 Lock 对象的 lock方法实现中可能抛出 unchecked 异常。而在使用尝试机制来获取锁的方式中，比如 tryLock()，在进入业务代码块之前，必须先判断当前线程是否持有锁。 public class LockTest { private final static Lock lock = new ReentrantLock(); public static void main(String[] args) { try { lock.tryLock(); } catch (Exception e) { e.printStackTrace(); } finally { lock.unlock(); } } } 所以这个unlock不会抛出异常 正确加锁写法 Lock lock = new XxxLock(); // ... lock.lock(); try { doSomething(); doOthers(); } finally { lock.unlock(); } ","date":"2019-07-03","objectID":"/java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C-%E8%BD%AC%E8%87%AA%E9%98%BF%E9%87%8C%E4%BA%91%E6%A0%96%E7%A4%BE%E5%8C%BA/:0:0","tags":["基础"],"title":"Java开发手册-(转自阿里云栖社区)","uri":"/java%E5%BC%80%E5%8F%91%E6%89%8B%E5%86%8C-%E8%BD%AC%E8%87%AA%E9%98%BF%E9%87%8C%E4%BA%91%E6%A0%96%E7%A4%BE%E5%8C%BA/"},{"categories":["并发编程学习"],"content":"初步认识JUC Java.util.concurrent 是在并发编程中比较常用的工具类，里面包含很多用来在并发场景中使用的组件。比如线程池、阻塞队列、计时器、同步器、并发集合等等。并发包的作者是大名鼎鼎的 Doug Lea。 ","date":"2019-06-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A04lock/:0:1","tags":["lock"],"title":"并发编程学习(4)Lock","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A04lock/"},{"categories":["并发编程学习"],"content":"lock 在 Lock 接口出现之前，Java 中的应用程序对于多线程的并发安全处理只能基于synchronized 关键字来解决。但是 synchronized 在有些场景中会存在一些短板，也就是它并不适合于所有的并发场景。但是在 Java5 以后，Lock 的出现可以解决synchronized 在某些场景中的短板，它比 synchronized 更加灵活。 Lock 本质上是一个接口，它定义了释放锁和获得锁的抽象方法，定义成接口就意味着它定义了锁的一个标准规范，也同时意味着锁的不同实现。实现 Lock 接口的类有很多，以下为几个常见的锁实现 Lock接口 void lock() // 如果锁可用就获得锁，如果锁不可用就阻塞直到锁释放 void lockInterruptibly() // 和lock()方法相似, 但阻塞的线程可中断，抛 出java.lang.InterruptedException 异常 boolean tryLock() // 非阻塞获取锁;尝试获取锁，如果成功返回 true boolean tryLock(longtimeout, TimeUnit timeUnit)//带有超时时间的获取锁方法 void unlock() // 释放锁 类图 ReentrantLock 表示重入锁，它是唯一一个实现了 Lock 接口的类。重入锁指的是线程在获得锁之后，再次获取该锁不需要阻塞，而是直接关联一次计数器增加重入次数，也就是如果当前线程 t1 通过调用 lock 方法获取了锁之后，再次调用 lock，是不会再阻塞去获取锁的，直接增加重试次数就行了。synchronized 和 ReentrantLock 都是可重入锁。 重入锁的设计目的 比如调用 demo 方法获得了当前的对象锁，然后在这个方法中再去调用demo2，demo2 中的存在同一个实例锁，这个时候当前线程会因为无法获得demo2 的对象锁而阻塞，就会产生死锁。重入锁的设计目的是避免线程的死锁。 public class ReentrantDemo{ public synchronized void demo(){// main获得对象锁 System.out.println(\"begin:demo\"); demo2(); } public void demo2(){ System.out.println(\"begin:demo1\"); // 在次获得对象锁 synchronized (this){ } } public static void main(String[] args) { ReentrantDemo rd=new ReentrantDemo(); new Thread(rd::demo).start(); } } ReentrantReadWriteLock(读写锁) 重入读写锁，它实现了 ReadWriteLock 接口，在这个类中维护了两个锁，一个是 ReadLock，一个是 WriteLock，他们都分别实现了 Lock接口。读写锁是一种适合读多写少的场景下解决线程安全问题的工具，基本原则是： 读和读不互斥、读和写互斥、写和写互斥。也就是说涉及到影响数据变化的操作都会存在互斥。 读写锁的设计目的 我们以前理解的锁，基本都是排他锁，也就是这些锁在同一时刻只允许一个线程进行访问，而读写锁在同一时刻可以允许多个线程访问，但是在写线程访问时，所有的读线程和其他写线程都会被阻塞。读写锁维护了一对锁，一个读锁、一个写锁;一般情况下，读写锁的性能都会比排它锁好，因为大多数场景读是多于写的。在读多于写的情况下，读写锁能够提供比排它锁更好的并发性和吞吐量. public class RWLock { static ReentrantReadWriteLock wrl=new ReentrantReadWriteLock(); static Map\u003cString,Object\u003e cacheMap=new HashMap\u003c\u003e(); static Lock read=wrl.readLock(); static Lock write=wrl.writeLock(); //线程B/C/D public static final Object get(String key){ System.out.println(\"begin read data:\"+key); read.lock(); //获得读锁-\u003e 阻塞 try { return cacheMap.get(key); }finally { read.unlock(); } } //线程A public static final Object put(String key,Object val){ write.lock();//获得了写锁 try{ return cacheMap.put(key,val); }finally { write.unlock(); } } public static void main(String[] args) { wrl.readLock();//B线程 -\u003e阻塞 wrl.writeLock(); //A线程 //读-\u003e读是可以共享 //读-\u003e写 互斥 //写-\u003e写 互斥 //读多写少的场景 } } 在这个案例中，通过 hashmap 来模拟了一个内存缓存，然后使用读写所来保证这个内存缓存的线程安全性。当执行读操作的时候，需要获取读锁，在并发访问的时候，读锁不会被阻塞，因为读操作不会影响执行结果。在执行写操作是，线程必须要获取写锁，当已经有线程持有写锁的情况下，当前线程会被阻塞，只有当写锁释放以后，其他读写操作才能继续执行。使用读写锁提升读操作的并发性，也保证每次写操作对所有的读写操作的可见性 读锁与读锁可以共享 读锁与写锁不可以共享（排他） 写锁与写锁不可以共享（排他） StampedLock stampedLock 是 JDK8 引入的新的锁机制，可以简单认为是读写锁的一个改进版本，读写锁虽然通过分离读和写的功能使得读和读之间可以完全并发，但是读和写是有冲突的，如果大量的读线程存在，可能会引起写线程的饥饿。stampedLock 是一种乐观的读策略，使得乐观锁完全不会阻塞写线程 ","date":"2019-06-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A04lock/:0:2","tags":["lock"],"title":"并发编程学习(4)Lock","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A04lock/"},{"categories":["并发编程学习"],"content":"ReentrantLock 的实现原理 AQS 在 Lock 中，用到了一个同步队列 AQS，全称 AbstractQueuedSynchronizer，它是一个同步工具也是 Lock 用来实现线程同步的核心组件独占锁，每次只能有一个线程持有锁，比如前面给大家演示的 ReentrantLock 就是以独占方式实现的互斥锁共 享 锁 ， 允 许 多 个线程同时获取锁，并发访问共享资源，比如ReentrantReadWriteLock AQS 队列内部维护的是一个 FIFO 的双向链表，这种结构的特点是每个数据结构都有两个指针，分别指向直接的后继节点和直接前驱节点。所以双向链表可以从任意一个节点开始很方便的访问前驱和后继。每个 Node 其实是由线程封装，当线程争抢锁失败后会封装成 Node 加入到 ASQ 队列中去；当获取锁的线程释放锁以后，会从队列中唤醒一个阻塞的节点(线程) Node 的组成 释放锁以及添加线程对于队列的变化 里会涉及到两个变化 新的线程封装成 Node 节点追加到同步队列中，设置 prev 节点以及修改当前节点的前置节点的 next 节点指向自己 通过 CAS 讲 tail 重新指向新的尾部节点 head 节点表示获取锁成功的节点，当头结点在释放同步状态时，会唤醒后继节点，如果后继节点获得锁成功，会把自己设置为头结点，节点的变化过程如下 涉及到两个变化 修改head节点指向下一个获得锁的节点 新的获得锁的节点，将prev的指针指向null 设置 head 节点不需要用 CAS，原因是设置 head 节点是由获得锁的线程来完成的，而同步锁只能由一个线程获得，所以不需要 CAS 保证，只需要把 head 节点设置为原首节点的后继节点，并且断开原 head 节点的 next 引用即可 ReentrantLock 的源码分析 ReentrantLock 默认是非公平锁，因为无论在什么场景非公平锁的效率都是会大于公平锁的。 时序图 public void lock(){ sync.lock(); } sync是一个静态内部类，它继承了AQS这个抽象类，前面说过AQS是一个同步工具，主要用来实现同步控制。我们在利用这个工具的时候，会继承它来实现同步控制功能。 通过进一步分析，发现Sync这个类有两个具体的实现，分别是 NofairSync(非公平锁), FailSync(公平锁). 公平锁 表示所有线程严格按照FIFO来获取锁 非公平锁 表示可以存在抢占锁的功能，也就是说不管当前队列上是否存在其他线程等待，新线程都有机会抢占锁 NonfairSync.lock 非公平锁和公平锁最大的区别在于，在非公平锁中我抢占锁的逻辑是，不管有没有线程排队，我先上来 cas 去抢占一下 CAS 成功，就表示成功获得了锁 CAS 失败，调用 acquire(1)走锁竞争逻辑 非公平的AQS队列头结点的下一个节点不一定会获得锁，就是因为在非公平锁的释放后，如果节点在cas的时候。另外一个线程正好进来cas锁，如果这个节点没他快，那么锁就会被这个插队的获得 final void lock() { if (compareAndSetState(0, 1)) //通过cas操作来修改state状态，表示争抢锁的操作 setExclusiveOwnerThread(Thread.currentThread());//设置当前获得锁状态的线程 else acquire(1);//尝试去获取锁 } protected final boolean compareAndSetState(int expect, int update) { // See below for intrinsics setup to support this return unsafe.compareAndSwapInt(this, stateOffset, expect, update); } 通过 cas 乐观锁的方式来做比较并替换，这段代码的意思是，如果当前内存中的state 的值和预期值 expect 相等，则替换为 update。更新成功返回 true，否则返回 false. 这个操作是原子的，不会出现线程安全问题，这里面涉及到Unsafe这个类的操作，以及涉及到 state 这个属性的意义。state 是 AQS 中的一个属性，它在不同的实现中所表达的含义不一样，对于重入锁的实现来说，表示一个同步状态。它有两个含义的表示 当 state=0 时，表示无锁状态 当 state\u003e0 时，表示已经有线程获得了锁，也就是 state=1，但是因为ReentrantLock 允许重入，所以同一个线程多次获得同步锁的时候，state 会递增，比如重入 5 次，那么 state=5。而在释放锁的时候，同样需要释放 5 次直到 state=0其他线程才有资格获得锁 Unsafe 类 Unsafe 类是在 sun.misc 包下，不属于 Java 标准。但是很多 Java 的基础类库，包括一些被广泛使用的高性能开发库都是基于 Unsafe 类开发的，比如 Netty、Hadoop、Kafka 等；Unsafe 可认为是 Java 中留下的后门，提供了一些低层次操作，如直接内存访问、线程的挂起和恢复、CAS、线程同步、内存屏障而 CAS 就是 Unsafe 类中提供的一个原子操作，第一个参数为需要改变的对象，第二个为偏移量(即之前求出来的 headOffset 的值)，第三个参数为期待的值，第四个为更新后的值整个方法的作用是如果当前时刻的值等于预期值 var4 相等，则更新为新的期望值 var5，如果更新成功，则返回 true，否则返回 false； acquire(1)方法 通过 tryAcquire 尝试获取独占锁，如果成功返回 true，失败返回 false 如果 tryAcquire 失败，则会通过 addWaiter 方法将当前线程封装成 Node 添加到 AQS 队列尾部 acquireQueued，将 Node 作为参数，通过自旋去尝试获取锁。 public final void acquire(int arg) { if (!tryAcquire(arg) \u0026\u0026 acquireQueued(addWaiter(Node.EXCLUSIVE), arg)) selfInterrupt(); } NonfairSync.tryAcquire 这个方法的作用是尝试获取锁，如果成功返回 true，不成功返回 false它是重写 AQS 类中的 tryAcquire 方法，并且大家仔细看一下 AQS 中 tryAcquire方法的定义，并没有实现，而是抛出异常。 protected final boolean tryAcquire(int acquires) { return nonfairTryAcquire(acquires); } nonfairTryAcquire final boolean nonfairTryAcquire(int acquires) { final Thread current = Thread.currentThread(); //获取当前执行的线程 int c = getState(); //获得 state 的值 if (c == 0) { //表示无锁状态 if (compareAndSetState(0, acquires)) { //cas 替换 state 的值，cas 成功表示获取锁成功 setExclusiveOwnerThread(current); //保存当前获得锁的线程,下次再来的时候不要再尝试竞争锁 return true; } } else if (current == getExclusiveOwnerThread()) { //如果同一个线程来获得锁，直接增加重入次数 int nextc = c + acquires; if (nextc \u003c 0) // overflow throw new Error(\"Maximum lock count exceeded\"); setState(nextc); return true; } return false; } addWaiter 当 tryAcquire 方法获取锁失败以后，则会先调用 addWaiter 将当前线程封装成Node. 入参 mode 表示当前节点的状态，传递的参数是 Node.EXCLUSIVE，表示独占状态。意味着重入锁用到了 AQS 的独占锁功能 将当前线程封装成 Node 当前链表中的 tail 节点是否为空，如果不为空，则通过 cas 操作把当前线程的node 添加到 AQS 队列 如果为空或者 cas 失败，调用 enq 将节点添加到 AQS 队列 private Node addWaiter(Node mode) { Node node = new Node(Thread.currentThread(), mode); // Try the fast path of enq; backup to full enq on failure Node pred = tail; //tail 是 AQS 中表示同比队列队尾的属性，默认是 null if (pred != null) { //tail 不为空的情况下，说明队列中存在节点 node.prev = pred; //把当前线程的 Node 的 prev 指向 tail if (compareAndSetTai","date":"2019-06-27","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A04lock/:0:3","tags":["lock"],"title":"并发编程学习(4)Lock","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A04lock/"},{"categories":["并发编程学习"],"content":"初步认识 Volatile public /*volatile*/ static boolean stop=false; public static void main( String[] args ) throws InterruptedException { Thread t1=new Thread(()-\u003e{ int i=0; while(!stop){ i++; } }); t1.start(); Thread.sleep(1000); stop=true; //true } 定义一个共享变量 stop 在main线程中创建一个子线程 thread，子线程读取到 stop的值做循环结束的条件 main线程中修改stop的值为 true 当 stop没有增加volatile修饰时，子线程对于主线程的 stop=true的修改是不可见的，这样将导致子线程出现死循环 当 stop增加了volatile修饰时，子线程可以获取到主线程对于 stop=true的值，子线程while循环条件不满足退出循环 增加volatile关键字以后，main线程对于共享变量 stop值的更新，对于子线程 thread可见，这就是volatile的作用 ","date":"2019-06-26","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/:0:1","tags":["volatile","可见性","JMM","happen-before"],"title":"并发编程学习(3)线程安全性分析","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/"},{"categories":["并发编程学习"],"content":"volatile 关键字是如何保证可见性的？ 我们可以使用【hsdis】这个工具，来查看前面演示的这段代码的汇编指令，然后在输出的结果中，查找下 lock 指令，会发现，在修改 带有 volatile 修饰的成员变量时，会多一个lock指令。lock是一种控制指令，在多处理器环境下，lock 汇编指令可以基于总线锁或者缓存锁的机制来达到可见性的一个效果。 ","date":"2019-06-26","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/:0:2","tags":["volatile","可见性","JMM","happen-before"],"title":"并发编程学习(3)线程安全性分析","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/"},{"categories":["并发编程学习"],"content":"从硬件层面了解 在并发编程中，线程安全问题的本质其实就是 原子性、有序性、可见性；接下来主要围绕这三个问题进行展开分析其本质，彻底了解可见性的特性 原子性 和数据库事务中的原子性一样，满足原子性特性的操作是不可中断的，要么全部执行成功要么全部执行失败 有序性 编译器和处理器为了优化程序性能而对指令序列进行重排序，也就是你编写的代码顺序和最终执行的指令顺序是不一致的，重排序可能会导致多线程程序出现内存可见性问题 可见性 多个线程访问同一个共享变量时，其中一个线程对这个共享变量值的修改，其他线程能够立刻获得修改以后的值 一台计算机中最核心的组件是 CPU、内存、以及 I/O 设备。在整个计算机的发展历程中，除了 CPU、内存以及 I/O 设备不断迭代升级来提升计算机处理性能之外，还有一个非常核心的矛盾点，就是这三者在处理速度的差异。CPU 的计算速度是非常快的，内存次之、最后是 IO 设备比如磁盘。而在绝大部分的程序中，一定会存在内存访问，有些可能还会存在 I/O 设备的访问为了提升计算性能，CPU 从单核升级到了多核甚至用到了超线程技术最大化提高 CPU 的处理性能，但是仅仅提升CPU 性能还不够，如果后面两者的处理性能没有跟上，意味着整体的计算效率取决于最慢的设备。为了平衡三者的速度差异，最大化的利用 CPU 提升性能，从硬件、操作系统、编译器等方面都做出了很多的优化 CPU 增加了高速缓存 操作系统增加了进程、线程。通过 CPU 的时间片切换最大化的提升 CPU 的使用率 (在IntelPentium4开始，引入了超线程技术，也就是一个CPU核心模拟出2个线程的CPU，实现多线程并行) 编译器的指令优化，更合理的去利用好 CPU 的高速缓存然后每一种优化，都会带来相应的问题，而这些问题也是导致线程安全性问题的根源。为了了解前面提到的可见性问题的本质，我们有必要去了解这些优化的过程 原子性 多线程并行访问同一个共享资源的时候的原子性问题，如果把问题放大到分布式架构里面，这个问题的解决方法就是锁。所以在CPU层面，提供了两种锁的机制来保证原子性 总线锁 处理器会提供一个LOCK#信号，当一个处理器在总线上输出这个信号时，其他处理器的请求会被阻塞，那么该处理器就可以独占共享内存 总线锁有一个弊端，总线锁相当于使得多个CPU由并行执行变成了串行，使得CPU的性能严重下降，所以在P6系列以后的处理器中，引入了缓存锁。 缓存锁 我们只需要保证 多个线程操作同一个被缓存的共享数据的原子性就行，所以只需要锁定被缓存的共享对象即可。所谓缓存锁是指被缓存在处理器中的共享数据，在Lock操作期间被锁定，那么当被修改的共享内存的数据回写到内存时，处理器不在总线上声明LOCK#信号，而是修改内部的内存地址，并通过 缓存一致性机制来保证操作的原子性。 所谓缓存一致性，就是多个CPU核心中缓存的同一共享数据的数据一致性，而(MESI)使用比较广泛的缓存一致性协议。MESI协议实际上是表示缓存的四种状态 M(Modify) 表示共享数据只缓存在当前CPU缓存中，并且是被修改状态，也就是缓存的数据和主内存中的数据不一致 E(Exclusive) 表示缓存的独占状态，数据只缓存在当前CPU缓存中，并且没有被修改 S(Shared) 表示数据可能被多个CPU缓存，并且各个缓存中的数据和主内存数据一致 I(Invalid) 表示缓存已经失效 每个CPU核心不仅仅知道自己的读写操作，也会监听其他Cache的读写操作 CPU的读取会遵循几个原则 如果缓存的状态是I，那么就从内存中读取，否则直接从缓存读取 如果缓存处于M或者E的CPU 嗅探到其他CPU有读的操作，就把自己的缓存写入到内存，并把自己的状态设置为S 只有缓存状态是M或E的时候，CPU才可以修改缓存中的数据，修改后，缓存状态变为M 可见性 首先cpu执行代码时，执行顺序会根据他自己的优化重排序代码执行顺序，这也就导致了乱序访问问题。这也就是加入内存屏障的原因。 内存屏障就是将 store bufferes中的指令写入到内存，从而使得其他访问同一共享内存的线程的可见性。 X86的memory barrier指令包括lfence(读屏障) sfence(写屏障) mfence(全屏障) Store Memory Barrier(写屏障) 告诉处理器在写屏障之前的所有已经存储在存储缓存(store bufferes)中的数据同步到主内存，简单来说就是使得写屏障之前的指令的结果对屏障之后的读或者写是可见的 Load Memory Barrier(读屏障) 处理器在读屏障之后的读操作,都在读屏障之后执行。配合写屏障，使得写屏障之前的内存更新对于读屏障之后的读操作是可见的 Full Memory Barrier(全屏障) 确保屏障前的内存读写操作的结果提交到内存之后，再执行屏障后的读写操作 总的来说，内存屏障的作用可以通过防止CPU对内存的乱序访问来保证共享数据在多线程并行执行下的可见性 有序性 编译器优化重排序，在不改变单线程程序语义的前提下，改变代码的执行顺序 指令集并行的重排序，对于不存在数据依赖的指令，处理器可以改变语句对应指令的执行顺序来充分利用CPU资源 内存系统的重排序，也就是前面说的CPU的内存乱序访问问题 也就是说，我们编写的源代码到最终执行的指令，会经过三种重排序 有序性会带来可见性问题，所以可以通过内存屏障指令来进制特定类型的处理器重排序 所以 volatile这个关键字会有一个lock指令这也就是相当于内存屏障的作用从而实现可见性 因为lock会实现cpu底层的缓存锁。 ","date":"2019-06-26","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/:0:3","tags":["volatile","可见性","JMM","happen-before"],"title":"并发编程学习(3)线程安全性分析","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/"},{"categories":["并发编程学习"],"content":"JMM层面 硬件层面的原子性、有序性、可见性在不同的CPU架构和操作系统中的实现可能都不一样，而Java语言的特性是 write once,run anywhere，意味着JVM层面需要屏蔽底层的差异，因此在JVM规范中定义了JMM(内存模型) 可见性根本原因是就是高速缓存与重排序 JMM属于语言级别的抽象内存模型，可以简单理解为对硬件模型的抽象，它定义了共享内存中多线程程序读写操作的行为规范，也就是在虚拟机中将共享变量存储到内存以及从内存中取出共享变量的底层细节。 通过这些规则来规范对内存的读写操作从而保证指令的正确性，它解决了CPU多级缓存、处理器优化、指令重排序导致的内存访问问题，保证了并发场景下的可见性。 需要注意的是，JMM并没有限制执行引擎使用处理器的寄存器或者高速缓存来提升指令执行速度，也没有限制编译器对指令进行重排序，也就是说在JMM中，也会存在缓存一致性问题和指令重排序问题。只是JMM把底层的问题抽象到JVM层面，再基于CPU层面提供的内存屏障指令，以及限制编译器的重排序来解决并发问题 JMM 层面的内存屏障 为了保证内存可见性，Java 编译器在生成指令序列的适当位置会插入内存屏障来禁止特定类型的处理器的重排序， HappenBefore 它的意思表示的是前一个操作的结果对于后续操作是可见的，所以它是一种表达多个线程之间对于内存的可见性。所以我们可以认为在 JMM 中，如果一个操作执行的结果需要对另一个操作课件，那么这两个操作必须要存在happens-before 关系。这两个操作可以是同一个线程，也可以是不同的线程 JMM 中有建立 happen-before 的规则 public class Demo { int a=0; volatile boolean flag=false; public void writer(){ //线程A a=1; //1 flag=true; //2 } public void reader(){ if(flag){ //3 int x=a; //4 } } } 一个线程中的每个操作，happens-before 于该线程中的任意后续操作; 可以简单认为是 as-if-serial(as-if-serial语义的意思是：不管怎么重排序（编译器和处理器为了提高并行度），（单线程）程序的执行结果不会改变)。单个线程中的代码顺序不管怎么变，对于结果来说是不变的顺序规则表示 1 happenns-before 2; 3 happensbefore 4 volatile 变量规则，对于 volatile 修饰的变量的写的操作，一定 happen-before 后续对于 volatile 变量的读操作；根据 volatile 规则，2 happens before 3 传递性规则，如果 1 happens-before 2; 3happensbefore 4; 那么传递性规则表示: 1 happens-before 4; start 规则 线程执行之前主线程改变的值一定对线程可见 public StartDemo{ int x=0; Thread t1 = new Thread(()-\u003e{ // 主线程调用 t1.start() 之前 // 所有对共享变量的修改，此处皆可见 // 此例中，x==10 }); // 此处对共享变量 x 修改 x = 10; // 主线程启动子线程 t1.start(); } join规则 线程t1改变的值一定对主线程可见 int x=0; Thread t1=new Thread(()-\u003e{ x=100; }); t1.start(); t1.join(); System.out.println(x); 监视器锁的规则，对一个锁的解锁，happens-before 于随后对这个锁的加锁（其实就是加锁后改变的值后续进来的线程是可以拿到的） synchronized (this) { // 此处自动加锁 // x 是共享变量, 初始值 =10 if (this.x \u003c 12) { this.x = 12; } } // 此处自动解锁 JMM层面解决原子性、有序性、可见性 原子性：Java中提供了两个高级指令 monitorenter和 monitorexit，也就是对应的synchronized同步锁来保证原子性 可见性：volatile、synchronized、final(修饰的东西初始化时就已存在并且不能更改happens-before于随后的操作)都可以解决可见性问题 有序性：synchronized和volatile可以保证多线程之间操作的有序性，volatile会禁止指令重排序 ","date":"2019-06-26","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/:0:4","tags":["volatile","可见性","JMM","happen-before"],"title":"并发编程学习(3)线程安全性分析","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A03%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E6%80%A7%E5%88%86%E6%9E%90/"},{"categories":["并发编程学习"],"content":"synchronized 的基本认识 在多线程并发编程中 synchronized 一直是元老级角色，很多人都会称呼它为重量级锁。但是，随着 Java SE 1.6 对synchronized 进行了各种优化之后，有些情况下它就并不那么重，Java SE 1.6 中为了减少获得锁和释放锁带来的性能消耗而引入的偏向锁和轻量级锁。 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:1","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"synchronized 的基本语法 synchronized 有三种方式来加锁，分别是 修饰实例方法，作用于当前实例加锁，进入同步代码前要获得当前实例的锁 静态方法，作用于当前类对象加锁，进入同步代码前要获得当前类对象的锁 修饰代码块，指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。 public class SyncDemo { Object lock = new Object(); public synchronized void demo1() { } public void demo2() { // TODO synchronized (this) { } // TODO } public synchronized static void demo3() { } public void demo4() { synchronized (SyncDemo.class) { } } public void demo5() { synchronized (lock) { } } } 不同的修饰类型，代表锁的控制粒度 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:2","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"锁如何存储 对象在内存中的布局 在 Hotspot 虚拟机中，对象在内存中的存储布局，可以分为三个区域:对象头(Header)、实例数据(Instance Data)、对齐填充(Padding) MarkWord 普通对象的对象头由两部分组成，分别是markOop以及类元信息，markOop官方称为Mark Word,在Hotspot中，markOop的定义在 markOop.hpp文件中，代码如下 class markOopDesc: public oopDesc { private: // Conversion uintptr_t value() const { return (uintptr_t) this; } public: // Constants enum { age_bits = 4, // 分代年龄 lock_bits = 2, // 所表示 biased_lock_bits = 1, // 是否为偏向锁 max_hash_bits = BitsPerWord - age_bits - lock_bits - biased_lock_bits, hash_bits = max_hash_bits \u003e 31 ? 31 : max_hash_bits, cms_bits = LP64_ONLY(1) NOT_LP64(0), epoch_bits = 2 // 偏向锁的时间戳 }; Mark word记录了对象和锁有关的信息，当某个对象被synchronized关键字当成同步锁时，那么围绕这个锁的一系列操作都和Mark word有关系。Mark Word在32位虚拟机的长度是32bit、在64位虚拟机的长度是64bit。 Mark Word里面存储的数据会随着锁标志位的变化而变化，Mark Word可能变化为存储以下5中情况 32位 64位的变化 锁标志位的表示意义 锁标识 lock=00 表示轻量级锁 锁标识 lock=10 表示重量级锁 偏向锁标识 biased_lock=1表示偏向锁 偏向锁标识 biased_lock=0且锁标识=01表示无锁状态 总结一下前面的内容，synchronized(lock)中的lock可以用Java中任何一个对象来表示，而锁标识的存储实际上就是在lock这个对象中的对象头内 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:3","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"为什么任何对象都可以实现锁 首先，Java中的每个对象都派生自Object类，而每个Java Object在JVM内部都有一个native的C++对象 oop/oopDesc进行对应。 其次，线程在获取锁的时候，实际上就是获得一个监视器对象(monitor) ,monitor可以认为是一个同步对象，所有的Java对象是天生携带monitor. 在hotspot源码的 markOop.hpp文件中；多个线程访问同步代码块时，相当于去争抢对象监视器修改对象中的锁标识。 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:4","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"synchronized 锁的升级 前面提到了锁的几个概念，偏向锁、轻量级锁、重量级锁。在JDK1.6之前，synchronized是一个重量级锁，性能比较差。从JDK1.6开始，为了减少获得锁和释放锁带来的性能消耗，synchronized进行了优化，引入了 偏向锁和 轻量级锁的概念。所以从JDK1.6开始，锁一共会有四种状态，锁的状态根据竞争激烈程度从低到高分别是:无锁状态-\u003e偏向锁状态-\u003e轻量级锁状态-\u003e重量级锁状态。这几个状态会随着锁竞争的情况逐步升级。为了提高获得锁和释放锁的效率，锁可以升级但是不能降级。 下面就详细讲解synchronized的三种锁的状态及升级原理 偏向锁的基本原理 在大多数的情况下，锁不仅不存在多线程的竞争，而且总是由同一个线程获得。因此为了让线程获得锁的代价更低引入了偏向锁的概念。偏向锁的意思是如果一个线程获得了一个偏向锁，如果在接下来的一段时间中没有其他线程来竞争锁，那么持有偏向锁的线程再次进入或者退出同一个同步代码块，不需要再次进行抢占锁和释放锁的操作。 当一个线程访问加了同步锁的代码块时，会在对象头中存储当前线程的 ID，后续这个线程进入和退出这段加了同步锁的代码块时，不需要再次加锁和释放锁。而是直接比较对象头里面是否存储了指向当前线程的偏向锁。如果相等表示偏向锁是偏向于当前线程的，就不需要再尝试获得锁了 偏向锁的获取 首先获取锁 对象的 Markword，判断是否处于可偏向状态。（biased_lock=1、且 ThreadId 为空） 如果是可偏向状态，则通过 CAS 操作，把当前线程的 ID写入到 MarkWord – 如果 cas 成功，那么 markword 就会变成这样。表示已经获得了锁对象的偏向锁，接着执行同步代码块 – 如果 cas 失败，说明有其他线程已经获得了偏向锁，这种情况说明当前锁存在竞争，需要撤销已获得偏向锁的线程，并且把它持有的锁升级为轻量级锁（这个操作需要等到全局安全点，也就是没有线程在执行字节码）才能执行 如果是已偏向状态，需要检查 markword 中存储的ThreadID 是否等于当前线程的 ThreadID – 如果相等，不需要再次获得锁，可直接执行同步代码块 – 如果不相等，说明当前锁偏向于其他线程，需要撤销偏向锁并升级到轻量级锁 CAS:表示自旋锁，由于线程的阻塞和唤醒需要CPU从用户态转为核心态，频繁的阻塞和唤醒对CPU来说性能开销很大。同时，很多对象锁的锁定状态指会持续很短的时间，因此引入了自旋锁，所谓自旋就是一个无意义的死循环，在循环体内不断的重行竞争锁。当然，自旋的次数会有限制，超出指定的限制会升级到阻塞锁。 偏向锁的撤销 偏向锁的撤销并不是把对象恢复到无锁可偏向状态（因为偏向锁并不存在锁释放的概念），而是在获取偏向锁的过程中，发现 cas 失败也就是存在线程竞争时，直接把被偏向的锁对象升级到被加了轻量级锁的状态。 原获得偏向锁的线程如果已经退出了临界区，也就是同步代码块执行完了，那么这个时候会把对象头设置成无锁状态并且争抢锁的线程可以基于 CAS 重新偏向当前线程 如果原获得偏向锁的线程的同步代码块还没执行完，处于临界区之内，这个时候会把原获得偏向锁的线程升级为轻量级锁后继续执行同步代码块 在我们的应用开发中，绝大部分情况下一定会存在 2 个以上的线程竞争，那么如果开启偏向锁，反而会提升获取锁的资源消耗。所以可以通过 jvm 参数UseBiasedLocking 来设置开启或关闭偏向锁** -XX:+UseBiasedLocking开启或者关闭** 轻量级锁的基本原理 当存在超过一个线程在竞争同一个同步代码块时，会发生偏向锁的撤销。偏向锁撤销以后对象会可能会处于两种状态 轻量级锁加锁 JVM会先在当前线程的栈帧中创建用于存储锁记录的空间(LockRecord) 将对象头中的Mark Word复制到锁记录中，称为Displaced Mark Word. 线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针 如果替换成功，表示当前线程获得轻量级锁，如果失败，表示存在其他线程竞争锁，那么当前线程会尝试使用CAS来获取锁，当自旋超过指定次数(可以自定义)时仍然无法获得锁，此时锁会膨胀升级为重量级锁 自旋锁 轻量级锁在加锁过程中，用到了自旋锁，所谓自旋，就是指当有另外一个线程来竞争锁时，这个线程会在原地循环等待，而不是把该线程给阻塞，直到那个获得锁的线程释放锁之后，这个线程就可以马上获得锁的。注意，锁在原地循环的时候，是会消耗 cpu 的，就相当于在执行一个啥也没有的 for 循环。所以，轻量级锁适用于那些同步代码块执行的很快的场景，这样，线程原地等待很短的时间就能够获得锁了。自旋锁的使用，其实也是有一定的概率背景，在大部分同步代码块执行的时间都是很短的。所以通过看似无异议的循环反而能提升锁的性能。但是自旋必须要有一定的条件控制，否则如果一个线程执行同步代码块的时间很长，那么这个线程不断的循环反而会消耗 CPU 资源。默认情况下自旋的次数是 10 次，可以通过 preBlockSpin 来修改 在 JDK1.6 之后，引入了自适应自旋锁，自适应意味着自旋的次数不是固定不变的，而是根据前一次在同一个锁上自旋的时间以及锁的拥有者的状态来决定。如果在同一个锁对象上，自旋等待刚刚成功获得过锁，并且持有锁的线程正在运行中，那么虚拟机就会认为这次自旋也是很有可能再次成功，进而它将允许自旋等待持续相对更长的时间。如果对于某个锁，自旋很少成功获得过，那在以后尝试获取这个锁时将可能省略掉自旋过程，直接阻塞线程，避免浪费处理器资源 轻量级锁的解锁 JVM会先在当前线程的栈帧中创建用于存储锁记录的空间(LockRecord) 将对象头中的Mark Word复制到锁记录中，称为Displaced Mark Word. 线程尝试使用CAS将对象头中的Mark Word替换为指向锁记录的指针 如果替换成功，表示当前线程获得轻量级锁，如果失败，表示存在其他线程竞争锁，那么当前线程会尝试使用CAS来获取锁，当自旋超过指定次数(可以自定义)时仍然无法获得锁，此时锁会膨胀升级为重量级锁 重量级锁的基本原理 private static Object Object; public static void main(String[] args) { Object=new Object(); synchronized (Object){ } } 我们通过javap -v 类 来查看信息 加了同步代码块以后，在字节码中会看到一个monitorenter 和 monitorexit。每一个 JAVA 对象都会与一个监视器 monitor 关联，我们可以把它理解成为一把锁，当一个线程想要执行一段被synchronized 修饰的同步方法或者代码块时，该线程得先获取到 synchronized 修饰的对象对应的 monitor。monitorenter 表示去获得一个对象监视器。monitorexit 表示释放 monitor 监视器的所有权，使得其他被阻塞的线程可以尝试去获得这个监视器 为什么重量级锁的开销比较大呢？ 原因是monitor 依赖操作系统的 MutexLock(互斥锁)来实现的，当系统检查到是重量级锁之后，会把等待想要获取锁的线程阻塞，被阻塞的线程不会消耗CPU，但是阻塞或者唤醒一个线程，都需要通过操作系统来实现，也就是相当于从用户态转化到内核态，而转化状态是需要消耗时间的 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:5","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"回顾线程的竞争机制 synchronized (lock) { // do something java 封装的东西越简单底层封装就越复杂 } 只有 Thread#1 会进入临界区； Thread#1 和 Thread#2 交替进入临界区,竞争不激烈； Thread#1/Thread#2/Thread3… 同时进入临界区，竞争激烈 偏向锁 此时当 Thread#1 进入临界区时，JVM 会将 lockObject 的对象头 Mark Word 的锁标志位设为“01”，同时会用 CAS 操作把 Thread#1 的线程 ID 记录到 Mark Word 中，此时进入偏向模式。所谓“偏向”，指的是这个锁会偏向于 Thread#1，若接下来没有其他线程进入临界区，则 Thread#1 再出入临界区无需再执行任何同步操作。也就是说，若只有Thread#1 会进入临界区，实际上只有 Thread#1 初次进入 临界区时需要执行 CAS 操作，以后再出入临界区都不会有同步操作带来的开销。 轻量级锁 偏向锁的场景太过于理想化，更多的时候是 Thread#2 也会尝试进入临界区， 如果 Thread#2 也进入临界区但是Thread#1 还没有执行完同步代码块时，会暂停 Thread#1并且升级到轻量级锁。Thread#2 通过自旋再次尝试以轻量级锁的方式来获取锁 重量级锁 如果 Thread#1 和 Thread#2 正常交替执行，那么轻量级锁基本能够满足锁的需求。但是如果 Thread#1 和 Thread#2同时进入临界区，那么轻量级锁就会膨胀为重量级锁，意味着 Thread#1 线程获得了重量级锁的情况下，Thread#2就会被阻塞 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:6","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"Synchronized 结合 Java Object 对象中的wait,notify,notifyAll 前面看synchronized的时候，发现被阻塞的线程什么时候被唤醒，取决于获得锁的线程什么时候执行完同步代码块并且释放锁。那怎么做到显示控制呢？我们就需要借助一个信号机制:在 Object 对 象 中，提供了wait/notify/notifyall，可以用于控制线程的状态 先看代码 public class ThreadA extends Thread{ Object lock; public ThreadA(Object lock) { this.lock = lock; } @Override public void run() { synchronized (lock){ try { System.out.println(\"start threadA\"); lock.wait(); System.out.println(\"end threadA\"); } catch (InterruptedException e) { e.printStackTrace(); } } } } public class ThreadB extends Thread{ Object lock; public ThreadB(Object lock) { this.lock = lock; } @Override public void run() { synchronized (lock){ System.out.println(\"start threadB\"); lock.notify(); System.out.println(\"end threadB\"); } } } public static void main(String[] args) { Object lock=new Object(); Thread threadA=new ThreadA(lock); threadA.start(); Thread threadB=new ThreadB(lock); threadB.start(); } 输出---------------------- start threadA start threadB end threadB end threadA 执行流程 wait/notify/notifyall 基本概念 wait：表示持有对象锁的线程 A 准备释放对象锁权限，释放 cpu 资源并进入等待状态。 notify：表示持有对象锁的线程 A 准备释放对象锁权限，通知 jvm 唤 醒 某 个 竞 争 该 对 象 锁 的 线 程 X,线 程 A synchronized 代码执行结束并且释放了锁之后，线程 X 直接获得对象锁权限，其他竞争线程继续等待(即使线程 X 同步完毕，释放对象锁，其他竞争线程仍然等待，直至有新的 notify ,notifyAll 被调用)。 notifyAll：notifyall 和 notify 的区别在于，notifyAll 会唤醒所有竞争同一个对象锁的所有线程，当已经获得锁的线程A 释放锁之后，所有被唤醒的线程都有可能获得对象锁权限 注意：三个方法都必须在synchronized同步关键字所限定的作用域中调用（一定要理解同步的原因），否则会报错java.lang.IllegalMonitorStateException，意思是因为没有同步，所以线程对对象锁的状态是不确定的，不能调用这些方法。 ","date":"2019-06-25","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/:0:7","tags":["synchronized","wait","notify","notifyAll"],"title":"并发编程学习(2)synchronized与锁的唤醒","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A02synchronized%E4%B8%8E%E9%94%81%E7%9A%84%E5%94%A4%E9%86%92/"},{"categories":["并发编程学习"],"content":"单核cpu执行程序的流程 有了进程以后，可以让操作系统从宏观层面实现多应用并发。而并发的实现是通过 CPU 时间片不端切换执行的。对于单核 CPU 来说，在任意一个时刻只会有一个进程在被CPU 调度 线程的出现 在多核 CPU 中，利用多线程可以实现真正意义上的并行执行 在一个应用进程中，会存在多个同时执行的任务，如果其中一个任务被阻塞，将会引起不依赖该任务的任务也 被阻塞。通过对不同任务创建不同的线程去处理，可以提升程序处理的实时性 线程可以认为是轻量级的进程，所以线程的创建、销毁比进程更快 ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:1","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["并发编程学习"],"content":"线程的创建 Runnable 接口 Thread类(本质上是Runnable 的实现) 实现 Callable 接口通过 FutureTask 包装器来创建 Thread 线程 ThreadPool 具体说明后面文章会单独学习 ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:2","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["并发编程学习"],"content":"Java线程的生命周期 Java 线程既然能够创建，那么也势必会被销毁，所以线程是存在生命周期的，那么我们接下来从线程的生命周期开始去了解线程。Thread类里面有个state枚举展示了所有生命周期的状态 线程一共有 6 种状态（NEW、RUNNABLE、BLOCKED、WAITING、TIME_WAITING、TERMINATED） NEW：初始状态，线程被构建，但是还没有调用 start 方法 RUNNABLED：运行状态，JAVA 线程把操作系统中的就绪和运行两种状态统一称为“运行中” BLOCKED：阻塞状态，表示线程进入等待状态,也就是线程因为某种原因放弃了 CPU 使用权，阻塞也分为几种情况 等待阻塞：运行的线程执行 wait 方法，jvm 会把当前线程放入到等待队列 同步阻塞：运行的线程在获取对象的同步锁时，若该同步锁被其他线程锁占用了，那么 jvm 会把当前的线程放入到锁池中 其他阻塞：运行的线程执行 Thread.sleep 或者 t.join 方法，或者发出了 I/O 请求时，JVM 会把当前线程设置为阻塞状态，当 sleep 结束、join 线程终止、io 处理完毕则线程恢复 WAITING: 无限期等待另一个线程执行特定操作的线程处于此状态 TIME_WAITING：超时等待状态，超时以后自动返回 TERMINATED：终止状态，表示当前线程执行完毕 public class ThreadStatsDemo { public static void main(String[] args) { new Thread(()-\u003e{ while (true){ try { TimeUnit.SECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } },\"Time_Waiting_Thread\").start(); new Thread(()-\u003e{ while (true){ synchronized (ThreadStatsDemo.class){ try { ThreadStatsDemo.class.wait(); } catch (InterruptedException e) { e.printStackTrace(); } } } },\"Waiting_Thread\").start(); new Thread(new BlockDemo(),\"blockThread1\").start(); new Thread(new BlockDemo(),\"blockThread2\").start(); } static class BlockDemo extends Thread{ @Override public void run() { synchronized (BlockDemo.class){ try { TimeUnit.SECONDS.sleep(100); } catch (InterruptedException e) { e.printStackTrace(); } } } } } 我们jstack 进入终端看状态 可以看到blockThread1线程锁住了blockdemo 所以blockThread2 无限期等待 这里线程一个个都看的很清楚，所以我们在定义线程时一定要定义名称！ ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:3","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["并发编程学习"],"content":"线程的启动原理 我们进入Thread类的start方法查看 用native修饰的方法只能下载hotspot源码查看 这也就是java能在多平台运行，因为他针对不同的操作系统有不同的处理 http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/00cd9dc3c2b5/src/share/native/java/lang/Thread.c 先通过这个地址找到方法在jvm里面的对应关系 在hotspot源码里面找到jvm.cpp的文件 找到JVM_StartThread 这个方法 我根据他调用的方法找到 之后就没必要看了。我们只需要知道启动线程是通过JVM调用底层操作系统操作启动线程。然后回调run方法。 ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:4","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["并发编程学习"],"content":"线程的终止 线程的终止，并不是简单的调用 stop 命令去。虽然 api 仍然可以调用，但是和其他的线程控制方法如 suspend、 resume 一样都是过期了的不建议使用，就拿 stop 来说，stop 方法在结束一个线程时并不会保证线程的资源正常释 放，因此会导致程序可能出现一些不确定的状态。要优雅的去中断一个线程，在线程中提供了一个 interrupt方法 这是之前通过循环读取一个值来判断是否中断循环的操作：其实thread里面的中断也是类似的思想 thread.isInterrupted() 中断 public static void main(String[] args) throws InterruptedException { Thread thread=new Thread(()-\u003e{ while(!Thread.currentThread().isInterrupted()){//默认是false i++; } System.out.println(\"i:\"+i); }); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); //把isInterrupted设置成true System.out.println(thread.isInterrupted()); //true } 这里抛出了一个InterruptedException 这个后面要分析 我们查看 isInterrupted 这个源代码 在jvm.cpp 里面找到 JVM_IsInterrupted方法 也是调用的OS的方法 不同平台不同操作 ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:5","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["并发编程学习"],"content":"线程的复位 public static void main(String[] args) throws InterruptedException { Thread thread=new Thread(()-\u003e{ while(true){//默认是false if(Thread.currentThread().isInterrupted()){ System.out.println(\"before:\"+Thread.currentThread().isInterrupted()); Thread.interrupted(); //复位- 回到初始状态 System.out.println(\"after:\"+Thread.currentThread().isInterrupted()); } } }); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); //把isInterrupted设置成true } ---------------- before:true after:false ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:6","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["并发编程学习"],"content":"InterruptedException复位异常 public static void main(String[] args) throws InterruptedException { Thread thread=new Thread(()-\u003e{ while(!Thread.currentThread().isInterrupted()){//默认是false try { TimeUnit.SECONDS.sleep(10); //中断一个处于阻塞状态的线程。包括join/wait/queue.take.. System.out.println(\"demo\"); } catch (InterruptedException e) { e.printStackTrace(); break; } } }); thread.start(); TimeUnit.SECONDS.sleep(1); thread.interrupt(); //把isInterrupted设置成true System.out.println(thread.isInterrupted()); //true } ------- java.lang.InterruptedException: sleep interrupted at java.lang.Thread.sleep(Native Method) at java.lang.Thread.sleep(Thread.java:340) at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:386) at com.gupaoedu.vip.ExceptionThreadDemo.lambda$main$0(ExceptionThreadDemo.java:17) at java.lang.Thread.run(Thread.java:748) false 这也就是所有阻塞方法都会抛出一个InterruptedException 的异常他会复位这个线程（这也就是预防阻塞线程一直阻塞的原因） 但是 这个方法是不会中断线程的 他只是告诉一个信号，所以我们需要在抛出异常时 自己在catch方法里面自己选择是否终止； Interrupted 的底层源码做了很多事情，后面还需分析。 ","date":"2019-06-17","objectID":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/:0:7","tags":["Interrupted","thread"],"title":"并发编程学习(1)线程的创建、启动、停止","uri":"/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E5%AD%A6%E4%B9%A01%E7%BA%BF%E7%A8%8B%E7%9A%84%E5%88%9B%E5%BB%BA%E5%90%AF%E5%8A%A8%E5%81%9C%E6%AD%A2/"},{"categories":["Mybatis"],"content":"手写基本流程 流程 定义接口 Mapper 和方法，用来调用数据库操作。Mapper 接口操作数据库需要通过代理类。 定义配置类对象 Configuration。 定义应用层的 API SqlSession。它有一个 getMapper()方法，我们会从配置类Configuration 里面使用 Proxy.newProxyInatance()拿到一个代理对象MapperProxy。 有了代理对象 MapperProxy 之后，我们调用接口的任意方法，就是调用代理对象的 invoke()方法。 代理对象 MapperProxy 的 invoke()方法调用了 SqlSession 的 selectOne()。 SqlSession 只是一个 API，还不是真正的 SQL 执行者，所以接下来会调用执行器 Executor 的 query()方法。 执行器 Executor 的 query()方法里面就是对 JDBC 底层的 Statement 的封装，最终实现对数据库的操作，和结果的返回。 附上手写Mybatis github地址 ps:只是简单的理解加深印象，里面有很多不足。 面试分析 resultType 和 resultMap 的区别？ resultType 是\u003cselect\u003e标签的一个属性，适合简单对象(POJO、JDK 自带类型：Integer、String、Map 等)只能自动映射，适合单表简单查询。 resultMap 是一个可以被引用的标签，适合复杂对象，可指定映射关系，适合关联复合查询。 标签collection 和 标签association 的区别？ association：一对一 collection：一对多、多对多 PrepareStatement 和 Statement 的区别？ 两个都是接口,PrepareStatement 是继承自 Statement 的； Statement 处理静态 SQL，PreparedStatement 主要用于执行带参数的语句； PreparedStatement 的 addBatch()方法一次性发送多个查询给数据库； PS 相似 SQL 只编译一次（对语句进行了缓存，相当于一个函数），减少编译次数； PS 可以防止 SQL 注入； MyBatis 默认值：PREPARED 也就是PreparedStatement MyBatis 解决了什么问题？ 1) 资源管理（底层对象封装和支持数据源） 2）结果集自动映射 3）SQL 与代码分离，集中管理 4）参数映射和动态 SQL 5）其他：缓存、插件等 MyBatis 编程式开发中的核心对象及其作用？ SqlSessionFactoryBuilder 创建工厂类 SqlSessionFactory 创建会话 SqlSession 提供操作接口 MapperProxy 代理 Mapper 接口后，用于找到 SQL 执行 Java 类型和数据库类型怎么实现相互映射？ 通过 TypeHandler，例如 Java 类型中的 String 要保存成 varchar，就会自动调用相应的 Handler。如果没有系统自带的 TypeHandler，也可以自定义--比如之前文章中写的json转对象的操作。 SIMPLE/REUSE/BATCH 三种执行器的区别？ SimpleExecutor 使用后直接关闭 Statement：closeStatement(stmt); ReuseExecutor 放在缓存中，可复用：PrepareStatement:getStatement(); BatchExecutor 支持复用且可以批量执行 update()，通过 ps.addBatch()实现handler.batch(stmt); MyBatis 一级缓存与二级缓存的区别？ 一级缓存：在同一个会话（SqlSession）中共享，默认开启，维护在 BaseExecutor中。 二级缓存：在同一个 namespace 共享，需要在 Mapper.xml 中开启，维护在CachingExecutor 中。 MyBaits 支持哪些数据源类型？ UNPOOLED：不带连接池的数据源。 POOLED ： 带 连 接 池 的 数 据 源 ， 在 PooledDataSource 中 维 护PooledConnection。 JNDI：使用容器的数据源，比如 Tomcat 配置了 C3P0。 自定义数据源：实现 DataSourceFactory 接口，返回一个 DataSource。当 MyBatis 集成到 Spring 中的时候，使用 Spring 的数据源 关联查询的延迟加载是怎么实现的？ 动态代理（JAVASSIST、CGLIB），在创建实体类对象时进行代理，在调用代理对象的相关方法时触发二次查询。 MyBatis 翻页的几种方式和区别？ 逻辑翻页：通过 RowBounds 对象。 物理翻页：通过改写 SQL 语句，可用插件拦截 Executor 实现。 解析全局配置文件的时候，做了什么？ 创建 Configuration，设置 Configuration 解析 Mapper.xml，设置 MappedStatement 没有实现类，MyBatis 的方法是怎么执行的？ MapperProxy 代理，代理类的 invoke()方法中调用了 SqlSession.selectOne() 接口方法和映射器的 statement id 是怎么绑定起来的？（怎么根据接口方法拿到 SQL 语句的？） MappedStatement 对象中存储了 statement 和 SQL 的映射关系 四大对象是什么时候创建的？ Executor：openSession() StatementHandler、ResultsetHandler、ParameterHandler： 执行 SQL 时，在 SimpleExecutor 的 doQuery()中创建 MyBatis 哪些地方用到了代理模式？ 接口查找 SQL：MapperProxy 日志输出：ConnectionLogger、StatementLogger 连接池：PooledDataSource 管理的 PooledConnection 延迟加载：ProxyFactory（JAVASSIST、CGLIB） 插件：Plugin Spring 集成：SqlSessionTemplate 的内部类 SqlSessionInterceptor MyBatis 插件怎么编写和使用？原理是什么? 使用：继承 Interceptor 接口，加上注解，在 mybatis-config.xml 中配置 原理：动态代理，责任链模式，使用 Plugin 创建代理对象 在被拦截对象的方法调用的时候，先走到 Plugin 的 invoke()方法，再走到Interceptor 实现类的 intercept()方法， 最后通过 Invocation.proceed()方法调用被拦截对象的原方法 JDK 动态代理，代理能不能被代理？ 能。层层代理先进后出 MyBatis 集成到 Spring 的原理是什么？ SqlSessionTemplate 中有内部类SqlSessionInterceptor对DefaultSqlSession进行代理； MapperFactoryBean 继 承 了 SqlSessionDaoSupport 获 取SqlSessionTemplate； 接口注册到 IOC 容器中的 beanClass 是 MapperFactoryBean。 DefaulSqlSession 和 SqlSessionTemplate 的区别是什么？ 一个线程安全一个线程不安全 1）为什么 SqlSessionTemplate 是线程安全的？ 其内部类 SqlSessionInterceptor 的 invoke()方法中的 getSqlSession()方法：如果当前线程已经有存在的 SqlSession 对象，会在 ThreadLocal 的容器中拿到SqlSessionHolder，获取 DefaultSqlSession。 如果没有，则会 new 一个 SqlSession，并且绑定到 SqlSessionHolder，放到ThreadLocal 中。SqlSessionTemplate 中在同一个事务中使用同一个 SqlSession。调用 closeSqlSession()关闭会话时，如果存在事务，减少 holder 的引用计数。否则直接关闭 SqlSession。 2) 在编程式的开发中，有什么方法保证 SqlSession 的线程安全？ SqlSessionManager 同时实现了 SqlSessionFactory、SqlSession 接口，通过ThreadLocal 容器维护 SqlSession。 ","date":"2019-05-21","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%906%E7%AE%80%E5%8D%95%E6%89%8B%E5%86%99%E6%80%9D%E8%B7%AF%E5%8F%8A%E9%9D%A2%E8%AF%95%E9%A2%98/:0:0","tags":["Mybatis3源码分析","面试"],"title":"Mybatis3源码分析(6)简单手写思路及面试题","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%906%E7%AE%80%E5%8D%95%E6%89%8B%E5%86%99%E6%80%9D%E8%B7%AF%E5%8F%8A%E9%9D%A2%E8%AF%95%E9%A2%98/"},{"categories":["Mybatis"],"content":"这里我们以传统的 Spring 为例，因为配置更直观，在 Spring 中使用配置类注解是一样的。在前面文章里面，我基于编程式的工程已经弄清楚了 MyBatis 的工作流程、核心模块和底层原理。编程式的工程，也就是 MyBatis 的原生 API 里面有三个核心对象： SqlSessionFactory、SqlSession、MapperProxy 大部分时候我们不会在项目中单独使用 MyBatis 的工程，而是集成到 Spring 里面使用，但是却没有看到这三个对象在代码里面的出现。我们直接注入了一个 Mapper 接口，调用它的方法。 所以有几个关键的问题，我要弄清楚： SqlSessionFactory 是什么时候创建的？ SqlSession 去哪里了？为什么不用它来 getMapper？ 为什么@Autowired 注入一个接口，在使用的时候却变成了代理对象？在 IOC的容器里面我们注入的是什么？ 注入的时候发生了什么事情？ 关键配置 引入mybatis-spring的整合jar包 然后在 Spring 的 applicationContext.xml 里面配置 SqlSessionFactoryBean，它是用来帮助我们创建会话的，其中还要指定全局配置文件和 mapper 映射器文件的路径 \u003cbean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"\u003e \u003cproperty name=\"configLocation\" value=\"classpath:mybatis-config.xml\"\u003e\u003c/property\u003e \u003cproperty name=\"mapperLocations\" value=\"classpath:mapper/*.xml\"\u003e\u003c/property\u003e \u003cproperty name=\"dataSource\" ref=\"dataSource\"/\u003e \u003c/bean\u003e 然后在 applicationContext.xml 配置需要扫描 Mapper 接口的路径。在 Mybatis 里面有几种方式，第一种是配置一个 MapperScannerConfigurer。 \u003cbean id=\"mapperScanner\" class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"\u003e \u003cproperty name=\"basePackage\" value=\"com.gupaoedu.crud.dao\"/\u003e \u003c/bean\u003e 第二种是配置一个\u003cscan\u003e标签： \u003cmybatis-spring:scan base-package=\"com.gupaoedu.crud.dao\"/\u003e 第三种就是直接用@MapperScan 注解，比如我们在 Spring Boot 的启动类上加上一个注解： @SpringBootApplication @MapperScan(\"com.gupaoedu.crud.dao\") public class MybaitsApp { public static void main(String[] args) { SpringApplication.run(MybaitsApp.class, args); } } 创建会话工厂 先记住实现了这几个接口 这个方法是spring IOC容器里面，对象设置属性完成后，可以调用的方法 这个是从ioc容器里面拿到这个对象的时候就会调用这个getobject方法另外两个是获取信息的。 很熟悉的配置文件解析 最后通过parse进入 现在已经是mybatis的操作了。在mybatis 的包里面了 也就是说spring 在创建SqlSessionFactoryBean的时候会通过getobject方法解析对应配置文件创建config设置属性 最后调用了build方法 这个和编程式mybatis单独使用调用的默认的DefaultSqlSessionFactory一样 获得会话 先看看DefaultSqlSessionFactory 我看到官方给的一个注释 Note that this class is not Thread-Safe 实际上我们用DefaultSqlSessionFactory的时候他就是单例的，在多个service之间共享，那一定就会存在线程安全问题。 我们看看mybatis-spring包提供的实现类：进入这个类我就看见注释上就写着 Thread safe, Spring managed, {@code SqlSession} that works with Spring 为什么线程安全呢：我们随便找个方法对比 这个sqlsessionProxy一看就有鬼:这个jdk动态代理又来了 然后我们进入被代理类SqlSessionInterceptor看看 我一步步进入发现是存放在一个threadLocal里面也就是我们获取到的每个DefaultSqlSession都是一个单独的–\u003e也就是一个事务一个sqlsession所以是线程安全的 我继续看他是怎么管理的–\u003e这里就是代理的DefaultSqlSession的–\u003e并且创建了一个事务管理器 我发现只是减少次数没有关闭，之前我们看到，是通过一个事务管理器创建的这个容器；也就是说在我们同一个事务里面可以使用相同的会话，然后每次都减少，直到只有一个的时候就关闭会话 我们知道在 Spring 里面会用 SqlSessionTemplate 替换 DefaultSqlSession，那么接下来看一下怎么在DAO层拿到一个SqlSessionTemplate。MyBatis 里面，它提供了一个 SqlSessionDaoSupport，里面持有一个SqlSessionTemplate 对象，并且提供了一个 getSqlSession()方法，让我们获得一个SqlSessionTemplate。 也就是说我们让 DAO 层的实现类继承 SqlSessionDaoSupport，就可以获得SqlSessionTemplate，然后在里面封装 SqlSessionTemplate 的方法。也就是说很多封装好的增删改的插件基本上就是继承这个SqlSessionDaoSupport早期的ibatis也就是这样来的。 {% raw %} 对象 生命周期 SqlSessionTemplate Spring 中 SqlSession 的替代品，是线程安全的，通过代理的方式调用 DefaultSqlSession 的方法 SqlSessionInterceptor（内部类） 代理对象，用来代理 DefaultSqlSession，在 SqlSessionTemplate 中使用 SqlSessionDaoSupport 用于获取 SqlSessionTemplate，只要继承它即可 MapperFactoryBean 注册到 IOC 容器中替换接口类，继承了 SqlSessionDaoSupport 用来获取SqlSessionTemplate，因为注入接口的时候，就会调用它的 getObject()方法 SqlSessionHolder 控制 SqlSession 和事务 {% endraw %} 接口扫描注册 在MapperScannerConfigurer一步步找到这个方法 在注册时并没有注册这个类本身的beanclass，而是注册了mapperFactoryBean 我们看看官方注释说明 the mapper interface is the original class of the bean but, the actual class of the bean is MapperFactoryBean 那我们去看看mapperFactorBean–\u003e果然继承了SqlSessionDaoSupport 所以：通过spring IOC 扫描注测接口本身的时候替换成了beanclass替换成了MapperFactoryBean这样也就可以拿到SqlSessionTemplate 接口 方法 作用 FactoryBean getObject() 返回由 FactoryBean 创建的 Bean 实例 InitializingBean afterPropertiesSet() bean 属性初始化完成后添加操作 BeanDefinitionRegistryPostProcessor postProcessBeanDefinitionRegistry() 注入 BeanDefination 时添加操作 注入使用 在spring 注入初始化时mapper作为有注解的属性，这个时候会调用MapperFactoryBean–\u003e然后调用getObject方法（因为他继承了SqlSessionDaoSupport所以能拿到SqlSessionTemplate）-\u003e然后就可以拿到我们的mapper代理对象（因为实现了 SqlSession）然后就可以调用mapper的数据库方法了 设计模式总结 {% raw %} 设计模式 类 工厂 SqlSessionFactory、ObjectFactory、MapperProxyFactory 建造者 XMLConfigBuilder、XMLMapperBuilder、XMLStatementBuidler 单例模式 SqlSessionFactory、Configuration、ErrorCont","date":"2019-05-19","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%905spring%E9%9B%86%E6%88%90%E5%88%86%E6%9E%90%E4%B8%8Emybatis%E6%89%80%E7%94%A8%E5%88%B0%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(5)spring集成分析与mybatis所用到的设计模式","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%905spring%E9%9B%86%E6%88%90%E5%88%86%E6%9E%90%E4%B8%8Emybatis%E6%89%80%E7%94%A8%E5%88%B0%E7%9A%84%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["Mybatis"],"content":"源码总结回顾 {% raw %} 对象 相关对象 作用 Configuration MapperRegistry TypeAliasRegistry TypeHandlerRegistry 包含了 MyBatis 的所有的配置信息 SqlSession SqlSessionFactory DefaultSqlSession 对操作数据库的增删改查的 API 进行了封装，提供给应用层使用 Executor BaseExecutor SimpleExecutor BatchExecutor ReuseExecutor MyBatis 执行器，是 MyBatis 调度的核心，负责 SQL 语句的生成和查 询缓存的维护 StatementHandler BaseStatementHandler SimpleStatementHandler PreparedStatementHandler CallableStatementHandler 封装了 JDBC Statement 操作，负责对 JDBC statement 的操作，如设 置参数、将 Statement 结果集转换成 List 集合 ParameterHandler DefaultParameterHandler 把用户传递的参数转换成 JDBC Statement 所需要的参数 ResultSetHandler DefaultResultSetHandler 把 JDBC 返回的 ResultSet 结果集对象转换成 List 类型的集合 MapperProxy MapperProxyFactory 代理对象，用于代理 Mapper 接口方法 MappedStatement SqlSource BoundSql MappedStatement 维护了一条select|update|delete|insert节点 的封装，包括了 SQL 信息、入参信息、出参信息 {% endraw %} ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:1","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":"插件拦截的四大对象 {% raw %} 对象 描述 可拦截的方法 方法作用 Executor 上层的对象，SQL 执行全过程，包括组装参数，组装结果集返回和执行 SQL 过程 update 执行 update、insert、delete 操作 query 执行 query 操作 flushStatements 在 commit 的时候自动调用，SimpleExecutor ReuseExecutor、BatchExecutor 处理不同 commit 提交事务 rollback 事务回滚 getTransaction 获取事务 close 结束（关闭）事务 isClosed 判断事务是否关闭 StatementHandler 上层的对象，SQL 执行全过程，包括组装参数，组装结果集返回和执行 SQL 过程 prepare （BaseSatementHandler）SQL 预编译 parameterize 设置参数 batch 批处理 update 增删改操作 query 查询操作 ParameterHandler SQL 参数组装的过程 getParameterObject 获取参数 setParameters 设置参数 ResultSetHandler 结果的组装 handleResultSets 处理结果集 handleOutputParameters 处理存储过程出参 {% endraw %} ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:2","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":"代理和拦截是怎么实现的? 我们从之前的课程知道插件是通过拦截时通过jdk动态代理的。 四大对象什么时候被代理，也就是：代理对象是什么时候创建的？ 我们还记得Executor上个文章里面讲到，实在openSession时创建的，StatementHandler是SimpleExecutor.doQuery()里面创建的，里面包含了处理参数的 ParameterHandler 和处理结果集的 ResultSetHandler 的创建，创建之后即调用InterceptorChain.pluginAll(),返回层层代理后的对象。 多个插件的情况下，代理能不能被代理？代理顺序和调用顺序的关系？ 谁来创建代理对象？ 我们可以看从pagehelper这个插件进入看看-首先我们先从pageHelper的注册类进去 可以看到实现了interceptor这个类—-这样就形成了代理类 下面这个方法就是mybatis的操作创建插件的方法：这个之后会分析 被代理后，调用的是什么方法？怎么调用到原被代理对象的方法？ 我们可以看到在intercept方法里面拿到执行器是调用的gettarget这个方法，利用代理类调用的，我们进入invocation看看 可以看到可以通过target方法拿到被代理对象 因为代理类是 Plugin，所以最后调用的是 Plugin 的 invoke()方法。它先调用了定义的拦截器的 intercept()方法。可以通过 invocation.proceed()调用到被代理对象被拦截的方法。 再来看看他是怎么对我们的天王代理的 通过openSession 进去找到创建executor的时候 他调用了我们放在Configuration里面的interceptorChain 可以看到调用了所有插件-拿到所有实现了Interceptor的拦截器，然后这里就调用了实现Interceptor的类的plugin这个方法进行创建代理插件代理对象。这样被代理后我们之后的操作都会走到代理类的invoke方法里面。 这样就会调用我们自定义的拦截器的intercept方法–之后里面就是我们自定已的逻辑操作了。 实现Interceptor的三个方法含义 intercept():做我们想要做的事情 可以通过invocation 这个参数获取被拦截对象方法参数或者执行代理方法 –调用proceed可以走到被拦截对象的被拦截方法，这样就走完整个流程了。 plugin(): 创建代理对象，通过代理对象的invoke方法走到我们的插件 setProperties():这是我们在配置我们插件时可以提供很多属性，比如分页插件设置一些参数等等。 ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:3","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":"pageHelper插件案例分析 他是怎么来改写我们的sql的呢–我们首先来看看PageInterceptor里面的自定义逻辑 根据方言进入不同的实现类 然后我们在看看分页参数哪里来的 可以看到这里startPage调用时传进来的，然后通过本地线程里面拿到的（每个线程里面拿到的都是属于自己独有的分页数据） 这也就是写了startPage那句话，就会修改我们的sql 分页–这也就出现一个问题，在一个线程里我们的查询分页就有不稳定性！ 对象 作用 PageInterceptor 自定义拦截器 Page 包装分页参数 PageInfo 包装结果 PageHelper 工具类 ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:4","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":"mybatis插件到底可以干些什么 动态改变数据源。水平分表，通过注解的方式：可以通过invocation拿到方法的注解，在接口上添加注解，通过反射获取接口注解，根据注解上配置的参数进行分表，修改原 SQL，例如 id 取模，按月分表 记录日志。 数据权限—对不同用户查询出来的数据有些筛选 脱敏也是可以的，数据加解密 对数据查询的执行分析，比如执行时长的分析。 ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:5","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":"插件调用的流程图 ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:6","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":"上一个简单的sql执行时长分析插件 type: 表示拦截的类，这里是StatementHandler的实现类 method：表示拦截的方法，这里是拦截StatementHandler的query方法 args：表示方法参数(可以通过数组参数接收然后强转) 看看pagehelper的 SpirngBoot注册插件 ","date":"2019-05-16","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/:0:7","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(4)插件分析","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%904%E6%8F%92%E4%BB%B6%E5%88%86%E6%9E%90/"},{"categories":["Mybatis"],"content":" 分析源码我们还是从编程式demo入手 ","date":"2019-05-15","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/:0:0","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(3)流程走向","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/"},{"categories":["Mybatis"],"content":"我们通过建造者模式创建一个工厂类，配置文件的解析就是在这一步完成的，包括 mybatis-config.xml 和 Mapper 适配器文件。 首先进入build 进入XMLconfigBuilder–这里就是配置文件创建的地方 可以看到这里有很多解析文件的类 解析节点–这里可以看出文件只解析了一次。 解析文件还是比较简单的，基本上就是读取文件的内容然后做存储设置，我们来看看时序图 ","date":"2019-05-15","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/:0:1","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(3)流程走向","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/"},{"categories":["Mybatis"],"content":"DefaultSqlSession–最主要的就是创建了一个执行器 创建回话时拿到全局配置文件的配置，并且创建了事务工厂和创建执行器。 对执行器做判断–两次判断的原因是因为怕有人把默认执行器设置为空–这里也判断了是否开启了二级缓存的–这里注意这个额插件调用方法先把他理解为拦截器进行了一次包装 我在baseExecutor里面还看到了模板模式–把具体的增删改查的方法都交给子类去做了 先看看继承关系 看看时序图 ","date":"2019-05-15","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/:0:2","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(3)流程走向","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/"},{"categories":["Mybatis"],"content":"getMapper 为什么要引入mapper对象——为了解决这种硬编码的问题 一步步走 注意这个注册器-点进去看 这里的代码就有点熟悉了吧。jdk的动态代理那么作为代理类一定有个规范，我们进去看 果然实现了invocationHandler 说明mapper对象是一个代理对象 为什么我们只需要mapper接口不需要去实现呢 目前来看，他的作用就是标识我们去找到对应的statementID找到sql语句他的功能也就完成了。 ","date":"2019-05-15","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/:0:3","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(3)流程走向","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/"},{"categories":["Mybatis"],"content":"MapperProxy 既然是代理对象肯定会走到上图中的invoke方法执行到mapperMethod.exeute中 为什么会判断defaultMethod方法的原因是jdk1.8接口都是可以有defaultMethod方法的，这里的这个cache是为了提升获取mapper的效率—这里调用了一个computeIfAbsent map的方法构建本地缓存 判断类型准备执行的语句类型 走到selectOne 为什么用list接收：我觉得这是一种比较前瞻的写法吧，下面那个报错应该都很熟悉，以前把查出来 的list用对象接收就是这个错误 这里可以看到传进来的是一个statementid的位置字符串，具体怎么找到的就在MappedStatement这行代码解决的。 走到query 的实现方法 –我们进入二级缓存这个实现方法 这里的这个缓存key 就是判断我们缓存是否命中的。判断statementid和翻页信息和sql是否一样 继续走query-他会有缓存走缓存没缓存走查询 这里是判断语句设置的一些参数做操作 走到queryFromDatabase –这样就到了baseExecutor 接下来就是基本执行器的操作了 走到默认的simple执行器 在这里感觉已经离真相越来越近了..这里已经看到了jdbc 的statement了。 根据这个newStatementHandler进入–这里就有我们熟悉的东西赋值了。 newParameterHandler 处理参数 newResultSetHandler 处理结果集 四大天王–能被插件拦截的四大天王 ParameterHandler，ResultSetHandler，StatementHandler，executor 然后对参数进行预编译–然后执行查询–这里已经调用–然后处理结果集 已经调用ps.execute了这已经是jdbc的操作了 ","date":"2019-05-15","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/:0:4","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(3)流程走向","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%903%E6%B5%81%E7%A8%8B%E8%B5%B0%E5%90%91/"},{"categories":["Mybatis"],"content":"工作流程分析 首先在 MyBatis 启动的时候我们要去解析配置文件，包括全局配置文件和映射器配置文件，这里面包含了我们怎么控制 MyBatis 的行为，和我们要对数据库下达的指令，也就是我们的 SQL 信息。我们会把它们解析成一个 Configuration 对象。 接下来就是我们操作数据库的接口，它在应用程序和数据库中间，代表我们跟数据库之间的一次连接：这个就是 SqlSession 对象。 我们要获得一个会话，必须有一个会话工厂SqlSessionFactory。SqlSessionFactory 里面又必须包含我们的所有的配置信息，所以我们会通过一个Builder 来创建工厂类。 我们知道，MyBatis 是对 JDBC 的封装，也就是意味着底层一定会出现 JDBC 的一些核心对象，比如执行 SQL 的 Statement，结果集 ResultSet。在 Mybatis 里面，SqlSession 只是提供给应用的一个接口，还不是 SQL 的真正的执行对象。 我们上次课提到了，SqlSession 持有了一个 Executor 对象，用来封装对数据库的操作。在执行器 Executor 执行 query 或者 update 操作的时候我们创建一系列的对象，来处理参数、执行 SQL、处理结果集，这里我们把它简化成一个对象：StatementHandler， 在阅读源码的时候我们再去了解还有什么其他的对象。 流程图 ","date":"2019-05-04","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%93%E5%AD%98/:0:1","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(2)体系结构与缓存","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%93%E5%AD%98/"},{"categories":["Mybatis"],"content":"MyBatis 架构分层与模块划分 跟Spring 一样，MyBatis 按照功能职责的不同，所有的 package可以分成不同的工作层次。 我们可以把 MyBatis 的工作流程类比成餐厅的服务流程。 第一个是跟客户打交道的服务员，它是用来接收程序的工作指令的，我们把它叫做接口层。 第二个是后台的厨师，他们根据客户的点菜单，把原材料加工成成品，然后传到窗口。这一层是真正去操作数据的，我们把它叫做核心层。 最后就是餐厅也需要有人做后勤（比如清洁、采购、财务），来支持厨师的工作和整个餐厅的运营。我们把它叫做基础层。 接口层 首先接口层是我们打交道最多的。核心对象是 SqlSession，它是上层应用和 MyBatis打交道的桥梁，SqlSession 上定义了非常多的对数据库的操作方法。接口层在接收到调用请求的时候，会调用核心处理层的相应模块来完成具体的数据库操作。 核心处理层 接下来是核心处理层。既然叫核心处理层，也就是跟数据库操作相关的动作都是在这一层完成的。核心处理层主要做了这几件事： 把接口中传入的参数解析并且映射成 JDBC 类型； 解析 xml 文件中的 SQL 语句，包括插入参数，和动态 SQL 的生成； 执行 SQL 语句； 处理结果集，并映射成 Java 对象 插件也属于核心层，这是由它的工作方式和拦截的对象决定的。 基础支持层 最后一个就是基础支持层。基础支持层主要是一些抽取出来的通用的功能（实现复用），用来支持核心处理层的功能。比如数据源、缓存、日志、xml 解析、反射、IO、事务等等这些功能。 ","date":"2019-05-04","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%93%E5%AD%98/:0:2","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(2)体系结构与缓存","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%93%E5%AD%98/"},{"categories":["Mybatis"],"content":"MyBatis 缓存详解 从上图中我们可以看出Cache的默认实现只有PerpetualCache；不过mybatis利用了装饰器模式来扩展的缓存接口：上面包decorators下面的东西就是装饰器。我们也可以自定义缓存类，比如用redis来实现Cache这个类也是可以的，但是都是必须实现Cache这个接口。 所有的缓存实现类总体上可分为三类：基本缓存、淘汰算法缓存、装饰器缓存。 参数的话我一般是使用注解@CacheNamespace到mapper上面也可以配置在Cache标签里面 {% raw %} 缓存实现类 描述 作用 装饰条件 基本缓存 缓存基本实现类 默认是 PerpetualCache,也可以自定义比如RedisCache、EhCache 等，具备基本功能的缓存类 无 LruCache LRU 策略的缓存 当缓存到达上限时候，删除最近最少使用的缓存（Least Recently Use） eviction=\"LRU\"（默认） FifoCache FIFO 策略的缓存 当缓存到达上限时候，删除最先入队的缓存 eviction=\"FIFO\" SoftCache、WeakCache 带清理策略的缓存 通过 JVM 的软引用和弱引用来实现缓存，当 JVM内存不足时，会自动清理掉这些缓存，基于SoftReference 和 WeakReference eviction=\"SOFT\" eviction=\"WEAK\" LoggingCache 带日志功能的缓存 比如：输出缓存命中率 基本 SynchronizedCache 同步缓存 基于 synchronized 关键字实现，解决并发问题 基本 BlockingCache 阻塞缓存 通过在 get/put 方式中加锁，保证只有一个线程操作缓存，基于 Java 重入锁实现 blocking=true SerializedCache 支持序列化的缓存 将对象序列化以后存到缓存中，取出时反序列化 readOnly=false（默认） ScheduledCache 定时调度的缓存 在进行 get/put/remove/getSize 等操作前，判断缓存时间是否超过了设置的最长缓存时间（默认是一小时），如果是则清空缓存--即每隔一段时间清空一次缓存 flushInterval 不为空 TransactionalCache 事务缓存 在二级缓存中使用，可一次存入多个缓存，移除多个缓存 在TransactionalCacheManager 中用 Map维护对应关系 {% endraw %} 可以看出缓存是用map存储的 一级缓存 一级缓存也叫本地缓存，MyBatis的一级缓存是在会话（SqlSession）层面进行缓存的。MyBatis 的一级缓存是默认开启的，不需要任何的配置。 禁用缓存可以使用@Options注解也可以使用在mapper.xml文件里面的flushCache 我们知道，缓存最终会放到PerpetualCache中。 sqlSession默认实现是defaultSession，然而defaultSession里面有个配置文件和执行器，那么执行器又是BaseExecutor维护，缓存就应该在BaseExecutor里面 一级缓存验证 执行更新操作后缓存会失效 其他会话更新了数据，导致读取到脏数据（一级缓存不能跨会话共享） 二级缓存 二级缓存是Namespace级别的，也就是mapper级别的查询会缓存，增删改会刷新缓存，所以开启是针对mapper配置就行了。我一般用@CacheNamespace或者@CacheNamespaceRef(代表引用别的命名空间的Cache配置，两个命名空间的操作使用的是同一个Cache，这个可以解决连表查询导致缓存失效，但是缓存粒度就变大了,刷新缓存会影响多个mapper) 二级缓存是用来解决一级缓存不能跨会话共享的问题的，可以被多个 SqlSession 共享（只要是同一个接口里面的相同方法，都可以共享），生命周期和应用同步。 二级缓存是工作在一级缓存之前的。实际上 MyBatis 用了一个装饰器的类来维护，就是 CachingExecutor。如果启用了二级缓存，MyBatis 在创建 Executor 对象的时候会对 Executor 进行装饰。CachingExecutor 对于查询请求，会判断二级缓存是否有缓存结果，如果有就直接返回，如果没有委派交给真正的查询器 Executor 实现类，比如 SimpleExecutor 来执行查询，再走到一级缓存的流程。最后会把结果缓存起来，并且返回给用户。 二级缓存验证 可以看到已经开启了二级缓存了，共享了sqlsession了。从上图中，我们可以看出，缓存是通过用户提交后才缓存起来的。 MyBatis二级缓存的工作流程和前文提到的一级缓存类似，只是在一级缓存处理前，用CachingExecutor装饰了BaseExecutor的子类，实现了二级缓存的查询和写入功能。 通过执行器进入CachingExecutor打的debug(可以看出一层层装饰最后才到基本实现类) 缓存是通过TransactionalCacheManager存储的 存储TransactionalCache这个对象 这里就已经说明了，可以执行多个操作然后在提交缓存。支持多个缓存的添加与删除。 总结 一级缓存 MyBatis一级缓存的生命周期和SqlSession一致。 MyBatis一级缓存内部设计简单，只是一个没有容量限定的HashMap，在缓存的功能性上有所欠缺。 MyBatis的一级缓存最大范围是SqlSession内部，有多个SqlSession或者分布式的环境下，数据库写操作会引起脏数据，建议设定缓存级别为Statement，这样也就失去了缓存的意义。 二级缓存 MyBatis的二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。 MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本而mybatis官网也提供了很多第三方缓存的方案,但是也是比较鸡肋的。 直接使用Redis、Memcached等分布式缓存可能成本更低，安全性也更高。实际上，一般开发中，要用真正的缓存来提高效率，目前还是单独的第三方缓存方案比较可行。 ","date":"2019-05-04","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%93%E5%AD%98/:0:3","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(2)体系结构与缓存","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%902%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E4%B8%8E%E7%BC%93%E5%AD%98/"},{"categories":["Mybatis"],"content":"单独用mybatis编程式进行DB操作 --------------------利用mapper String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); SqlSession session = sqlSessionFactory.openSession(); try { BlogMapper mapper = session.getMapper(BlogMapper.class); Blog blog = mapper.selectBlogById(1); System.out.println(blog); } finally { session.close(); } --------------------利用mybatis的接口 String resource = \"mybatis-config.xml\"; InputStream inputStream = Resources.getResourceAsStream(resource); SqlSessionFactory sqlSessionFactory = new SqlSessionFactoryBuilder().build(inputStream); SqlSession session = sqlSessionFactory.openSession(); // ExecutorType.BATCH try { Blog blog = (Blog) session.selectOne(\"com.yakax.mapper.BlogMapper.selectBlogById\", 1); System.out.println(blog); } finally { session.close(); } 这案例非常重要，后面分析源码还是基于它。 ","date":"2019-05-02","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/:0:1","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(1)生命周期与核心配置解读以及批量操作","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/"},{"categories":["Mybatis"],"content":"生命周期 我们先从官网介绍开始一步步开始看,我们从每个对象的作用的角度来理解一下，只有理解了它们是干什么的，才知道什么时候应该创建，什么时候应该销毁。 SqlSessionFactoryBuiler 首先是SqlSessionFactoryBuiler。它是用来构建SqlSessionFactory的，而SqlSessionFactory只需要一个，所以只要构建了这一个SqlSessionFactory，它的使命就完成了，也就没有存在的意义了。所以它的生命周期只存在于方法的局部。而SqlSessionFactoryBuilder一般都是配置文件构建SqlSessionFactory。 SqlSessionFactory SqlSessionFactory 是用来创建 SqlSession 的，每次应用程序访问数据库，都需要创建一个会话。因为我们一直有创建会话的需要，所以 SqlSessionFactory 应该存在于应用的整个生命周期中（作用域是应用作用域）。创建 SqlSession 只需要一个实例来做这件事就行了，否则会产生很多的混乱，和浪费资源。所以我们要采用单例模式 SqlSession SqlSession 是一个会话，因为它不是线程安全的，不能在线程间共享。所以我们在请求开始的时候创建一个 SqlSession 对象，在请求结束或者说方法执行完毕的时候要及时关闭它（一次请求或者操作中）。 Mapper Mapper（实际上是一个代理对象）是从 SqlSession 中获取的。它的作用是发送 SQL来操作数据库的数据。它应该在一个SqlSession 事务方法之内。 总结生命周期 对象 生命周期 SqlSessionFactoryBuiler 方法局部（method） SqlSessionFactory（单例） 应用级别（application） SqlSession 请求和操作（request/method） Mapper 方法（method） ","date":"2019-05-02","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/:0:2","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(1)生命周期与核心配置解读以及批量操作","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/"},{"categories":["Mybatis"],"content":"核心配置解读 虽然说现在基本上都是springboot开发，但是我们学习mybatis还是得从他的配置文件开始学习。 configuration configuration 是整个配置文件的根标签，实际上也对应着 MyBatis 里面最重要的配置类 Configuration。它贯穿 MyBatis 执行流程的每一个环节。我们打开这个类看一下，这里面有很多的属性，跟其他的子标签也能对应上。注意：MyBatis 全局配置文件顺序是固定的，否则启动的时候会报错。 我们可以通过build进去找到XMLConfigBuilder在里面他会new一个Configuration；这个configuration就是我们配置类对应的类，并且我们可以在里面看到许多设置的默认值。 属性（properties） 为了避免直接把参数写死在 xml 配置文件中，我们可以把这些参数单独放在properties 文件中，用 properties 标签引入进来，然后在 xml 配置文件中用${}引用就可以了。 \u003cproperties resource=\"db.properties\"\u003e\u003c/properties\u003e 这样也可以把数据库的信息分离出来；把配置文件放在外部，避免修改后重新编译打包，只需要重启应用； 设置（settings） 这是 MyBatis 中极为重要的调整设置，它们会改变 MyBatis 的运行时行为。 描述了设置中各项的意图、默认值等。核心配置。具体可以看官网链接。主要的东西就是懒加载(通过代理用于关联查询)，缓存，查询执行器，日志。 类型别名（typeAliases） TypeAlias 是类型的别名，跟 Linux 系统里面的 alias 一样，主要用来简化全路径类名的拼写。比如我们的参数类型和返回值类型都可能会用到我们的 Bean，如果每个地方都配置全路径的话，那么内容就比较多，还可能会写错。我们可以为自己的 Bean 创建别名，既可以指定单个类，也可以指定一个 package，自动转换。配置了别名以后，只需要写别名就可以了，比如 com.yakax.Blog都只要写 blog 就可以了。我们也可以指定某个包下面，这样包下面的全部默认都简写了。MyBatis 里面有系统预先定义好的类型别名，也是在TypeAliasRegistry 中。 类型处理器（typeHandlers）【重点】 由于 Java 类型和数据库的 JDBC 类型不是一一对应的（比如 String 与 varchar），所以我们把 Java 对象转换为数据库的值，和把数据库的值转换成 Java 对象，需要经过一定的转换，这两个方向的转换就要用到 TypeHandler。这就是为什么数据库类型是varchar而mybatis 晓得对应string 类型一样。因为 MyBatis 已经内置了很多 TypeHandler（在mybatis源码 type 包下），它们全部全部注册在 TypeHandlerRegistry 中，他们都继承了抽象类 BaseTypeHandler，泛型就是要处理的 Java 数据类型。 案例 在开发过程中，我们常常把一些易变动的数据，或者易变动的对象存储为json字符串到数据库中(MySQL从5.7开始就有json格式和json查询语法)，而以前的操作就是,我们查询出来利用string类型接收json字符串，然后在自己利用json解析转成对象封装到VO给前端；而typehandlers可以帮我们解决这一点。 数据库字段与数据(一个id，一个对象，一个list，[对象里面也存在list]) 数据库 create table blogdo (id int not null,blog json null,blog_list json null); 数据 id=1; blog={\"bid\": 1, \"name\": \"yakax\", \"naList\": [{\"na\": \"123\"}, {\"na\": \"234\"}], \"authorId\": 120}; blog_list=[{\"bid\": 1, \"name\": \"yakax\", \"naList\": [{\"na\": \"123\"}], \"authorId\": 120}, {\"bid\": 1, \"name\": \"yakax\", \"naList\": [{\"na\": \"123\"}], \"authorId\": 120}]; DO与json对象 这个是数据库查询转换后的对象 @Data @TableName(\"blogdo\") public class BlogDO { private Integer id; private Blog blog; private List\u003cBlog\u003e blogList; } JSON 对象 @Data public class Blog { private Integer bid; private String name; private Integer authorId; private List\u003cJson\u003e naList; } @Data public class Json { private String na; } 建立处理对象的TypeHandler @MappedTypes({Blog.class, List.class, Json.class})//要处理对应得Java类型 @MappedJdbcTypes(value = JdbcType.BLOB)//要对什么数据库类型进行操作 public class MyTypeHandlerone\u003cE\u003e extends BaseTypeHandler\u003cE\u003e { /** * 处理的类型(注册类型与传类型进来) */ private Class\u003cE\u003e type; /** * 注册类型 * @param type */ public MyTypeHandlerone(Class\u003cE\u003e type) { if (type == null) { throw new IllegalArgumentException(\"Type argument cannot be null\"); } this.type = type; } /** * 设置非空参数 */ @Override public void setNonNullParameter(PreparedStatement preparedStatement, int i, E t, JdbcType jdbcType) throws SQLException { // set进入数据库 preparedStatement.setString(i, JSON.toJSONString(t)); } /** * 获取空结果集（根据列名），一般都是调用这个 */ @Override public E getNullableResult(ResultSet resultSet, String s) throws SQLException { String sqlJson = resultSet.getString(s); if (null != sqlJson) { return JSONObject.parseObject(sqlJson, type); } return null; } /** * 获取空结果集（根据下标值） */ @Override public E getNullableResult(ResultSet resultSet, int i) throws SQLException { String sqlJson = resultSet.getString(i); if (null != sqlJson) { return JSONObject.parseObject(sqlJson, type); } return null; } /** * 存储过程用的 */ @Override public E getNullableResult(CallableStatement callableStatement, int i) throws SQLException { String sqlJson = callableStatement.getString(i); if (null != sqlJson) { return JSONObject.parseObject(sqlJson, type); } return null; } } 然后在配置中心增加type-handlers-package=包地址 测试 BlogDO blogdo = blogMapper.selectById(1); log.info(blogdo.toString()); 从上图中可以看出，typehandlers已经帮我们转化好数据了，注意看数据格式，如果是对象，那么对象下面注册了的东西他都会转化，如果是list，那么他存储对象是用JSONobject，存值是hashMap，存储list是用JSONArray； 我们看看他是在什么时候怎么注册进入的 程序启动时调用构造方法-然后进入TypeHandlerRegistry进行注册， 我们可以看到这个类里面有很多预置的类型对应 这里我们可以看到我们的JAVA类型已经注册进来了 这里也看到他有专门的一个map来存储我们自定义的handler 这就是mybatis自定义typehandlers的查询流程-循环根据MappedTypes({Blog.class, List.class, ","date":"2019-05-02","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/:0:3","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(1)生命周期与核心配置解读以及批量操作","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/"},{"categories":["Mybatis"],"content":"批量操作 开发中批量插入肯定不能使用java 的循环来插入:增删改查批量同理主要是sql的组装。 数据库默认接收数据包大小是4M(max_allowed_packet)所以如果批量过大，会报错 foreach insert into 表名(列名) values (插入的字段值),（插入的字段值），...... 使用Batch Executor查询 使用Batch Executor 需要开启setting里面的defaultExecutorType 里面有三个参数 也可以在会话 SqlSession session = sqlSessionFactory.openSession(ExecutorType.BATCH); SIMPLE 默认 普通的sql 对应jdbc的statement REUSE 执行器会重用预处理语句对应jdbc PreparedStatement ------也就是说我们执行的sql 他会先缓存起来，要用的时候通过缓存拿 BATCH 重用语句并执行批量更新 statement与PreparedStatement 的区别 PreparedStatement对象不仅包含了SQL语句，而且大多数情况下这个语句已经被预编译过，因而当其执行时，只需应用程序运行SQL语句，而不必先编译。加快了访问数据库的速度。这种转换也给你带来很大的便利，不必重复SQL语句的句法，而只需更改其中变量的值，便可重新执行SQL语句。 PreparedStatement接口代表预编译的语句，它主要的优势在于可以减少SQL的编译错误并增加SQL的安全性（减少SQL注射攻击的可能性）因为Statement 接口不接受参数， PreparedStatement 接口运行时接受输入的参数这也就是$与#的区别 statement每次执行sql是需要编译的。 ","date":"2019-05-02","objectID":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/:0:4","tags":["Mybatis3源码分析"],"title":"Mybatis3源码分析(1)生命周期与核心配置解读以及批量操作","uri":"/mybatis3%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E7%94%9F%E5%91%BD%E5%91%A8%E6%9C%9F%E4%B8%8E%E6%A0%B8%E5%BF%83%E9%85%8D%E7%BD%AE%E8%A7%A3%E8%AF%BB%E4%BB%A5%E5%8F%8A%E6%89%B9%E9%87%8F%E6%93%8D%E4%BD%9C/"},{"categories":["面试"],"content":"面试题如下 public class Cat { private int inr; private Cat(int inr) { this.inr = inr; } private static void print(Cat cat) { System.out.println(cat.inr); } private void setInr(int inr) { this.inr = inr; } public static void main(String[] args) { Cat A = new Cat(100); Cat B = new Cat(101); System.out.println(A); System.out.println(B); swap(A, B); Cat.print(A); Cat.print(B); } private static void swap(Cat A, Cat B) { Cat temp = A; A = B; B = temp; System.out.println(A.inr); System.out.println(B.inr); // temp = new Cat(400); // B.setInr(400); // System.out.println(temp.inr); // System.out.println(B.inr); } } 输出 com.yakax.Cat@299a06ac com.yakax.Cat@383534aa 101 100 100 101 从这里的输出我们可以理解到，对象的传值引用只在方法内起作用方法外的地址也是改变不了的。 放掉注释输出 com.yakax.Cat@299a06ac com.yakax.Cat@383534aa 101 100 400 400 400 101 分析 teap已经是新的对象了输出400。 B.setInr 时B对象指向的是A对象的地址，也就是说他吧com.yakax.Cat@299a06ac 这个地址的对象的值改了。所以方法外A打印400。 最后B的地址依旧没有变，打印101。 这里我们只要记住改变指向地址对象的值只需利用属性方法更改。new对象是产生新的对象地址。 ","date":"2019-04-02","objectID":"/%E6%96%B9%E6%B3%95%E5%A4%96%E5%AF%B9%E8%B1%A1%E5%9C%B0%E5%9D%80%E4%BA%A4%E6%8D%A2/:0:0","tags":["对象传值"],"title":"方法外对象地址交换","uri":"/%E6%96%B9%E6%B3%95%E5%A4%96%E5%AF%B9%E8%B1%A1%E5%9C%B0%E5%9D%80%E4%BA%A4%E6%8D%A2/"},{"categories":["Spring"],"content":" 源码地址(带有中文注解)git@github.com:yakax/spring-framework-5.0.2.RELEASE--.git Spring 的设计初衷其实就是为了简化我们的开发 基于 POJO 的轻量级和最小侵入性编程； 通过依赖注入和面向接口松耦合； 基于切面和惯性进行声明式编程； 通过切面和模板减少样板式代码； 而他主要是通过：面向 Bean(BOP)、依赖注入（DI）以及面向切面（AOP）这三种方式来达成的。 面向Bean Spring 是面向Bean的编程(Bean Oriented Programming, BOP),Bean 在 Spring 中才是真正的主角。 IOC(控制反转)：控制反转最常用的实现方式就是通过DI(依赖注入)所以在 Spring 中控制反转也被直接称作依赖注入。 在早期的spring中是有DL(依赖查找)的，后来由于使用频率过低就被移除了。 依赖注入(DI)的基本概念 Spring 设计的核心 org.springframework.beans 包（架构核心是 org.springframework.core 包），它的设计目标是与 JavaBean 组件一起使用。这个包通常不是由用户直接使用，而是由服务器将 其用作其他多数功能的底层中介。下一个最高级抽象是 BeanFactory 接口，它是工厂设计模式的实现， 允许通过名称创建和检索对象。BeanFactory 也可以管理对象之间的关系。 BeanFactory 最底层支持两个对象模型。 单例：提供了具有特定名称的全局共享实例对象，可以在查询时对其进行检索。Singleton 是默 认的也是最常用的对象模型。 原型：确保每次检索都会创建单独的实例对象。在每个用户都需要自己的对象时，采用原型模式。 Bean 工厂的概念是 Spring 作为 IOC 容器的基础。IOC 则将处理事情的责任从应用程序代码转移到 框架。 AOP 编程理念 面向切面编程，即 AOP，是一种编程思想，它允许程序员对横切关注点或横切典型的职责分界线的行为（例如日志和事务管理）进行模块化。AOP 的核心构造是方面（切面），它将那些影响多个类的行为封装到可重用的模块中,AOP 的功能完全集成到了 Spring 事务管理、日志和其他各种特性的上下文中。 Spring5 系统架构 核心容器 spring-core 和 spring-beans Spring 框架的核心模块，包含了控制反转（Inversion ofControl, IOC）和依赖注入（Dependency Injection, DI）。BeanFactory 接口是 Spring 框架中的核心接口，它是工厂模式的具体实现。BeanFactory 使用控制反转对应用程序的配置和依赖性规范与实际的应用程序代码进行了分离。但 BeanFactory 容器实例化后并不会自动实例化 Bean，只有当 Bean 被使用时 BeanFactory 容器才会对该 Bean 进行实例化与依赖关系的装配。 spring-context 核心模块之上，他扩展了 BeanFactory，为她添加了 Bean 生命周期控制、框架事件体系以及资源加载透明化等功能。此外该模块还提供了许多企业级支持，如邮件访问、远程访问、任务调度等，ApplicationContext 是该模块的核心接口，她是 BeanFactory 的超类，与 BeanFactory 不同，ApplicationContext 容器实例化后会自动对所有的单实例 Bean 进行实例化与依赖关系的装配，使之处于待用状态。 spring-context-support 对 Spring IOC 容器的扩展支持，以及 IOC 子容器。 spring-context-indexer Spring 的类管理组件和 Classpath 扫描。 spring-expression 统一表达式语言（EL）的扩展模块，可以查询、管理运行中的对象，同时也方便的可以调用对象方法、操作数组、集合等。它的语法类似于传统 EL，但提供了额外的功能，最出色的要数函数调用和简单字符串的模板函数。这种语言的特性是基于 Spring 产品的需求而设计，他可以非常方便地同 Spring IOC 进行交互。 AOP 和设备支持 spring-aop Spring 的另一个核心模块，是 AOP 主要的实现模块。作为继 OOP 后，对程序员影响最大的编程思想之一，AOP 极大地开拓了人们对于编程的思路。在 Spring 中，他是以 JVM 的动态代理技术为基础，然后设计出了一系列的 AOP 横切实现，比如前置通知、返回通知、异常通知等，同时，Pointcut 接口来匹配切入点，可以使用现有的切入点来设计横切面，也可以扩展相关方法根据需求进行切入。 spring-aspects 集成自 AspectJ 框架，主要是为 Spring AOP 提供多种 AOP 实现方法 spring-instrument 是基于 JAVA SE 中的\"java.lang.instrument\"进行设计的，应该算是 AOP的一个支援模块，主要作用是在 JVM 启用时，生成一个代理类，程序员通过代理类在运行时修改类的字节，从而改变一个类的功能，实现 AOP 的功能。在分类里，我把他分在了 AOP 模块下，在 Spring 官方文档里对这个地方也有点含糊不清，这里是纯个人看法。 数据访问与集成 spring-jdbc Spring 提供的 JDBC 抽象框架的主要实现模块，用于简化 Spring JDBC 操作 。主要是提供 JDBC 模板方式、关系数据库对象化方式、SimpleJdbc 方式、事务管理来简化 JDBC 编程，主要实现类是 JdbcTemplate、SimpleJdbcTemplate 以及 NamedParameterJdbcTemplate。 spring-tx Spring JDBC 事务控制实现模块。使用 Spring 框架，它对事务做了很好的封装，通过它的 AOP 配置，可以灵活的配置在任何一层；但是在很多的需求和应用，直接使用 JDBC 事务控制还是有其优势的。其实，事务是以业务逻辑为基础的；一个完整的业务应该对应业务层里的一个方法；如果业务操作失败，则整个事务回滚；所以，事务控制是绝对应该放在业务层的；但是，持久层的设计则应该遵循一个很重要的原则：保证操作的原子性，即持久层里的每个方法都应该是不可以分割的。所以，在使用 Spring JDBC 事务控制时，应该注意其特殊性。 spring-orm ORM 框架支持模块，主要集成 Hibernate, Java Persistence API (JPA) 和Java Data Objects (JDO) 用于资源管理、数据访问对象(DAO)的实现和事务策略。 spring-oxm 主要提供一个抽象层以支撑 OXM（OXM 是 Object-to-XML-Mapping 的缩写，它是一个 O/M-mapper，将 java 对象映射成 XML 数据，或者将 XML 数据映射成 java 对象），例如：JAXB, Castor, XMLBeans, JiBX 和 XStream 等。 spring-jms （Java Messaging Service）能够发送和接收信息，自 Spring Framework 4.1 以后，他还提供了对 spring-messaging 模块的支撑。 Web组件 -spring-web Spring 提供了最基础 Web 支持，主要建立于核心容器之上，通过 Servlet 或者 Listeners 来初始化 IOC 容器，也包含一些与 Web 相关的支持。 spring-webmvc 模块 众所周知是一个的 Web-Servlet 模块，实现了 Spring MVC（model-view-Controller）的 Web 应用。spring-websocket 模块主要是与 Web 前端的全双工通讯的协议。 spring-web 是一个新的非堵塞函数式 Reactive Web 框架，可以用来建立异步的，非阻塞，事件驱动的服务，并且扩展性非常好 通信报文 spring-messaging 模块 从 Spring4 开始新加入的一个模块，主要职责是为 Spring 框架集成一些基础的报文传送应用。 集成测试 spring-test 主要为测试提供支持的，毕竟在不需要发布（程序）到你的应用服务器或者连接到其他企业设施的情况下能够执行一些集成测试或者其他测试对于任何企业都是非常重要的。 集成兼容 spring-framework-bom Bill of Materials.解决 Spring 的不同模块依赖版本不同问题 各模块关系图 Spring 版本命名规则 {% raw %} 描述方式 说明 含义 Snapshot 快照版 尚不不稳定、尚处于开发中的版本 Release 稳定版 功能相对稳定，可以对外发行，但有时间限制 GA 正式版 代表广泛可用的稳定版(General Availability) M 里程碑版 (M 是 Milestone 的意思）具有一些全新的功能或是具有里程碑意义的版本。 RC 终测版 Release Candidate（最终测试），即将作为正式版发布。 {% endraw %}","date":"2019-03-25","objectID":"/spring%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B8%8E%E7%BB%93%E6%9E%84/:0:0","tags":["Spring5源码分析"],"title":"Spring5源码分析(1)设计思想与结构","uri":"/spring%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%901%E8%AE%BE%E8%AE%A1%E6%80%9D%E6%83%B3%E4%B8%8E%E7%BB%93%E6%9E%84/"},{"categories":["设计模式"],"content":"{% raw %} 分类 设计模式 创建型 工厂方法模式(Factory Method)、抽象工厂模式(Abstract Factory)、建造者模式(Builder)、原型模式(Prototype)、单例模式(Singleton) 结构型 适配器模式(Adapter)、桥接模式(Bridge)、组合模式(Composite)、装饰器模式(Decorator)、门面模式(Facade)、享元模式(Flyweight)、代理模式(Proxy) 行为型 解释器模式(Interpreter)、模板方法模式(Template Method)、责任链模式(Chain of Responsibility)、命令模式(Command)、迭代器模式(Iterator)、调解者模式(Mediator)、备忘录模式(Memento)、观察者模式(Observer)、状态模式(State)、策略模式(Strategy)、访问者模式(Visitor) {% endraw %} 设计模式之间的关联关系和对比 单例模式和工厂模式 实际业务代码中，通常会把工厂类设计为单例。 策略模式和工厂模式 工厂模式包含工厂方法模式和抽象工厂模式是创建型模式，策略模式属于行为型模式。 工厂模式主要目的是封装好创建逻辑，策略模式接收工厂创建好的对象，从而实现不同的行为。 策略模式和委派模式 策略模式是委派模式内部的一种实现形式，策略模式关注的结果是否能相互替代。 委派模式更关注分发和调度的过程。 模板方法模式和工厂方法模式 工厂方法是模板方法的一种特殊实现。 对于工厂方法模式的 create()方法而言，相当于只有一个步骤的模板方法模式。这一个步骤交给子类去实现。而模板方法呢，将 pourVegetables()方法和 pourSauce()方法交给子类实现，pourVegetables()方法和 pourSauce()方法又属于父类的某一个步骤且不可变更。 模板方法模式和策略模式 模板方法和策略模式都有封装算法。 策略模式是使不同算法可以相互替换，且不影响客户端应用层的使用。 模板方法是针对定义一个算法的流程，将一些有细微差异的部分交给子类实现。 模板方法模式不能改变算法流程，策略模式可以改变算法流程且可替换。策略模式通 装饰者模式和静态代理模式 装饰者模式关注点在于给对象动态添加方法，而代理更加注重控制对对象的访问。 代理模式通常会在代理类中创建被代理对象的实例，而装饰者模式通常把被装饰者作为构造参数。 装饰者模式和适配器模式 装饰者模式和适配器模式都是属于包装器模式(Wrapper Pattern)。 装饰者模式可以实现被装饰者与相同的接口或者继承被装饰者作为它的子类，而适配器和被适配者可以实现不同的接口。 适配器模式和静态代理模式 适配器可以结合静态代理来实现，保存被适配对象的引用，但不是唯一的实现方式。 适配器模式和策略模式 在适配业务复杂的情况下，利用策略模式优化动态适配逻辑。 Spring 中常用的设计模式对比 设计模式 一句话归纳 举例 工厂模式(Factory) 只对结果负责，封装创建过程 BeanFactory、Calender 单例模式(Singleton) 保证独一无二 ApplicationContext、Calender 原型模式(Prototype) 拔一根猴毛，吹出千万个 ArrayList、PrototypeBean 代理模式(Proxy) 找人办事，增强职责 ProxyFactoryBean、JdkDynamicAopProxy、CglibAopProxy 委派模式(Delegate) 干活算你的(普通员工)，功劳算我的(项目经理) DispatcherServlet、BeanDefinitionParserDelegate 策略模式(Strategy) 用户选择，结果统一 InstantiationStrategy 模板模式(Template) 流程标准化，自己实现定制 JdbcTemplate、HttpServlet 适配器模式(Adapter) 兼容转换头、充电器 AdvisorAdapter、HandlerAdapter 装饰器模式(Decorator) 包装，同宗同源 BufferedReader、InputStream、OutputStream、HttpHeadResponseDecorator 观察者模式(Observer) 任务完成时通知 ContextLoaderListener Spring 中的编程思想总结 {% raw %} Spring 思想 应用场景(特点) 一句话归纳 OOP Object Oriented Programming（面向对象编程）用程序归纳总结生活中一切事物 封装、继承、多态 BOP Bean Oriented Programming（面向 Bean 编程）面向 Bean（普通的 Java 类）设计程序，解放程序员 一切从 Bean 开始。 AOP Aspect Oriented Programming(面向切面编程)找出多个类中有一定规律的代码，开发时拆开，运行时再合并。面向切面编程，即面向规则编程 解耦，专人做专事 IOC Inversion of Control（控制反转）将 new 对象的动作交给 Spring 管理，并由Spring 保存已创建的对象（IOC 容器） 转交控制权(即控制权反转) DI/DL Dependency Injection(依赖注入)或者Dependency Lookup（依赖查找）依赖注入、依赖查找，Spring不仅保存自己创建的对象，而且保存对象与对象之间的关系。注入即赋值，主要三种方式构造方法、set 方法、直接赋值 赋值 {% endraw %} 学习 AOP 之前必须明白的几个概念： Aspect(切面)：通常是一个类，里面可以定义切入点和通知。 JointPoint(连接点)：程序执行过程中明确的点，一般是方法的调用。 Advice(通知)：AOP 在特定的切入点上执行的增强处理，有 before、after、afterReturning、afterThrowing、around Pointcut(切入点)：就是带有通知的连接点，在程序中主要体现为书写切入点表达式AOP 框架创建的对象，实际就是使用代理对目标对象功能增强。Spring 中的 AOP 代理可以使 JDK 动态代理，也可以是 CGLIB 代理，前者基于接口，后者基于子类。 AOP 在 Spring 中的应用 SpringAOP 是一种编程范式，主要目的是将非功能性需求从功能性需求中分离出来，达到解耦的目的。主要应用场景有：Authentication（权限认证）、Auto Caching（自动缓存处理）、Error Handling（统一错误处理）、Debugging（调试信息输出）、Logging（日志记录）、Transactions（事务处理） ","date":"2019-03-24","objectID":"/%E5%90%84%E7%B1%BB%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94%E4%B8%8Espring%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%80%BB%E7%BB%93/:0:0","tags":["设计模式总结","Spring"],"title":"各类设计模式对比与Spring设计模式的总结","uri":"/%E5%90%84%E7%B1%BB%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E5%AF%B9%E6%AF%94%E4%B8%8Espring%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E7%9A%84%E6%80%BB%E7%BB%93/"},{"categories":["设计模式"],"content":"应用场景 观察者模式有时也叫做发布订阅模式。 主要用于在关联行为之间建立一套触发机制的场景(例如一些提醒业务、MQ等等) java.awt.Event 就是观察者模式的一种，只不过 Java 很少被用来写桌面程序。上面的比如点击事件等等都是通过发布订阅绑定来触发事件。 在spring 中ContextLoaderListener是实现了ServletContextListener接口,这个接口也是继承的EventListener。这也是观察者模式的代表。 java语言中，有一个接口Observer，以及它的实现类Observable，对观察者角色进行了实现。我们通过这个api写一个发布订阅 代码 观察者 public class EyeOfGod implements Observer { private String name; @Override public void update(Observable o, Object arg) { // 被观察者信息(观察多个的话可以通过类型判断来最强转) ObserveAims observeAims = (ObserveAims) o; // 给被观察者发布的信息 Question question = (Question)arg; System.out.println(this.name+\"给\"+observeAims.getName()+\"发布了\"+question.getMsg()); } } 被观察者 class ObserveAims extends Observable { private String name; void publishQuestion(Question question) { System.out.println(this.name+ \"收到了一条\" + question.getMsg() + \"的消息。\"); setChanged(); notifyObservers(question); } } 消息信息类 class Question { private String msg; } 测试 public class Test { public static void main(String[] args) { EyeOfGod eyeOfGod=new EyeOfGod(); eyeOfGod.setName(\"老师\"); ObserveAims observeAims =new ObserveAims(); observeAims.setName(\"小明\"); Question question=new Question(); question.setMsg(\"写作业\"); eyeOfGod.update(observeAims,question); observeAims.publishQuestion(question); } } 输出-------------------- 老师给小明发布了写作业 小明收到了一条写作业的消息。 优点 观察者和被观察者之间建立了一个抽象的耦合。 观察者模式支持广播通信。 缺点 观察者之间有过多的细节依赖、提高时间消耗及程序的复杂度。 使用要得当，要避免循环调用。 总结 观察者模式有很多写法，现在主流比较简单的写法是通过google的guava包的@Subscribe注解 public class GuavaEvent { @Subscribe public void subscribe(String str){ // 可以编写业务逻辑 System.out.println(\"执行subscribe方法，传入的参数是：\" + str); } } public static void main(String[] args) { //消息总线 EventBus eventBus = new EventBus(); GuavaEvent guavaEvent = new GuavaEvent(); eventBus.register(guavaEvent); eventBus.post(\"传入信息\"); } 输出------------------ 执行subscribe方法，传入的参数是：传入信息 类图 ","date":"2019-03-23","objectID":"/%E8%A7%82%E5%AF%9F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["观察模式"],"title":"观察设计模式","uri":"/%E8%A7%82%E5%AF%9F%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 用于扩展一个类的功能或给一个类添加附加职责 动态的给一个对象添加功能，这些功能可以再动态的撤销 在spring mvc中 HttpHeadResponseDecorator类就是使用的装饰者 在mybatis中(org.apache.ibatis.cache.Cache)里面就有许多关于缓存的装饰者类(后续我会在mybatis源码分析里面看)。 在jdk中io相关的类(比如InputStream)都是使用了装饰设计模式。 代码 创建一个方法基类 public abstract class BaseBattercake { /** * 我要买什么 * * @return */ protected abstract String Msg(); /** * 多少钱 * * @return */ protected abstract int price(); } 创建默认信息 public class Battercake extends BaseBattercake { @Override protected String Msg() { return \"煎饼\"; } @Override protected int price() { return 5; } } 创建基础装饰 public abstract class BaseBattercakeDecotator extends BaseBattercake { private BaseBattercake baseBattercake; BaseBattercakeDecotator(BaseBattercake baseBattercake) { this.baseBattercake = baseBattercake; } @Override protected String Msg() { return this.baseBattercake.Msg(); } @Override protected int price() { return this.baseBattercake.price(); } } 加蛋装饰 public class EggDecorator extends BaseBattercakeDecotator { EggDecorator(BaseBattercake baseBattercake) { super(baseBattercake); } @Override protected String Msg() { return super.Msg() + \"加一个蛋\"; } @Override protected int price() { return super.price() + 1; } } 加香肠装饰 public class SausageDecorator extends BaseBattercakeDecotator { SausageDecorator(BaseBattercake baseBattercake) { super(baseBattercake); } @Override protected String Msg() { return super.Msg() + \"加一根香肠\"; } @Override protected int price() { return super.price() + 2; } } 测试 public class Test { public static void main(String[] args) { // 买一个煎饼 BaseBattercake baseBattercake = new Battercake(); // 加鸡蛋 baseBattercake = new EggDecorator(baseBattercake); log.info(baseBattercake.Msg() + baseBattercake.price()); // 加香肠 baseBattercake = new SausageDecorator(baseBattercake); log.info(baseBattercake.Msg() + baseBattercake.price()); } } 输出----------------------- 煎饼加一个蛋6 煎饼加一个蛋加一根香肠8 // 通过得到父类的信息来做成动态的扩展父类功能，灵活增加业务需求 这也就不需要多继承 优点 是继承的有力补充，比继承灵活，不改变原有对象 的情况下动态地给一个对象扩展功能，即插即用。 使用不同装饰类以及这些装饰类的排列组合，可以实 现不同效果。 者完全遵守开闭原则。 缺点 多的代码，更多的类，增加程序复杂性。 装饰时，多层装饰时会更复杂。 类图 ","date":"2019-03-22","objectID":"/%E8%A3%85%E9%A5%B0%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["装饰模式"],"title":"装饰设计模式","uri":"/%E8%A3%85%E9%A5%B0%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 已经存在的类，它的方法和需求不匹配（方法结果相同或相似）的情况 适配器模式不是软件设计阶段考虑的设计模式，是随着软件维护，由于不同产品、不同厂家造成功能类似而接口不相同情况下的解决方案 代码 方法类 public interface Target { void methed(); } 准备适配的类（这里可以是抽象类也可以是普通类） class ReadyAdapter { void methed() { log.info(\"适配的东西！\"); } } 适配类：(如果有抽象类就继承来做适配) public class Adapter implements Target { private ReadyAdapter readyAdapter; Adapter(ReadyAdapter readyAdapter) { this.readyAdapter = readyAdapter; } @Override public void methed() { readyAdapter.methed(); log.info(\"可以在这里做适配操作\"); } } 测试类 public static void main(String[] args) { Target target = new Adapter(new ReadyAdapter()); target.methed(); } 输出------------------------------------ ReadyAdapter - 适配的东西！ Adapter - 可以在这里做适配操作 总结 上面写这个属于对象适配器代码实现：这里写的是单个对象的适配，当要适配多个对象时我们可以把多个对象就继承一个类，然后在适配的时候通过InstanceOf来判断做适配。 将目标类和适配者类解耦，通过引入一个适配器类来重用现有的适配者类，而无须修改原有代码。 类图 ","date":"2019-03-21","objectID":"/%E9%80%82%E9%85%8D%E5%99%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["委派模式"],"title":"适配器设计模式","uri":"/%E9%80%82%E9%85%8D%E5%99%A8%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 委派其实就是静态代理和策略模式一种特殊的组合,因为代理模式注重过程，而委派模式注重结果。 在我们日常开发中的spring mvc中的DispatcherServlet类就是用了委派模式,其原理就是根据用户的url在handlerMapping里面找到对应的处理类,然后委派到具体方法。在 Spring 源码中，只要以 Delegate 结尾的都是实现了委派模式。例如：BeanDefinitionParserDelegate 根据不同类型委派不同的逻辑解析 BeanDefinition。 由于不属于GOF 23种设计模式之一 程序当中一般是精简程序逻辑提升代码可读性 代码 老板叫领导做事情,领导根据需求分发任务案例 创建老板类命令领导的方法： class Boss { /** * 传命令给领导让他分发任务 * * @param command 什么事情 * @param leader 那个领导 */ void command(String command, Leader leader) { leader.doing(command); } } 干活接口:领导跟员工都是打工的 public interface Action { /** * 接收方法执行 * * @param command 命令信息 */ void doing(String command); } 领导类： public class Leader implements Action { private Map\u003cString, Action\u003e targets = new HashMap\u003c\u003e(); Leader() { targets.put(\"加密\", new ProgrammerA()); targets.put(\"登录\", new ProgrammerB()); } @Override public void doing(String command) { targets.get(command).doing(command); } } 员工类： public class ProgrammerA implements Action { @Override public void doing(String command) { log.info(\"我是码农A：我开始工作\" + command); } } public class ProgrammerB implements Action { @Override public void doing(String command) { log.info(\"我是码农B：我开始工作\" + command); } } 测试委派： public class Test { public static void main(String[] args) { // boss Boss boss = new Boss(); // 那个领导 Leader leader = new Leader(); // boss安排任务给这个领导 boss.command(\"登录\", leader); boss.command(\"加密\", leader); } } 输出---------------------------- 我是码农B：我开始工作登录 我是码农A：我开始工作加密 类图 ","date":"2019-03-17","objectID":"/%E5%A7%94%E6%B4%BE%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["委派模式"],"title":"委派设计模式","uri":"/%E5%A7%94%E6%B4%BE%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 定义一个模板结构，将具体内容延迟到子类去实现,在不改变模板结构的前提下在子类中重新定义模板中的内容。 比如JDBC； 优点 提高代码复用性 将相同部分的代码放在抽象的父类中，而将不同的代码放入不同的子类中 实现了反向控制 通过一个父类调用其子类的操作，通过对子类的具体实现扩展不同的行为，实现了反向控制 \u0026 符合“开闭原则” 缺点 引入了抽象类，每一个不同的实现都需要一个子类来实现，导致类的个数增加，从而增加了系统实现的复杂度。 代码 一个做家常菜的案例 创建一个模板： abstract class AbstractComputer { final void cookProcess() { //第一步：倒油 this.pourOil(); //第二步：热油 this.hotOil(); //第三步：倒蔬菜 this.pourVegetables(); //第四步：倒调味料 this.pourSauce(); //第五步：翻炒 this.fry(); } private void pourOil() { System.out.println(\"开始倒油\"); } private void hotOil() { System.out.println(\"开始热油咯\"); } /** * 具体倒什么蔬菜 */ abstract void pourVegetables(); /** * 具体放什么调料 */ abstract void pourSauce(); private void fry() { System.out.println(\"炒熟\"); } } 具体实现 /** * 炒白菜 */ class FriedCabbage extends AbstractComputer { @Override void pourVegetables() { System.out.println(\"把白菜倒下去\"); } @Override void pourSauce() { System.out.println(\"放干辣椒\"); } } /** * 炖鸡汤 */ class StewedChickenSoup extends AbstractComputer { @Override void pourVegetables() { System.out.println(\"放老母鸡下去\"); } @Override void pourSauce() { System.out.println(\"放枸杞\"); } } 测试类 public class Test { public static void main(String[] args) { AbstractComputer friedCabbage=new FriedCabbage(); friedCabbage.cookProcess(); AbstractComputer stewedChickenSoup=new StewedChickenSoup(); stewedChickenSoup.cookProcess(); } } 输出------------------- 开始倒油 开始热油咯 把白菜倒下去 放干辣椒 炒熟 开始倒油 开始热油咯 放老母鸡下去 放枸杞 炒熟 类图 ","date":"2019-02-17","objectID":"/%E6%A8%A1%E6%9D%BF%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["模板模式"],"title":"模板设计模式","uri":"/%E6%A8%A1%E6%9D%BF%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 一个系统需要动态地在几种算法中选择一种的情况 优点 易于扩展, 增加一个新的策略只需要添加一个具体的策略类即可,基本不需要改变原有的代码,符合开闭原则; 避免使用多重条件选择语句(if else),充分体现面向对象设计思想。 缺点 客户端必须知道所有的策略类，并自行决定使用哪一个策略类 代码示例 Strategy接口 定义策略方法 public interface Strategy { /** * 计算方法 * * @param a * @param b * @return */ int calculate(int a, int b); } 具体策略方法 实现策略接口，提供具体算法 public class AddCalculate implements Strategy{ @Override public int calculate(int a, int b) { return a+b; } } public class SubtractCalculate implements Strategy { @Override public int calculate(int a, int b) { return a - b; } } Context类，持有具体策略类的实例，负责调用具体算法 class Context { private Strategy strategy; Context(Strategy strategy) { this.strategy = strategy; } int calculate(int a, int b) { return strategy.calculate(a, b); } } 测试类 public class Test { public static void main(String[] args) { Strategy addCalculate = new AddCalculate(); Context context = new Context(addCalculate); log.info(String.valueOf(context.calculate(3, 2))); Strategy subtractCalculate = new SubtractCalculate(); context = new Context(subtractCalculate); log.info(String.valueOf(context.calculate(3, 2))); } } 输出--------------------------- 5 1 这里的context类可以改成枚举类更方便 enum ContextEnum { // 加法 ADD(new AddCalculate()), // 减法 SUBTRACT(new SubtractCalculate()); private Strategy strategy; ContextEnum(Strategy strategy) { this.strategy = strategy; } Strategy getStrategy() { return this.strategy; } } 测试 public class Test { public static void main(String[] args) { log.info(String.valueOf(ContextEnum.ADD.getStrategy().calculate(3, 2))); log.info(String.valueOf(ContextEnum.SUBTRACT.getStrategy().calculate(3, 2))); } } 输出------------------- 5 1 类图 ","date":"2019-02-12","objectID":"/%E7%AD%96%E7%95%A5%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["策略模式"],"title":"策略设计模式","uri":"/%E7%AD%96%E7%95%A5%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 为其他对象提供一种代理以控制对这个对象的访问。即通过代理对象访问目标对象.这样做的好处是:可以在目标对象实现的基础上,增强额外的功能操作,即扩展目标对象的功能。 关键:代理对象与目标对象.代理对象是对目标对象的扩展,并会调用目标对象;这里使用到编程中的一个思想:不要随意去修改别人已经写好的代码或者方法,如果需改修改,可以通过代理的方式来扩展该方法; 静态代理 说明 静态代理在使用时,需要定义接口或者父类,被代理对象与代理对象一起实现相同的接口或者是继承相同父类. 关键：在编译期确定代理对象，在程序运行前代理类的.class文件就已经存在了。 代码(我让媒婆代理我帮我找个女朋友) 接口类Person public interface Person { /** * 我喜欢什么 */ void findLove(); } 代理类Meipo class Meipo implements Person { private Person person; Meipo(Person person) { this.person = person; } @Override public void findLove() { System.out.println(\"媒婆在帮你寻找\"); this.person.findLove(); System.out.println(\"目前没找到\"); } } 被代理类Me public class Me implements Person { @Override public void findLove() { System.out.println(\"年龄比我小\"); System.out.println(\"有共同爱好\"); System.out.println(\"好相处\"); } } 测试类 public class Test { public static void main(String[] args) { // Meipo是代理对象 Me是被代理对象 Meipo meipo = new Meipo(new Me()); meipo.findLove(); } } 输出 ----------------------------------- 媒婆在帮你寻找 年龄比我小 有共同爱好 好相处 目前没找到 总结 可以做到在不修改目标对象的功能前提下,对目标功能扩展. 缺点:代理类和被代理类实现相同的接口，同时要实现相同的方法。这样就出现了大量的代码重复。如果接口增加一个方法，除了所有实现类需要实现这个方法外，所有代理类也需要实现此方法。增加了代码维护的复杂度。 字节码重组 以下这个过程就叫字节码重组 拿到被代理对象的引用，并且获取到它的所有的接口，反射获取 JDK Proxy类重新生成一个新的类、同时新的类要实现被代理类所有实现的所有的接口 动态生成Java代码，把新加的业务逻辑方法由一定的逻辑代码去调用（在代码中体现） 编译新生成的Java代码.class 再重新加载到JVM中运行 jdk动态代理 说明 在运行时，通过反射机制创建一个实现了一组给定接口的新类 代码(我让我妈妈代理我去寄快递和代我点一份外卖) 接口类Person public interface Person { /** * 点外卖 */ void pointTakeaway(); /** * 送快递 */ void sendDelivery(); } 代理类Mom (这里也就是类似拦截器、AOP的实现过程) public class Mom implements InvocationHandler { private Object object; Object getInstance(Object o) { this.object = o; Class\u003c?\u003e clazz = o.getClass(); // 生成新的对象 return Proxy.newProxyInstance(clazz.getClassLoader(), clazz.getInterfaces(), this); } /** * 类似动态代理拦截器 * * @param proxy * @param method * @param args * @return * @throws Throwable */ @Override public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { System.out.println(\"调用代理类\"); if (\"pointTakeaway\".equals(method.getName())) { System.out.println(\"我儿子叫我帮他点外卖\"); } else if (\"sendDelivery\".equals(method.getName())) { System.out.println(\"我儿子叫我帮他寄快递\"); } method.invoke(this.object, args); return null; } } 被代理类Me public class Me implements Person { @Override public void pointTakeaway() { System.out.println(\"我想要一份炸鸡\"); } @Override public void sendDelivery() { System.out.println(\"帮我寄快递\"); } } 测试类 (这里我通过反编译工具查看代理类具体代码) public class Test { public static void main(String[] args) { // 被代理对象 Me me = new Me(); try { // 动态代理 Person instance = (Person) new Mom().getInstance(me); System.out.println(\"生成的代理类\"+instance.getClass()); instance.pointTakeaway(); instance.sendDelivery(); //JDK中有个规范，只要要是$开头的一般都是自动生成的 //通过反编译工具可以查看源代码 byte [] bytes = ProxyGenerator.generateProxyClass(\"$Proxy0\",new Class[]{Person.class}); FileOutputStream os = new FileOutputStream(\"E://$Proxy0.class\"); os.write(bytes); os.close(); } catch (Exception e) { e.printStackTrace(); } } } 输出 --------------------------- 生成的代理类class com.sun.proxy.$Proxy0 调用代理类 我儿子叫我帮他点外卖 我想要一份炸鸡 调用代理类 我儿子叫我帮他寄快递 帮我寄快递 查看自动生成的代理类 这个刚刚生成的代理类，我把他放进idea查看源代码 可以看出是通过反射生成的并且继承了Proxy和实现了接口 public final class $Proxy0 extends Proxy implements Person { private static Method m1; private static Method m4; private static Method m3; private static Method m2; private static Method m0; public $Proxy0(InvocationHandler var1) throws { super(var1); } public final boolean equals(Object var1) throws { try { return (Boolean)super.h.invoke(this, m1, new Object[]{var1}); } catch (RuntimeException | Error var3) { throw var3; } catch (Throwable var4) { throw new UndeclaredThrowableException(var4); } } public final void sendDelivery() throws { try { super.h.invoke(this, m4, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new UndeclaredThrowableException(var3); } } public final void pointTakeaway() throws { try { super.h.invoke(this, m3, (Object[])null); } catch (RuntimeException | Error var2) { throw var2; } catch (Throwable var3) { throw new","date":"2019-02-01","objectID":"/%E4%BB%A3%E7%90%86%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/:0:0","tags":["代理模式"],"title":"代理设计模式","uri":"/%E4%BB%A3%E7%90%86%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/"},{"categories":["设计模式"],"content":"应用场景 原型模式就是从一个对象再创建另外一个可定制的对象，而且不需要知道任何创建的细节。 所谓原型模式，就是 Java 中的克隆技术，以某个对象为原型。复制出新的对象。显然新的对象具备原 型对象的特点，效率高（避免了重新执行构造过程步骤,而且实例的创建开销比较大或者需要输入较多参数）。 浅克隆 说明 浅复制仅仅复制所克隆的对象，而不复制它所引用的对象。 Object类提供的方法clone只是拷贝本对象，其对象内部的数组、引用对象等都不拷贝，还是指向原生对象的内部元素地址 代码(使用lombok插件省去get与set) student类—clone都需要实现Cloneable这个类 @Getter @Setter public class Student implements Cloneable { private String name; private int id; private Lesson lesson; @Override protected Object clone() throws CloneNotSupportedException { return super.clone(); } } Lesson类 @Setter @Getter public class Lesson { private String name; } 浅克隆测试类 public class ShallowTest { public static void main(String[] args) { Student student = new Student(); Lesson lesson = new Lesson(); student.setLesson(lesson); try { Student cloneStuden = (Student) student.clone(); System.out.println(cloneStuden == student); System.out.println(cloneStuden.getLesson() == student.getLesson()); } catch (CloneNotSupportedException e) { System.out.println(\"克隆失败!\"); e.printStackTrace(); } } } ------------------------------- 输出 false true 从上面可以看出Object类提供的方法clone只是拷贝本对象，其中对象内部的引用对象还是指向原来的地址 深克隆 说明 深复制把要复制的对象所引用的对象都复制了一遍 深复制需要把对象序列化并且所引用的对象也需要序列化 代码 @Getter @Setter public class Student implements Serializable { private String name; private int id; private Lesson lesson; protected Object deepClone() throws IOException, ClassNotFoundException { /* 写入当前对象的二进制流 */ ByteArrayOutputStream byteArrayOutputStream = new ByteArrayOutputStream(); ObjectOutputStream objectOutputStream = new ObjectOutputStream(byteArrayOutputStream); objectOutputStream.writeObject(this); /* 读出二进制流产生的新对象 */ ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(byteArrayOutputStream.toByteArray()); ObjectInputStream objectInputStream = new ObjectInputStream(byteArrayInputStream); return (Student) objectInputStream.readObject(); } } Lesson类 @Setter @Getter public class Lesson implements Serializable { private String name; } 浅克隆测试类 public class deepTest { public static void main(String[] args) { Student student = new Student(); Lesson lesson = new Lesson(); student.setLesson(lesson); try { Student cloneStuden = (Student) student.clone(); System.out.println(cloneStuden == student); System.out.println(cloneStuden.getLesson() == student.getLesson()); } catch (CloneNotSupportedException e) { System.out.println(\"克隆失败!\"); e.printStackTrace(); } } } ------------------------------- 输出 false false ","date":"2019-01-29","objectID":"/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/:0:0","tags":["原型模式"],"title":"原型设计模式","uri":"/%E5%8E%9F%E5%9E%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["云原生"],"content":"1.准备基础环境 我们将使用kubeadm部署3个节点的 Kubernetes Cluster 节点详细信息： 节点主机名 节点IP 节点角色 操作系统 节点配置 k8s-master 192.168.217.131 master CentOS7.6 2C4G k8s-node1 192.168.217.132 node CentOS7.6 2C4G k8s-node2 192.168.217.133 node CentOS7.6 2C4G 节点组件分布： Master 和 Node 节点由于分工不一样，所以安装的服务不同，最终安装完毕，Master 和 Node 启动的核心服务分别如下： Master节点 Node节点 kube-apiserver kube-flannel kube-scheduler other apps kube-proxy — etcd — coredns — kube-flannel — 无特殊说明以下操作在所有节点执行： 修改主机名(master与node都要执行)： #master节点: hostnamectl set-hostname k8s-master #node1节点： hostnamectl set-hostname k8s-node1 #node2节点: hostnamectl set-hostname k8s-node2 基本配置(master与node都要执行)： #修改/etc/hosts文件 cat \u003e\u003e /etc/hosts \u003c\u003c EOF 192.168.217.131 k8s-master 192.168.217.132 k8s-node1 192.168.217.133 k8s-node2 EOF #关闭防火墙和selinux systemctl stop firewalld \u0026\u0026 systemctl disable firewalld sed -i 's/^SELINUX=enforcing$/SELINUX=disabled/' /etc/selinux/config \u0026\u0026 setenforce 0 #关闭swap swapoff -a yes | cp /etc/fstab /etc/fstab_bak cat /etc/fstab_bak |grep -v swap \u003e /etc/fstab 配置时间同步(master与node都要执行): 使用chrony同步时间，配置master节点与网络NTP服务器同步时间，所有node节点与master节点同步时间。 配置master节点： #安装chrony： yum install -y chrony #注释默认ntp服务器 sed -i 's/^server/#\u0026/' /etc/chrony.conf #指定上游公共 ntp 服务器，并允许其他节点同步时间 cat \u003e\u003e /etc/chrony.conf \u003c\u003c EOF server 0.asia.pool.ntp.org iburst server 1.asia.pool.ntp.org iburst server 2.asia.pool.ntp.org iburst server 3.asia.pool.ntp.org iburst allow all EOF #重启chronyd服务并设为开机启动： systemctl enable chronyd \u0026\u0026 systemctl restart chronyd #开启网络时间同步功能 timedatectl set-ntp true 配置所有node节点：(注意修改master IP地址) #安装chrony： yum install -y chrony #注释默认服务器 sed -i 's/^server/#\u0026/' /etc/chrony.conf #指定内网 master节点为上游NTP服务器 echo server 192.168.217.131 iburst \u003e\u003e /etc/chrony.conf #重启服务并设为开机启动： systemctl enable chronyd \u0026\u0026 systemctl restart chronyd 所有节点执行chronyc sources命令，查看存在以^*开头的行，说明已经与服务器时间同步 设置网桥包经过iptalbes(master与node都要执行) RHEL / CentOS 7上的一些用户报告了由于iptables被绕过而导致流量路由不正确的问题。创建/etc/sysctl.d/k8s.conf文件，添加如下内容： cat \u003c\u003cEOF \u003e /etc/sysctl.d/k8s.conf vm.swappiness = 0 net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward = 1 EOF # 使配置生效 modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf kube-proxy开启ipvs的前提条件(master与node都要执行) 由于ipvs已经加入到了内核的主干，所以为kube-proxy开启ipvs的前提需要加载以下的内核模块： 在所有的Kubernetes节点执行以下脚本: cat \u003e /etc/sysconfig/modules/ipvs.modules \u003c\u003cEOF #!/bin/bash modprobe -- ip_vs modprobe -- ip_vs_rr modprobe -- ip_vs_wrr modprobe -- ip_vs_sh modprobe -- nf_conntrack_ipv4 EOF #执行脚本 chmod 755 /etc/sysconfig/modules/ipvs.modules \u0026\u0026 bash /etc/sysconfig/modules/ipvs.modules \u0026\u0026 lsmod | grep -e ip_vs -e nf_conntrack_ipv4 上面脚本创建了/etc/sysconfig/modules/ipvs.modules文件，保证在节点重启后能自动加载所需模块。 使用lsmod | grep -e ip_vs -e nf_conntrack_ipv4命令查看是否已经正确加载所需的内核模块。 接下来还需要确保各个节点上已经安装了ipset软件包。 为了便于查看ipvs的代理规则，最好安装一下管理工具ipvsadm。 # yum install ipset ipvsadm -y 安装Docker（这个可以提前自己安装–也可以选用我之前的方案离线安装+阿里云镜像加速） Kubernetes默认的容器运行时仍然是Docker，使用的是kubelet中内置dockershim CRI实现。需要注意的是，Kubernetes 1.13最低支持的Docker版本是1.11.1，最高支持是18.06，而Docker最新版本已经是18.09了，故我们安装时需要指定版本为18.06.1-ce。 #配置docker yum源 yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo #安装指定版本，这里安装18.06 yum list docker-ce --showduplicates | sort -r yum install -y docker-ce-18.06.1.ce-3.el7 systemctl start docker \u0026\u0026 systemctl enable docker 脚本安装docker-ce并配置daocloud镜像加速(可选)： bash Install_docker-ce.sh 安装kubeadm、kubelet、kubectl(master与node都要执行) 官方安装文档可以参考： https://kubernetes.io/docs/setup/independent/install-kubeadm/ kubelet 在群集中所有节点上运行的核心组件, 用来执行如启动pods和containers等操作。 kubeadm 引导启动k8s集群的命令行工具，用于初始化 Cluster。 kubectl 是 Kubernetes 命令行工具。通过 kubectl 可以部署和管理应用，查看各种资源，创建、删除和更新各种组件。 #配置kubernetes.repo的源，由于官方源国内无法访问，这里使用阿里云yum源 cat \u003c\u003cEOF \u003e /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/ enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF #在所有节点上安装指定版本 ","date":"2019-01-14","objectID":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/:1:0","tags":["kubernetes"],"title":"完整安装kubernetes集群环境(不需要科学上网)","uri":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"},{"categories":["云原生"],"content":"2.部署master节点 完整的官方文档可以参考： https://kubernetes.io/docs/setup/independent/create-cluster-kubeadm/ https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/ Master节点执行初始化： 注意这里执行初始化用到了- -image-repository选项，指定初始化需要的镜像源从阿里云镜像仓库拉取。 kubeadm init \\ --apiserver-advertise-address=192.168.217.131 \\ --image-repository registry.aliyuncs.com/google_containers \\ --kubernetes-version v1.13.1 \\ --pod-network-cidr=10.244.0.0/16 初始化命令说明： --apiserver-advertise-address 指明用 Master 的哪个 interface 与 Cluster 的其他节点通信。如果 Master 有多个 interface，建议明确指定，如果不指定，kubeadm 会自动选择有默认网关的 interface。 --pod-network-cidr 指定 Pod 网络的范围。Kubernetes 支持多种网络方案，而且不同网络方案对 –pod-network-cidr 有自己的要求，这里设置为 10.244.0.0/16 是因为我们将使用 flannel 网络方案，必须设置成这个 CIDR。 --image-repository Kubenetes默认Registries地址是 k8s.gcr.io，在国内并不能访问 gcr.io，在1.13版本中我们可以增加–image-repository参数，默认值是 k8s.gcr.io，将其指定为阿里云镜像地址：registry.aliyuncs.com/google_containers。 --kubernetes-version=v1.13.1 关闭版本探测，因为它的默认值是stable-1，会导致下载最新版本，这里固定版本1.13.1的 初始化过程如下： [root@k8s-master ~]# kubeadm init \\ \u003e --image-repository registry.aliyuncs.com/google_containers \\ \u003e --kubernetes-version v1.13.1 \\ \u003e --pod-network-cidr=10.244.0.0/16 [init] Using Kubernetes version: v1.13.1 [preflight] Running pre-flight checks [preflight] Pulling images required for setting up a Kubernetes cluster [preflight] This might take a minute or two, depending on the speed of your internet connection [preflight] You can also perform this action in beforehand using 'kubeadm config images pull' [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Activating the kubelet service [certs] Using certificateDir folder \"/etc/kubernetes/pki\" [certs] Generating \"etcd/ca\" certificate and key [certs] Generating \"etcd/healthcheck-client\" certificate and key [certs] Generating \"apiserver-etcd-client\" certificate and key [certs] Generating \"etcd/server\" certificate and key [certs] etcd/server serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.217.131 127.0.0.1 ::1] [certs] Generating \"etcd/peer\" certificate and key [certs] etcd/peer serving cert is signed for DNS names [k8s-master localhost] and IPs [192.168.217.131 127.0.0.1 ::1] [certs] Generating \"ca\" certificate and key [certs] Generating \"apiserver\" certificate and key [certs] apiserver serving cert is signed for DNS names [k8s-master kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.217.131] [certs] Generating \"apiserver-kubelet-client\" certificate and key [certs] Generating \"front-proxy-ca\" certificate and key [certs] Generating \"front-proxy-client\" certificate and key [certs] Generating \"sa\" key and public key [kubeconfig] Using kubeconfig folder \"/etc/kubernetes\" [kubeconfig] Writing \"admin.conf\" kubeconfig file [kubeconfig] Writing \"kubelet.conf\" kubeconfig file [kubeconfig] Writing \"controller-manager.conf\" kubeconfig file [kubeconfig] Writing \"scheduler.conf\" kubeconfig file [control-plane] Using manifest folder \"/etc/kubernetes/manifests\" [control-plane] Creating static Pod manifest for \"kube-apiserver\" [control-plane] Creating static Pod manifest for \"kube-controller-manager\" [control-plane] Creating static Pod manifest for \"kube-scheduler\" [etcd] Creating static Pod manifest for local etcd in \"/etc/kubernetes/manifests\" [wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory \"/etc/kubernetes/manifests\". This can take up to 4m0s [apiclient] All control plane components are healthy after 21.009858 seconds [uploadconfig] storing the configuration used in ConfigMap \"kubeadm-config\" in the \"kube-system\" Namespace [kubelet] Creating a ConfigMap \"kubelet-config-1.13\" in namespace kube-system with the configuration for the kubelets in the cluster [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to t","date":"2019-01-14","objectID":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/:2:0","tags":["kubernetes"],"title":"完整安装kubernetes集群环境(不需要科学上网)","uri":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"},{"categories":["云原生"],"content":"3.部署worker(node)节点 Kubernetes 的 Worker 节点跟 Master 节点几乎是相同的，它们运行着的都是一个 kubelet 组件。唯一的区别在于，在 kubeadm init 的过程中，kubelet 启动后，Master 节点上还会自动运行 kube-apiserver、kube-scheduler、kube-controller-manger 这三个系统 Pod。 在 k8s-node1 和 k8s-node2 上分别执行如下命令，将其注册到 Cluster 中： #执行以下命令将节点接入集群 kubeadm join 192.168.217.131:6443 --token 67kq55.8hxoga556caxty7s --discovery-token-ca-cert-hash sha256:7d50e704bbfe69661e37c5f3ad13b1b88032b6b2b703ebd4899e259477b5be69 #如果执行kubeadm init时没有记录下加入集群的命令，可以通过以下命令重新创建 kubeadm token create --print-join-command 在k8s-node1上执行kubeadm join ： [root@k8s-node1 ~]# kubeadm join 192.168.217.131:6443 --token 67kq55.8hxoga556caxty7s --discovery-token-ca-cert-hash sha256:7d50e704bbfe69661e37c5f3ad13b1b88032b6b2b703ebd4899e259477b5be69 [preflight] Running pre-flight checks [discovery] Trying to connect to API Server \"192.168.217.131:6443\" [discovery] Created cluster-info discovery client, requesting info from \"https://192.168.217.131:6443\" [discovery] Requesting info from \"https://192.168.217.131:6443\" again to validate TLS against the pinned public key [discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server \"192.168.217.131:6443\" [discovery] Successfully established connection with API Server \"192.168.217.131:6443\" [join] Reading configuration from the cluster... [join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml' [kubelet] Downloading configuration for the kubelet from the \"kubelet-config-1.13\" ConfigMap in the kube-system namespace [kubelet-start] Writing kubelet configuration to file \"/var/lib/kubelet/config.yaml\" [kubelet-start] Writing kubelet environment file with flags to file \"/var/lib/kubelet/kubeadm-flags.env\" [kubelet-start] Activating the kubelet service [tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap... [patchnode] Uploading the CRI Socket information \"/var/run/dockershim.sock\" to the Node API object \"k8s-node1\" as an annotation This node has joined the cluster: * Certificate signing request was sent to apiserver and a response was received. * The Kubelet was informed of the new secure connection details. Run 'kubectl get nodes' on the master to see this node join the cluster. [root@k8s-node1 ~]# 重复执行以上操作将k8s-node2也加进去（注意重新执行kubeadm token create –print-join-command）。 然后根据提示，我们可以通过 kubectl get nodes 查看节点的状态： [centos@k8s-master ~]$ kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready master 11h v1.13.1 k8s-node1 Ready \u003cnone\u003e 24m v1.13.1 k8s-node2 Ready \u003cnone\u003e 4m9s v1.13.1 [centos@k8s-master ~]$ nodes状态全部为ready，由于每个节点都需要启动若干组件，如果node节点的状态是 NotReady，可以查看所有节点pod状态，确保所有pod成功拉取到镜像并处于running状态： [centos@k8s-master ~]$ kubectl get pod --all-namespaces -o wide NAMESPACE NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES kube-system coredns-78d4cf999f-7jdx7 1/1 Running 0 11h 10.244.0.3 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system coredns-78d4cf999f-s6mhk 1/1 Running 0 11h 10.244.0.2 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system etcd-k8s-master 1/1 Running 1 12h 192.168.217.131 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system kube-apiserver-k8s-master 1/1 Running 1 12h 192.168.217.131 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system kube-controller-manager-k8s-master 1/1 Running 1 12h 192.168.217.131 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system kube-flannel-ds-amd64-d2r8p 1/1 Running 0 6m43s 192.168.58.102 k8s-node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-flannel-ds-amd64-d85c6 1/1 Running 0 27m 192.168.58.101 k8s-node1 \u003cnone\u003e \u003cnone\u003e kube-system kube-flannel-ds-amd64-lkf2f 1/1 Running 0 11h 192.168.217.131 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-k8jx8 1/1 Running 0 6m43s 192.168.58.102 k8s-node2 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-n95ck 1/1 Running 0 27m 192.168.58.101 k8s-node1 \u003cnone\u003e \u003cnone\u003e kube-system kube-proxy-przwf 1/1 Running 1 12h 192.168.217.131 k8s-master \u003cnone\u003e \u003cnone\u003e kube-system kube-scheduler-k8s-master 1/1 Running 1 12h 192.168.217.131 k8s-master \u003cnone\u003e \u003cnone\u003e [centos@k8s-master ~]$ 这时，所有的节点都","date":"2019-01-14","objectID":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/:3:0","tags":["kubernetes"],"title":"完整安装kubernetes集群环境(不需要科学上网)","uri":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"},{"categories":["云原生"],"content":"测试集群各个组件 首先验证kube-apiserver, kube-controller-manager, kube-scheduler, pod network 是否正常： 部署一个 Nginx Deployment，包含2个Pod 参考：https://kubernetes.io/docs/concepts/workloads/controllers/deployment/ [centos@k8s-master ~]$ kubectl create deployment nginx --image=nginx:alpine deployment.apps/nginx created [centos@k8s-master ~]$ kubectl scale deployment nginx --replicas=2 deployment.extensions/nginx scaled [centos@k8s-master ~]$ 验证Nginx Pod是否正确运行，并且会分配10.244.开头的集群IP [centos@k8s-master ~]$ kubectl get pods -l app=nginx -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-54458cd494-p2qgx 1/1 Running 0 111s 10.244.1.2 k8s-node1 \u003cnone\u003e \u003cnone\u003e nginx-54458cd494-sdlm7 1/1 Running 0 103s 10.244.2.2 k8s-node2 \u003cnone\u003e \u003cnone\u003e [centos@k8s-master ~]$ 再验证一下kube-proxy是否正常： 以 NodePort 方式对外提供服务 参考：https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/ [centos@k8s-master ~]$ kubectl expose deployment nginx --port=80 --type=NodePort service/nginx exposed [centos@k8s-master ~]$ kubectl get services nginx NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE nginx NodePort 10.108.17.2 \u003cnone\u003e 80:30670/TCP 12s [centos@k8s-master ~]$ 可以通过任意 NodeIP:Port 在集群外部访问这个服务： [centos@k8s-master ~]$ curl 192.168.217.131:30670 [centos@k8s-master ~]$ curl 192.168.58.102:30670 [centos@k8s-master ~]$ curl 192.168.58.101:30670 访问k8s-master ip 访问k8s-node1 ip 访问k8s-node2 ip 最后验证一下dns, pod network是否正常： 运行Busybox并进入交互模式 [centos@k8s-master ~]$ kubectl run -it curl --image=radial/busyboxplus:curl kubectl run --generator=deployment/apps.v1 is DEPRECATED and will be removed in a future version. Use kubectl run --generator=run-pod/v1 or kubectl create instead. If you don't see a command prompt, try pressing enter. [ root@curl-66959f6557-s5qqs:/ ]$ 输入nslookup nginx查看是否可以正确解析出集群内的IP，以验证DNS是否正常 [ root@curl-66959f6557-s5qqs:/ ]$ nslookup nginx Server: 10.96.0.10 Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.local Name: nginx Address 1: 10.108.17.2 nginx.default.svc.cluster.local 通过服务名进行访问，验证kube-proxy是否正常 [ root@curl-66959f6557-q472z:/ ]$ curl http://nginx/ \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e ...... \u003c/body\u003e \u003c/html\u003e [ root@curl-66959f6557-q472z:/ ]$ 分别访问一下2个Pod的内网IP，验证跨Node的网络通信是否正常 [ root@curl-66959f6557-s5qqs:/ ]$ curl 10.244.1.2 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e ...... \u003c/body\u003e \u003c/html\u003e [ root@curl-66959f6557-s5qqs:/ ]$ curl 10.244.2.2 \u003c!DOCTYPE html\u003e \u003chtml\u003e \u003chead\u003e \u003ctitle\u003eWelcome to nginx!\u003c/title\u003e ...... \u003c/body\u003e \u003c/html\u003e [ root@curl-66959f6557-s5qqs:/ ]$ ","date":"2019-01-14","objectID":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/:4:0","tags":["kubernetes"],"title":"完整安装kubernetes集群环境(不需要科学上网)","uri":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"},{"categories":["云原生"],"content":"Pod调度到Master节点（可以不做） 出于安全考虑，默认配置下Kubernetes不会将Pod调度到Master节点。查看Taints字段默认配置： [centos@k8s-master ~]$ kubectl describe node k8s-master ...... Taints: node-role.kubernetes.io/master:NoSchedule 如果希望将k8s-master也当作Node节点使用，可以执行如下命令,其中k8s-master是主机节点hostname： kubectl taint node k8s-master node-role.kubernetes.io/master- 修改后Taints字段状态： [centos@k8s-master ~]$ kubectl describe node k8s-master ...... Taints: \u003cnone\u003e 如果要恢复Master Only状态，执行如下命令： kubectl taint node k8s-master node-role.kubernetes.io/master=\"\" ","date":"2019-01-14","objectID":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/:5:0","tags":["kubernetes"],"title":"完整安装kubernetes集群环境(不需要科学上网)","uri":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"},{"categories":["云原生"],"content":"kube-proxy开启ipvs 修改ConfigMap的kube-system/kube-proxy中的config.conf，mode: “ipvs”： [centos@k8s-master ~]$ kubectl edit cm kube-proxy -n kube-system configmap/kube-proxy edited 之后重启各个节点上的kube-proxy pod： [centos@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy | awk '{system(\"kubectl delete pod \"$1\" -n kube-system\")}' pod \"kube-proxy-2w9sh\" deleted pod \"kube-proxy-gw4lx\" deleted pod \"kube-proxy-thv4c\" deleted [centos@k8s-master ~]$ kubectl get pod -n kube-system | grep kube-proxy kube-proxy-6qlgv 1/1 Running 0 65s kube-proxy-fdtjd 1/1 Running 0 47s kube-proxy-m8zkx 1/1 Running 0 52s [centos@k8s-master ~]$ 查看日志： [centos@k8s-master ~]$ kubectl logs kube-proxy-6qlgv -n kube-system I1213 09:50:15.414493 1 server_others.go:189] Using ipvs Proxier. W1213 09:50:15.414908 1 proxier.go:365] IPVS scheduler not specified, use rr by default I1213 09:50:15.415021 1 server_others.go:216] Tearing down inactive rules. I1213 09:50:15.461658 1 server.go:464] Version: v1.13.0 I1213 09:50:15.467827 1 conntrack.go:52] Setting nf_conntrack_max to 131072 I1213 09:50:15.467997 1 config.go:202] Starting service config controller I1213 09:50:15.468010 1 controller_utils.go:1027] Waiting for caches to sync for service config controller I1213 09:50:15.468092 1 config.go:102] Starting endpoints config controller I1213 09:50:15.468100 1 controller_utils.go:1027] Waiting for caches to sync for endpoints config controller I1213 09:50:15.568766 1 controller_utils.go:1034] Caches are synced for endpoints config controller I1213 09:50:15.568950 1 controller_utils.go:1034] Caches are synced for service config controller [centos@k8s-master ~]$ 日志中打印出了Using ipvs Proxier，说明ipvs模式已经开启。 由于本人初学kubernetes:以上是结合大白老师的文档实测安装成功,过程有曲折,但是肯定能安装成功;有问题可以问我。 ","date":"2019-01-14","objectID":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/:6:0","tags":["kubernetes"],"title":"完整安装kubernetes集群环境(不需要科学上网)","uri":"/%E5%AE%8C%E6%95%B4%E5%AE%89%E8%A3%85kubernetes%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E4%B8%8D%E9%9C%80%E8%A6%81%E7%A7%91%E5%AD%A6%E4%B8%8A%E7%BD%91/"},{"categories":["设计模式"],"content":"单一职责原则（Single Responsibility Principle，简称SRP ） 核心思想：应该有且仅有一个原因引起类的变更 问题描述：假如有类Class1完成职责T1，T2，当职责T1或T2有变更需要修改时，有可能影响到该类的另外一个职责正常工作。 好处：类的复杂度降低、可读性提高、可维护性提高、扩展性提高、降低了变更引起的风险 需注意：单一职责原则提出了一个编写程序的标准，用“职责”或“变化原因”来衡量接口或类设计得是否优良，但是“职责”和“变化原因”都是不可以度量的，因项目和环境而异。 里氏替换原则（Liskov Substitution Principle,简称LSP） 核心思想：在使用基类的的地方可以任意使用其子类，能保证子类完美替换基类。 通俗来讲：只要父类能出现的地方子类就能出现。反之，父类则未必能胜任。 好处：增强程序的健壮性，即使增加了子类，原有的子类还可以继续运行。 需注意：如果子类不能完整地实现父类的方法，或者父类的某些方法在子类中已经发生“畸变”，则建议断开父子继承关系 采用依赖、聚合、组合等关系代替继承。 依赖倒置原则（Dependence Inversion Principle,简称DIP） 核心思想：高层模块不应该依赖底层模块，二者都该依赖其抽象；抽象不应该依赖细节；细节应该依赖抽象； 说明：高层模块就是调用端，低层模块就是具体实现类。抽象就是指接口或抽象类。细节就是实现类。 通俗来讲：依赖倒置原则的本质就是通过抽象（接口或抽象类）使个各类或模块的实现彼此独立，互不影响，实现模块间的松耦合。 问题描述：类A直接依赖类B，假如要将类A改为依赖类C，则必须通过修改类A的代码来达成。这种场景下，类A一般是高层模块，负责复杂的业务逻辑；类B和类C是低层模块，负责基本的原子操作；假如修改类A，会给程序带来不必要的风险。 解决方案：将类A修改为依赖接口interface，类B和类C各自实现接口interface，类A通过接口interface间接与类B或者类C发生联系，则会大大降低修改类A的几率。 好处：依赖倒置的好处在小型项目中很难体现出来。但在大中型项目中可以减少需求变化引起的工作量。使并行开发更友好。 接口隔离原则（Interface Segregation Principle,简称ISP） 核心思想：类间的依赖关系应该建立在最小的接口上 通俗来讲：建立单一接口，不要建立庞大臃肿的接口，尽量细化接口，接口中的方法尽量少。也就是说，我们要为各个类建立专用的接口，而不要试图去建立一个很庞大的接口供所有依赖它的类去调用。 问题描述：类A通过接口interface依赖类B，类C通过接口interface依赖类D，如果接口interface对于类A和类B来说不是最小接口，则类B和类D必须去实现他们不需要的方法。 需注意接口尽量小，但是要有限度:对接口进行细化可以提高程序设计灵活性，但是如果过小，则会造成接口数量过多，使设计复杂化。所以一定要适度 提高内聚，减少对外交互:使接口用最少的方法去完成最多的事情 为依赖接口的类定制服务：只暴露给调用的类它需要的方法，它不需要的方法则隐藏起来。只有专注地为一个模块提供定制服务，才能建立最小的依赖关系。 迪米特法则（Law of Demeter,简称LoD） 核心思想：类间解耦。 通俗来讲： 一个类对自己依赖的类知道的越少越好。自从我们接触编程开始，就知道了软件编程的总的原则：低耦合，高内聚。无论是面向过程编程还是面向对象编程，只有使各个模块之间的耦合尽量的低，才能提高代码的复用率。低耦合的优点不言而喻，但是怎么样编程才能做到低耦合呢？那正是迪米特法则要去完成的。 开放封闭原则（Open Close Principle,简称OCP） 核心思想：尽量通过扩展软件实体来解决需求变化，而不是通过修改已有的代码来完成变化 通俗来讲： 一个软件产品在生命周期内，都会发生变化，既然变化是一个既定的事实，我们就应该在设计的时候尽量适应这些变化，以提高项目的稳定性和灵活性。 合成复用原则（Composite/Aggregate Reuse Principle,CARP） 核心思想：是指尽量使用对象组合(has-a)/聚合(contanis-a)，而不是继承关系达到软件复用的目的。可以使系统更加灵活，降低类与类之间的耦合度，一个类的变化对其他类造成的影响相对较少 通俗来讲：继承我们叫做白箱复用，相当于把所有的实现细节暴露给子类。组合/聚合也称之为黑箱复用，对类以外的对象是无法获取到实现细节的(其实我们的注入也就是符合了合成复用原则)。 总结 单一职责原则告诉我们实现类要职责单一 里氏替换原则告诉我们不要破坏继承体系 依赖倒置原则告诉我们要面向接口编程 接口隔离原则告诉我们在设计接口的时候要精简单一 迪米特法则告诉我们要降低耦合 合成复用告诉我们根据具体业务增加组合对象 而开闭原则是总纲，他告诉我们要对扩展开放，对修改关闭 设计模式的使用 设计模式从来都是组合来使用的，做到你中有我，我中有你，一个好的程序不是一个设计模式能搞定的。理解设计模式的思想才是最重要的 ","date":"2019-01-10","objectID":"/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99/:0:0","tags":["七大原则"],"title":"七大设计原则","uri":"/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F%E4%B8%83%E5%A4%A7%E5%8E%9F%E5%88%99/"},{"categories":["设计模式"],"content":" 因为抽象工厂模式在spring中用的比较多，例如spring的心脏BeanFactory就是利用抽象工厂模式来管理bean的。最主要的就是把具体功能抽象出来 上代码 先建立产品的接口方法类 /** * 定义一个牛奶的接口 */ public interface Milk { String getName(); } 产品实现功能 /** * 伊利产品 */ public class Erie implements Milk { @Override public String getName() { return \"伊利\"; } } /** * 蒙牛产品 */ public class Mengniu implements Milk { @Override public String getName() { return \"蒙牛\"; } } 创建抽象工厂类 /** * 抽象工厂生产那些产品 */ public abstract class AbstractFactory { abstract Milk getMengniu(); abstract Milk getErie(); } 创建具体产品类 /** * 具体怎么生产 */ public class MilkFactory extends AbstractFactory { @Override Milk getMengniu() { return new Mengniu(); } @Override Milk getErie() { return new Erie(); } } 对外的调用类 MilkFactory milkFactory = new MilkFactory(); // 当增加产品时，调用方根本不需要改东西，只管有没有具体方法 milkFactory.getSanlu().getName(); 优点 降低耦合 抽象工厂模式将具体产品的创建延迟到具体工厂的子类中，这样将对象的创建封装起来，可以减少客户端与具体产品类之间的依赖，从而使系统耦合度低，这样更有利于后期的维护和扩展； 更符合开-闭原则新增一种产品类时，只需要增加相应的具体产品类和相应的工厂子类即可；而简单工厂模式需要修改工厂类的判断逻辑 符合单一职责原则每个具体工厂类只负责创建对应的产品 缺点 抽象工厂模式很难支持新种类产品的变化。 这是因为抽象工厂接口中已经确定了可以被创建的产品集合，如果需要添加新产品，此时就必须去修改抽象工厂的接口，这样就涉及到抽象工厂类的以及所有子类的改变，这样也就违背了“开发——封闭”原则 类图 ","date":"2019-01-10","objectID":"/%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/:0:0","tags":["抽象工厂模式"],"title":"抽象工厂模式","uri":"/%E6%8A%BD%E8%B1%A1%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F/"},{"categories":["云原生"],"content":"docker的配置安装 基本命令 docker info docker基本信息 docker --help 帮助信息 比如docker search --help 这样就可以查看search语法 docker version docker版本号 docker images 显示镜像 docker search 镜像名 搜索docker镜像 docker pull 镜像名 拉取镜像到本地 docker rmi 镜像id 删除镜像 容器命令 启动容器命令 docker run -d 后台运行 -p 指定端口映射 宿主机端口:容器端口 -restart=always 是docker官方的给的自启动方案 --name 为容器指定一个名称 -it 通过伪终端交互方式运行 -P 随机端口映射 -v 启用数据卷 让外部文件在容器内运行 -e 把文件映射到宿主机 镜像操作 docker ps -a 显示所有镜像包括停止了的 docker start (容器id或者名称) 启动镜像 docker stop 停止镜像 docker restart 重启镜像 docker rm 删除镜像 删除多个可以用空格隔开 docker kill 强制停止镜像 docker logs 查看日志 docker logs -f --tail 容器id 可以动态查看 docker top 查看容器内进程 docker inspect 查看容器详细信息 docker exec -it 容器id bash 进入容器 docker cp 容器id:容器内路径:宿主机路径 拷贝文档 创建镜像 docker commit -m='提交信息' -a='作者' 容器id 制作后的名称：标签 Dockerfile 通过dockerfile来构建镜像是最舒服最方便的 FROM java:8 指令指明了当前镜像的基镜像，编译当前镜像时自动下载基镜像 MAINTAINER bingo 作者 ADD demo-0.0.1-SNAPSHOT.jar demo.jar 复制jar文件到镜像中去并重命名为demo.jar RUN yum -y install vim 构建时需要运行的命令 WORKDIR 容器创建后默认在那个目录 EXPOSE 8080 暴露8080端口 ENV JAVAHOME /usr/jvm/jdk1.8.0_181 设置环境变量 COPY 复制宿主机文件 VOLUME 数据卷 CMD 只执行最后一条 并且这个会被run 后面的命令替换 ENTRYPOINT [\"java\",\"-jar\",\"demo.jar\"] 启动时执行java -jar demo.jar 这个命令会追加到run命令后面 --------------------------------------------- 然后运行 docker build 名称：标签 . 构建镜像 容器间访问 docker 容器访问是通过宿主机的NAT来转发的所以，docker容器内的ip是宿主机分配的B类私有地址 容器间访问是通过172的私有地址访问，访问外网是通过宿主机转发访问。 ","date":"2019-01-02","objectID":"/%E5%AD%A6%E4%B9%A0docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/:0:0","tags":["docker"],"title":"学习docker常用命令","uri":"/%E5%AD%A6%E4%B9%A0docker%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"},{"categories":["云原生"],"content":"git与svn的区别 首先核心的点就是git是分布式的，而svn不是分布式的 git中每一个开发人员的电脑上都有一个Local Repository,所以即使没有网络也一样可以Commit，查看历史版本记录，创建项 目分支等操作，等网络再次连接上Push到Server端；这也就是为什么git适合分布式的原因。 svn相当于知识在本地拷贝了一份服务器上的代码，本地需要看日志记录是需要联网。 svn比较简单只是需要一个放代码的地方拿下来用就ok。这种属于集中式的代码管理。 .git目录 HEAD: 当前指向的分支 config：配置目录 objects：git存储的文件对象，经过压缩编码 index：git的索引文件通过这个找到objects的文件 创建git项目 不懂命令可以 git --help 你想查询的命令 比如 git --help push 就可以跳转浏览器帮助页面 生成 ssh key ssh-keygen -t rsa -C '邮箱' 提交的用户名与邮箱标识 // 这个是全局的 //windows下可以找到用户目录下.gitconfig这个文件更改 git config –-global user.name ‘xx’ git config –-global user.email ‘xx’ 开始项目初始化 git init //在项目文件下（会生成一个.git的目录） git remote add origin git地址 // 这样会绑定远端地址 git pull origin master // 把远端的代码拉下来(这里的分支具体看公司的git flow) git add . // 跟踪所有改动文件（可以指明具体文件） git commit -m '提交说明' // 提交跟踪文件到本地工作目录 git push origin master //推动代码到远程分支 我工作中常用命令 解决冲突（个人见解） 首先把握一点：就是把有冲突的代码pull下来与本地修改了然后再次commit 我在开发中是拿我的分支与有冲突的分支 rebase head一下 解决完代码后保存，再次提交commit git rabase -i HEAD~~ 删除我本地之前的版本（这里有多少次本地提交就有多少个~） 然后 git push -f 到远程分支上面去 再在远程仓库进行合并 切换分支 git checkout 分支名 git checkout . // 可以撤销当前分支的更改 ","date":"2018-12-17","objectID":"/%E5%88%9D%E5%85%A5%E5%AD%A6%E4%B9%A0git/:0:0","tags":["git"],"title":"初入学习git","uri":"/%E5%88%9D%E5%85%A5%E5%AD%A6%E4%B9%A0git/"},{"categories":["MySQL性能调优"],"content":" 军规适用场景：并发量大、数据量大的互联网业务 只是解读：没必要完全效仿 基础规范 必须使用InnoDB存储引擎 解读：支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 必须使用UTF8字符集不过现在基本上用 UTF8MB4 **解读：万国码，无需转码，无乱码风险，节省空间 UTF8MB4支持图像兼容4字节 ** 数据表、数据字段必须加入中文注释 解读：N年后谁tm知道这个r1,r2,r3字段是干嘛的 禁止使用存储过程、视图、触发器、Event 解读：高并发大数据的互联网业务，架构设计思路是“解放数据库CPU，将计算转移到服务层”，并发量大的情况下，这些功能很可能将数据库拖死，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能”。数据库擅长存储与索引，CPU计算还是上移吧 划重点：面试遇到要做存储过程的公司还是走了吧 禁止存储大文件或者大照片 解读：为何要让数据库做它不擅长的事情？大文件和照片存储在文件系统或者存云服务也行，数据库里存URI多好 命名规范（这个知道就好） 只允许使用内网域名，而不是ip连接数据库 库名、表名、字段名：小写，下划线风格，不超过32个字符,必须见名知意，禁止拼音英文混用。 表名t_xxx，非唯一索引名idx_xxx，唯一索引名uniq_xxx 线上环境、开发环境、测试环境数据库内网域名遵循命名规范 这个根据公司要求，只要人员能轻易区分数据库名称 业务名称：xxx 线上环境：dj.xxx.db 开发环境：dj.xxx.rdb 测试环境：dj.xxx.tdb 从库在名称后加-s标识，备库在名称后加-ss标识 线上从库：dj.xxx-s.db 线上备库：dj.xxx-sss.db 表设计规范 单实例表数目必须小于500 单表列数目必须小于30 表必须有主键，例如自增主键 主键递增,数据行写入可以提高插入性能,可以避免page分裂,减少表碎片提升空间和内存的使用 主键要选择较短的数据类型,Innodb引擎普通索引都会保存主键的值,较短的数据类型可以有效的减少索引的磁盘空间,提高索引的缓存效率 无主键的表删除，在row模式的主从架构，会导致备库夯住 禁止使用外键(现在一般都建立逻辑外键)，如果有外键完整性约束，需要应用程序控制 解读：：外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能，大数据高并发业务场景数据库使用以性能优先 字段设计规范 必须把字段定义为NOT NULL并且提供默认值 null的列使索引/索引统计/值比较都更加复杂,对MySQL来说更难优化 null 这种类型MySQL内部需要进行特殊处理,增加数据库处理记录的复杂性; 同等条件下,表中有较多空字段的时候,数据库的处理性能会降低很多 null值需要更多的存储空间,无论是表还是索引中每行中的 对null的处理时候,只能采用is null或is not null,而不能采用=、in、\u003c\u003e、!= not in这些操作符号如：where name!=’shenjian’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 禁止使用TEXT、BLOB类型 解读：会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能 禁止使用小数存储货币 解读：使用整数吧，小数容易导致钱对不上 必须使用varchar(20)存储手机号 解读：涉及到区号或者国家代号，可能出现+-() 手机号不会去做数学运算,所以请别弄成int类型 varchar可以支持模糊查询，例如：like“138%” 禁止使用ENUM，可使用TINYINT代替 解读：增加新的ENUM值要做DDL操作 ENUM的内部实际存储就是整数，你以为自己定义的是字符串？ 索引设计规范 单表索引建议控制在5个以内 单索引字段数不允许超过5个 解读：字段超过5个时，实际已经起不到有效过滤数据的作用了 禁止在更新十分频繁、区分度不高的属性上建立索引 离散性原因（离散性太差,优化器可能都不会走索引） 更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 “性别”这种区分度不大的属性，建立索引是没有什么意义的，不能有效过滤数据，性能与全表扫描类似 建立组合索引，必须把区分度高的字段放在前面 解读：能够更加有效的过滤数据（其中有离散性原因和最左匹配原则的原因） SQL使用规范 禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 解读：读取不需要的列会增加CPU、IO、NET消耗、并且不能有效的利用覆盖索引 禁止使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 解读：容易在增加或者删除字段后出现程序BUG 禁止使用属性隐式转换 解读：SELECT uid FROM t_user WHERE phone=13812345678 会导致全表扫描并且把phone字段转换成int类型来，而不能命中phone索引 禁止在WHERE条件的属性上使用函数或者表达式 这个非常重要 解读：SELECT uid FROM t_user WHERE from_unixtime(day)\u003e=‘2017-02-15’ 会导致全表扫描 正确的写法是：SELECT uid FROM t_user WHERE day\u003e= unix_timestamp(‘2017-02-15 00:00:00’) 禁止负向查询，以及%开头的模糊查询 解读：负向查询条件：NOT、!=、\u003c\u003e、!\u003c、!\u003e、NOT IN、NOT LIKE等，会导致全表扫描，在b+tree中在不确定情况下不知道走那条索引、%开头的模糊查询，会导致全表扫描，不走索引 禁止大表使用JOIN查询，禁止大表使用子查询 解读：连接查询在表多(一般3个)的或者大数据量情况下会非常慢(因为大数据量进行笛卡尔积的匹对,会耗费很多内存进行中间结果计算)并且不利于维护表结构：这样就应该在应用层做join拆分成单表查询 子查询会产生临时表，消耗较多内存与CPU，极大影响数据库性能 这里引申出一个老师给我的答案：非常感谢Alex老师 子查询会产生临时表效率低，效率低体现在两方面: 一、因为要产生临时表，所以空间复杂度很高，硬盘IO也很大； 二、因为是嵌套查询，所以时间复杂度也很高。查询算法的好坏要从空间复杂度和时间复杂度两个角度来思考。 JOIN查询有两个特点: 一、它的时间复杂度和嵌套查询是一样的，都是乘法级别的时间复杂度，举个例子，两张100条数据的表，两张表的join查询在 时间复杂度方面就相当于是查一张有10000条数据的表。如果是大表，一张表有1亿条数据，乘法级别的时间复杂度是很难想象的。 二、再来说说空间复杂度:子查询会产生临时表，并不代表JOIN查询就一定不会产生像临时表一样的东西，大多数的JOIN查询不 会产生像临时表一样的东西，这是因为数据量小，工作内存很够用。想像一下，两张有1亿条数据的表，把数据全部导到内存中， 然后进行条件查询比对，如果MySQL真的这样做，那么它可以改名叫redis缓存了。它的本质是硬盘数据库，不是缓存，所以数据量大时， 临时文件也一定要存在硬盘。也就是说，如果是JOIN查询的是大表，其实和你想像的大表子查询在过程方面是没有本质区别的。 这不光是58的军规，很多企业都是这样的，领导没有时间和每一个程序员去分析查询算法的空间复杂度与时间复杂度，所以才定军规。 禁止使用OR条件，必须改为IN查询 解读：旧版本MySQL的OR查询是不能命中索引的,即使能命中索引,为何要让数据库耗费更多的CPU帮助实施查询优化呢？ in 的查询算法是类似于二分法查找,条件在多的情况下效率肯定要比or好 应用程序必须捕获SQL异常，并有相应处理 总结：大数据量高并发的互联网业务，极大影响数据库性能的都不让用，不让用哟！所以你出问题了要有相应的处理并且方便维护，及时“查漏补缺” ","date":"2018-12-12","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%986%E8%A7%A3%E8%AF%BB58%E5%90%8C%E5%9F%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%86%9B%E8%A7%8430%E6%9D%A1/:0:0","tags":["MySQL"],"title":"MySQL性能调优(6)解读58同城数据库设计军规30条","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%986%E8%A7%A3%E8%AF%BB58%E5%90%8C%E5%9F%8E%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AE%BE%E8%AE%A1%E5%86%9B%E8%A7%8430%E6%9D%A1/"},{"categories":["MySQL性能调优"],"content":"MVCC Multiversion concurrency control (多版本并发控制) 并发访问(读或写)数据库时，对正在事务内处理的数据做多版本的管理。以达到用来避免写操作的堵塞，从而引发读操作的并发问题。 这里看一个案例 begin // 这里默认是加上X锁的，在未做commit/rollback操作之前 update users set lastUpdate=now() where id =1; // 在其他的事务我们能不能进行对应数据的查询 但是我这时查询是能查到的当前时间这条记录 select * from users where id = 1; 先来看看MVCC的处理机制 插入数据 同一个事务，事务行版本号相同 删除数据 ##### 修改数据 ##### 查询数据 这也就是之前案例为什么能查询数据的原因所在 ``` 记住图上的规则 1. 查找数据行版本号早于当前事务版本数据行 // 行的系统版本号小于或等于事务的系统版本号,这里就已经筛选出id等于1的两条数据 2. 查找删除版本号要么为null要么大于当前事务版本号记录 //为null说明没删除,大于当前事务版本号说明是事务开启过后 别的事务开启做的数据,这个是为了再次判断当前事务更改数据 ``` 那这些数据时怎么来的呢 Undo Log undo意为取消，以撤销操作为目的，返回指定某个状态的操作 UndoLog是为了实现事务的原子性而出现的产物 Undo Log实现事务原子性：事务处理过程中如果出现了错误或者用户执行了 ROLLBACK语句,MySQL可以利用Undo Log中的备份将数据恢复到事务开始之前的状态。 UndoLog在MySQL innodb存储引擎中用来实现多版本并发控制 Undo log实现多版本并发控制：事务未提交之前，Undo保存了未提交之前的版本数据，Undo 中的数据可作为数据旧版本快照供其他并发事务进行快照读 执行流程 这就是为什么加了X锁的数据我们还能查询的原因，他实际上是读取undo log里面的数据进行快照读 当前读与快照读的区别 Innodb引擎的普通select都是快照读。只有增删改和加了锁的才是当前读 Redo Log Redo，顾名思义就是重做。以恢复操作为目的，重现操作； Redo log指事务中操作的任何数据,将最新的数据备份到一个地方 (Redo Log) Redo log的持久：不是随着事务的提交才写入的，而是在事务的执行过程中，便开始写入redo 中。具体的落盘策略可以进行配置 RedoLog是为了实现事务的持久性而出现的产物 Redo Log实现事务持久性：防止在发生故障的时间点，尚有脏页未写入磁盘，在重启MySQL服务的时候，根据redo log进行重做，从而达到事务的未入磁盘数据进行持久化这一特性。 执行流程 Redo Log补充说明 这里可以看到redo log 默认大小是48M默认位置是在数据库根路径下面 指定Redo log 记录在{datadir}/ib_logfile1\u0026ib_logfile2 可通过innodb_log_group_home_dir 配置指定目录存储 一旦事务成功提交且数据持久化落盘之后，此时Redo log中的对应事务数据记录就失去了意义，所以Redo log的写入是日志文件循环写入的 指定Redo log日志文件组中的数量 innodb_log_files_in_group 默认为2 指定Redo log每一个日志文件最大存储量innodb_log_file_size 默认48M 指定Redo log在cache/buffer中的buffer池大小innodb_log_buffer_size 默认16M Redo buffer 持久化Redo log的策略， innodb_flush_log_at_trx_commit： 取值 0 每秒提交 Redo buffer --\u003e Redo log OS cache --\u003eflush cache to disk[可能丢失一秒内的事务数据] 取值 1 默认值，每次事务提交执行Redo buffer --\u003e Redo log OS cache --\u003eflush cache to disk[最安全，性能最差的方式] 取值 2 每次事务提交执行Redo buffer --\u003e Redo log OS cache 再每一秒执行 -\u003eflush cache to disk操作 配置优化 改配置说明 如果是docker可以通过启动选择配置文件启动 如果不想重启的话可以利用 set global set global 参数=? 寻找配置文件位置和加载顺序 mysql --help | grep -A 1 'Default options are read from the following files in the given order' 这是我docker中MySQL的配置文件位置和加载顺序 MySQL服务器参数类型 全局参数 set global autocommit = ON/OFF; 会话参数(\u003cfont color=red \u003e会话参数不单独设置则会采用全局参数\u003c/font\u003e) set session autocommit = ON/OFF; 注意： 全局参数的设定对于已经存在的会话无法生效 会话参数的设定随着会话的销毁而失效 全局类的统一配置建议配置在默认配置文件中，否则重启服务会导致配置失效 全局配置文件配置 最大连接数配置（如果连接数过大会有约束，就需要考虑改句柄数了） max_connections 系统句柄数配置 /etc/security/limits.conf ulimit -a MySQL句柄数配置 /usr/lib/systemd/system/mysqld.service 常用配置说明 port = 3306 #端口号 socket = /tmp/mysql.sock #为MySQL客户程序与服务器之间的本地通信指定一个套接字文件 basedir = /usr/local/mysql #安装 MySQL 的安装路径 datadir = /data/mysql #数据库文件路径 pid-file = /data/mysql/mysql.pid #记录当前MySQL进程的pid user = mysql #表示MySQL的管理用户 bind-address = 0.0.0.0 #MySQL服务器的IP地址。如果MySQL服务器所在的计算机有多个IP地址，这个选项将非常重要 max_connections=2000 #MySQL服务器同时处理的数据库连接的最大数量 lower_case_table_names = 0 #表名区分大小写 server-id = 1 #数据库id号一般主从复制的时候会用到 tmp_table_size=16M #临时表大小 transaction_isolation = REPEATABLE-READ #隔离级别 ready_only=1 #0是读写1是只读 其他参数配置 wait_timeout #服务器关闭非交互连接之前等待活动的秒数 innodb_open_files #限制Innodb能打开的表的个数 innodb_write_io_threads #innodb使用后台线程处理innodb缓冲区数据页上的写 I/O(输入输出)请求数 innodb_read_io_threads #innodb使用后台线程处理innodb缓冲区数据页上的读 I/O(输入输出)请求数 innodb_lock_wait_timeout #InnoDB事务在被回滚之前可以等待一个锁定的超时秒数 ","date":"2018-12-11","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%985innodb%E5%BC%95%E6%93%8E%E7%9A%84mvcc/:0:0","tags":["MySQL"],"title":"MySQL性能调优(5)Innodb引擎的MVCC","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%985innodb%E5%BC%95%E6%93%8E%E7%9A%84mvcc/"},{"categories":["MySQL性能调优"],"content":"事务 数据库操作的最小工作单元，是作为单个逻辑工作单元执行的一系列操作;事务是一组不可再分割的操作集合(工作逻辑单元); MySQL中如何开启事务： begin / start transaction -- 手工 commit / rollback -- 事务提交或回滚 set session autocommit = on/off; -- 设定事务是否自动开启 JDBC编程开启事务 connection.setAutoCommit(boolean); true：sql命令的提交（commit）由驱动程序负责 false：sql命令的提交由应用程序负责，程序必须调用commit或者rollback方法// connection.commit() 事务的ACID特性 原子性（Atomicity） 最小的工作单元，整个工作单元要么一起提交成功，要么全部失败回滚 一致性（Consistency） 事务中操作的数据及状态改变是一致的，即写入资料的结果必须完全符合预设的规则，不会因为出现系统意外等原因导致状态的不一致 原子性和一致性的的侧重点不同：原子性关注状态，要么全部成功，要么全部失败，不存在部分成功的状态。 而一致性关注数据的可见性，中间状态的数据对外部不可见，只有最初状态和最终状态的数据对外可见。 隔离性（Isolation） 一个事务所操作的数据在提交之前，对其他事务的可见性设定（一般设定为不可见） 持久性（Durability） 事务所做的修改就会永久保存，不会因为系统意外导致数据的丢失 理解并发带来的脏读、幻读、不可重复读 脏读 脏读就是指当一个事务正在访问数据，并且对数据进行了修改，而这种修改还没有提交到数据库中，这时，另外一个事务也访问这个数据，然后使用了这个数据。 不可重复读（同样的条件,你读取过的数据,再次读取出来发现值不一样了 ） 是指在一个事务内，多次读同一数据。在这个事务还没有结束时，另外一个事务也访问该同一数据。那么，在第一个事务中的两次读数据之间，由于第二个事务的修改，那么第一个事务两次读到的的数据可能是不一样的。这样在一个事务内两次读到的数据是不一样的，因此称为是不可重复读。 幻读 ( 幻读的重点在于新增或者删除) 是指当事务不是独立执行时发生的一种现象，例如第一个事务对一个表中的数据进行了修改，这种修改涉及到表中的全部数据行。同时，第二个事务也修改这个表中的数据，这种修改是向表中插入一行新数据。那么，以后就会发生操作第一个事务的用户发现表中还有没有修改的数据行，就好象发生了幻觉一样。 事务的隔离级别 MySQL 8.0 查看事务的默认隔离级别 show variables like 'transaction_isolation'; Read Uncommitted（未提交读简称RU） –未解决并发问题 事务未提交对其他事务也是可见的,也被称之为脏读(Dirty Read)。 Read Committed（提交读简称RC） –解决脏读问题 事务成功提交后才可以被查询到,也被称为不可重复读(Nonrepeatable Read)。 Repeatable Read (可重复读简称RR) –解决不可重复读问题 在同一个事务中多次读取同样的数据结果是一样的，这种隔离级别未定义解决幻读的问题MySQL默认级别;Innodb引擎通过MVCC来解决幻读问题 Serializable（串行化） –解决所有问题 最高的隔离级别，通过强制事务的串行执行 Innodb对隔离级别的支持程度 Innodb事务的隔离级别是通过锁+MVCC实现的 | 事务的隔离级别 | 脏读 | 不可重复读 |幻读| | :——: | :——: | :——: | | 未提交读(Read Uncommitted) | 可能 |可能|可能| | 已提交读(Read Committed) | 不可能 |可能|可能| | 可重复读(Repeatable Read) | 不可能 |不可能|对Innodb不可能(具体看MVCC)| | 可串行化(Serializable) | 不可能 |不可能|不可能| 锁 锁是用于管理不同事务对共享资源的并发访问 表锁与行锁的区别： 锁定粒度：表锁 \u003e 行锁 加锁效率：表锁 \u003e 行锁 冲突概率：表锁 \u003e 行锁 并发性能：表锁 \u003c 行锁 InnoDB存储引擎支持行锁和表锁（整个表所有行加锁） 行锁 InnoDB的行锁是通过给索引上的索引项加锁来实现的。只有通过索引条件进行数据检索，InnoDB才使用行级锁，否则，InnoDB将使用表锁（锁住索引的所有记录） lock tables xx read/write；// 锁定表分别是表的读锁和写锁 UNLOCK TABLES;// 释放锁 共享锁(Shared Locks) 又称为读锁，简称S锁，顾名思义，共享锁就是多个事务对于同一数据可以共享一把锁，都能访问到数据，但是只能读不能修改; 加锁释锁方式： select * from users WHERE id=1 LOCK IN SHARE MODE; commit/rollback //提交或者回滚 排他锁(Exclusive Locks) 又称为写锁，简称X锁，排他锁不能与其他锁并存，如一个事务获取了一个数据行的排他锁，其他事务就不能再获取该行的锁（共享锁、排他锁）, 只有该获取了排他锁的事务是可以对数据行进行读取和修改，（其他事务要读取数据可来自于快照）： 加锁释锁方式： delete / update / insert 默认加上X锁 SELECT * FROM table_name WHERE ... FOR UPDATE commit/rollback 意向共享锁(IS)与意向排它锁(IX) 意向锁(IS、IX)是InnoDB数据操作之前自动加的，不需要用户干预，但是要理解有这么一个流程 意义：当事务想去进行锁表时，可以先判断意向锁是否存在，存在时则可快速返回该表不能启用表锁。 意向共享锁(IS) 表示事务准备给数据行加入共享锁，即一个数据行加共享锁前必须先取得该表的IS锁，意向共享锁之间是可以相互兼容的 意向排它锁(IX) 表示事务准备给数据行加入排他锁，即一个数据行加排他锁前必须先取得该表的IX锁，意向排它锁之间是可以相互兼容的 自增锁 针对自增列自增长的一个特殊的表级别锁； 开启事务，添加数据时，然后回滚了，那么这个id就永久消失了； show variables like 'innodb_autoinc_lock_mode'; MySQL8.0 默认取值 2 0：traditonal （每次都会产生表锁） 1：consecutive （会产生一个轻量锁，simple insert会获得批量的锁，保证连续插入） 2：interleaved （不会锁表，来一个处理一个，并发最高） // 这块没有深入理解 记录锁(Record)间隙锁(Gap)临键锁(Next-key) 首先来看看测试的数据库表结构 Next-key locks临键锁（Innodb） 锁住记录+区间（左开右闭） 当sql执行按照索引进行数据的检索时,查询条件为范围查找（between and、\u003c、\u003e等）并有数据命中则此时SQL语句加上的锁为Next-key locks，锁住索引的记录+区间（左开右闭） Gap locks间隙锁： 锁住数据不存在的区间（左开右开） 当sql执行按照索引进行数据的检索时，查询条件的数据不存在，这时SQL语句加上的锁即为Gap locks，锁住索引不存在的区间（左开右开） Record locks记录锁： 记录锁可以说是最好的性能了：只锁了一条记录：这就是我们为什么要键索引，索引要建立好的原因。 锁住具体的索引项 当sql执行按照唯一性（Primary key、Unique key）索引进行数据的检索时，查询条件等值匹配且查询的数据是存在，这时SQL语句加上的锁即为记录锁Record locks，锁住具体的索引项。 利用锁解决脏读、不可重复读、幻读 脏读：可以通多写数据时给数据加上X排它锁解决 不可重复读：可以通过读取时加上S共享锁来解决 幻读：可以通过临键锁来解决 死锁 多个并发事务(2个或者以上),事务之间产生加锁的循环等待，形成死锁。 怎样避免死锁 类似的业务逻辑以固定的顺序访问表和行。 大事务拆小。大事务更倾向于死锁，如果业务允许，将大事务拆小。 在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁概 率。 降低隔离级别，如果业务允许，将隔离级别调低也是较好的选择 为表添加合理的索引。可以看到如果不走索引将会为表的每一行记录添加上锁（或者说是表锁） 一但不走索引，innodb就会提升为表锁，这样就会增加死锁冲突几率 ","date":"2018-12-10","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%984innodb%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E7%9A%84%E4%BA%8B%E5%8A%A1/:0:0","tags":["MySQL"],"title":"MySQL性能调优(4)Innodb存储引擎的事务","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%984innodb%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E7%9A%84%E4%BA%8B%E5%8A%A1/"},{"categories":["JVM"],"content":" 类的静态成员 –\u003e 类的实例成员 –\u003e 类的构造方法 说到这里，有一个很好的问题。比如一个类中静态变量是一个该类的实例，那么此时类的初始化顺序是什么样的呢？ 昨天在群里看到这个问题，很有趣，今天来分析一波 public class test { public static test t1=new test(); { System.out.println(\"执行1\"); } static { System.out.println(\"执行2\"); } public static void main(String[] args) { test t2=new test(); } } 首先我们得理解的执行顺序 由于test类是JVM启动类，所以首先被加载，也就是执行加载、连接、初始化这些流程。 进入初始化阶段，该阶段会对static域进行初始化。 同为static按顺序执行所以这时只初始化了t1 由于这时test类已经处于初始化阶段了，static无需再次初始化，否则会导致static域多次初始化情况，所以暂时忽略static域初始化操作，对非static域进行初始化操作这时打印了第一句话 然后这时再执行t2的操作由于t1已经加载，所以加载静态块然后在加载构造块。 最后执行结果 执行1 执行2 执行1 如果理解了上面：来看一哈Alibaba的14年的一道校招附加题 public class Alibaba { public static int k = 0; public static Alibaba t1 = new Alibaba(\"t1\"); public static Alibaba t2 = new Alibaba(\"t2\"); public static int i = print(\"i\"); public static int n = 99; private int a = 0; public int j = print(\"j\"); { print(\"构造块\"); } static { print(\"静态块\"); } public Alibaba(String str) { System.out.println((++k) + \":\" + str + \" i=\" + i + \" n=\" + n); ++i; ++n; } public static int print(String str) { System.out.println((++k) + \":\" + str + \" i=\" + i + \" n=\" + n); ++n; return ++i; } public static void main(String args[]) { Alibaba t = new Alibaba(\"init\"); } } 由于Alibaba是 JVM 的启动类，属于主动调用，所以会依此进行 loading(加载)、linking(连接)、initialization(初始化)三个过程。 经过 loading与 linking 阶段后，所有的属性都有了默认值，然后进入最后的 initialization 阶段。 在 initialization 阶段，先对 static 属性赋值，然后在非 static 的。k 第一个显式赋值为 0 。 接下来是t1属性，由于这时Alibaba这个类已经处于initialization 阶段，static 变量无需再次初始化了，所以忽略 static 属性的赋值，只对非 static 的属性进行赋值，所有有了开始的： // 这时调用的是第9行的代码进入print方法 // 再执行构造块 // 执行构造方法 1:j i=0 n=0 2:构造块 i=1 n=1 3:t1 i=2 n=2 接着对t2进行赋值，过程与t1相同 // 这时调用的是第9行的代码进入print方法 // 再执行构造块 // 执行构造方法 4:j i=3 n=3 5:构造块 i=4 n=4 6:t2 i=5 n=5 之后到了 static 的 i ： // 这时调用的是第9行的代码进入print方法 7:i i=6 n=6 执行static 的 n 赋值 到现在为止，所有的static的成员变量已经赋值完成，接下来就到了 static 代码块 // 调用静态块 8:静态块 i=7 n=99 至此，所有的 static 部分赋值完毕，接下来是非 static 的 j // 调用print 9:j i=8 n=100 所有属性都赋值完毕，然后是构造块 // 调用构造块{} 10:构造块 i=9 n=101 执行构造函数 // 最后构造函数 11:init i=10 n=102 Alibaba这个类的初始化过程就算完成了。这里面比较容易出错的是认为会再次初始化 static 变量或代码块。而实际上是没必要，否则会出现多次初始化的情况。 最后执行结果 1:j i=0 n=0 2:构造块 i=1 n=1 3:t1 i=2 n=2 4:j i=3 n=3 5:构造块 i=4 n=4 6:t2 i=5 n=5 7:i i=6 n=6 8:静态块 i=7 n=99 9:j i=8 n=100 10:构造块 i=9 n=101 11:init i=10 n=102 ","date":"2018-12-07","objectID":"/jvm%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F/:0:0","tags":["JVM"],"title":"JVM初始化顺序","uri":"/jvm%E5%88%9D%E5%A7%8B%E5%8C%96%E9%A1%BA%E5%BA%8F/"},{"categories":["MySQL性能调优"],"content":"存储引擎介绍 插拔式的插件方式（存储引擎本身是数据库服务器的组件，负责对在物理服务器层面上维护的基本数据进行实际操作） 存储引擎是指定在表之上的，即一个库中的每一个表都可以指定专用的存储引擎 最新的MySQL 8.0 发布之后，对数据库数据字典方面做了较大的改进。 首先是，将所有原先存放于数据字典文件中的信息，全部存放到数据库系统表中，即将之前版本的.frm,.opt,.par,.TRN,.TRG,.isl文件都移除了，不再通过文件的方式存储数据字典信息。 其次是对INFORMATION_SCHEM，MySQL，sys系统库中的存储引擎做了改进，原先使用MyISAM存储引擎的数据字典表都改为使用InnoDB存储引擎存放。 从不支持事务的MyISAM存储引擎转变到支持事务的InnoDB存储引擎，为原子DDL的实现，提供了可能性。 // 查看数据库当前支持的存储引擎 show engines; 存储引擎 CVS存储引擎 数据存储以CSV文件 特点： 不能定义没有索引、列定义必须为NOT NULL、不能设置自增列 –\u003e不适用大表或者数据的在线处理 CSV数据的存储用,隔开，可直接编辑CSV文件进行数据的编排 –\u003e数据安全性低 注：编辑之后，要生效使用flush table 表名 命令 应用场景： 数据的快速导出导入 表格直接转换成CSV Archive存储引擎 压缩协议进行数据的存储 数据存储为ARZ文件格式 特点： 只支持insert和select两种操作 只允许自增ID列建立索引 行级锁 不支持事务 数据占用磁盘少 应用场景： 日志系统 大量的设备数据采集 我同时在Archive和innodb同结构表中插入1000000条数据:看看大小区别：这是只有一个id和name的情况下。如果字段更多的话，archive的压缩率区别就更大了。 Memory存储引擎 数据都是存储在内存中，IO效率要比其他引擎高很多 服务重启数据丢失，内存数据表默认只有16M 特点： 支持hash索引，B tree索引，默认hash（查找复杂度0(1)） 字段长度都是固定长度varchar(32)=char(32) 不支持大数据存储类型字段如 blog，text 表级锁 应用场景： 等值查找热度较高数据 查询结果内存中的计算，大多数都是采用这种存储引擎作为临时表存储需计算的数据 不过现在基本上用缓存数据库了 Myisam存储引擎 MySQL5.5版本之前的默认存储引擎 特点： select count(*) from table 无需进行数据的扫描 数据（MYD）和索引（MYI）分开存储 表级锁 不支持事务 Innodb存储引擎 现在是数据库默认引擎 特点： 支持事务 行级锁 聚集索引（主键索引）存储，如果没有主键，那么会有唯一键来做索引，如果还没有就生成一个隐藏的主键索引。 支持外键关系保证数据完整性（不过我现在开发中比较少用了。依开发项目实质性来取决于做不做外键关联。） 为何说外键有性能问题： 1.数据库需要维护外键的内部管理； 2.外键等于把数据的一致性事务实现，全部交给数据库服务器完成； 3.有了外键，当做一些涉及外键字段的增，删，更新操作之后，需要触发相关操作去检查，而不得不消耗资源； 4.外键还会因为需要请求对其他表内部加锁而容易出现死锁情况； 对比 MVCC是多版本并发控制：后续单独分析； MySQL体系结构及运行机理 先看看体系结构 客户端连接器：JDBC,ODBC,.NET,PHP等等 连接管理： Connection Pool连接池：管理缓冲用户连接、用户名、密码、权限校验、线程处理等需要缓存的需求 Management Serveices \u0026 Utilities系统管理和控制工具，mysqldump、 MySQL复制集群、分区管理等 SQL接口：接受用户的SQL命令，并且返回用户需要查询的结果 SQL解析器:SQL命令传递到解析器的时候会被解析器验证和解析。解析器是由Lex和YACC实现的 查询优化器:SQL语句在查询之前会使用查询优化器对查询进行优化 Cache和Buffer（高速缓存区）：如果查询缓存有命中的查询结果，查询语句就可以直接去查询缓存中取数据 存储引擎：pluggable storage Engines 插件式存储引擎、存储引擎是MySQL中具体的与文件打交道的子系统 系统管理：file system 文件系统，数据、日志（redo，undo）、索引、错误日志、查询记录、慢查询等 单独分析查询优化 ","date":"2018-12-06","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%982%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E4%BB%8B%E7%BB%8D%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%8F%8A%E8%BF%90%E8%A1%8C%E6%9C%BA%E7%90%86/:0:0","tags":["MySQL"],"title":"MySQL性能调优(2)存储引擎介绍、体系结构及运行机理","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%982%E5%AD%98%E5%82%A8%E5%BC%95%E6%93%8E%E4%BB%8B%E7%BB%8D%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84%E5%8F%8A%E8%BF%90%E8%A1%8C%E6%9C%BA%E7%90%86/"},{"categories":["MySQL性能调优"],"content":"查询执行路径 ##### MySQL 客户端/服务端通信 - MySQL客户端与服务端的通信方式是“半双工”; - 客户端一旦开始发送消息，另一端要接收完整个消息才能响应。客户端一旦开始接收数据没法停下来发送指令。 - 对于一个MySQL连接，或者说一个线程，时刻都有一个状态来标识这个连接正在做什么 - 查看命令 show full processlist / show processlist 我正在通过navicat向虚拟机里面的数据库导入数据 ``` Sleep 线程正在等待客户端发送数据 Query 连接线程正在执行查询 Locked 线程正在等待表锁的释放 Sorting result 线程正在对结果进行排序 Sending data 向请求端返回数据 可通过kill {id}的方式进行连接的杀掉 ``` [官网状态全集](https://dev.mysql.com/doc/refman/8.0/en/general-thread-states.html) 查询缓存 前话：为什么MySQL默认关闭了缓存开启？？ MySQL 8.0不支持查询缓存，用户升级后将被鼓励使用服务器端查询重写或ProxySQL作为中间缓存。 下面是了解缓存应该知道的知识： 工作原理： 缓存SELECT操作的结果集和SQL语句； 新的SELECT语句，先去查询缓存，判断是否存在可用的记录集； 判断标准： 与缓存的SQL语句，是否完全一样(包括语句内的空格)，区分大小写 (简单认为存储了一个key-value结构，key为sql，value为sql查询结果集) 不会缓存的情况 当查询语句中有一些不确定的数据时，则不会被缓存。如包含函数NOW()，CURRENT_DATE()等类似的函数，或者用户自定义的函数，存储函数，用户变量等都不会被缓存 当查询的结果大于query_cache_limit(查询缓存返回行数)设置的值时，结果不会被缓存 对于InnoDB引擎来说，当一个语句在事务中修改了某个表，那么在这个事务提交之前，所有与这个表相关的查询都无法被缓存。因此长时间执行事务，会大大降低缓存命中率 查询的表是系统表 查询语句不涉及到表 当前表在之前做过其他增删改，这个表中其他数据做的缓存都会失效、 比如当前表中一行数据你做了缓存，表中其他数据更改了，也会影响这行数据的缓存失效。 查询优化处理 解析sql 通过lex词法分析,yacc语法分析将sql语句解析成解析树学习连接 预处理阶段 根据MySQL的语法的规则进一步检查解析树的合法性，如：检查数据的表和列是否存在，解析名字和别名的设置。还会进行权限的验证 查询优化器 优化器的主要作用就是找到最优的执行计划 如何找到最优执行计划 使用等价变化规则 5 = 5 and a \u003e 5 改写成 a \u003e 5 a \u003c b and a = 5 改写成 b \u003e 5 and a = 5 基于联合索引，调整条件位置等 优化count 、min、max等函数 min函数只需找索引最左边(因为innodb本身索引有序) max函数只需找索引最右边(因为innodb本身索引有序) myisam引擎count(*) 覆盖索引扫描 比如你查询返回的字段命中了索引，他就会直接返回数据：这里我给uname建立了索引 子查询优化（我们做优化的话：最好就是把子查询改成连接查询） 子查询合并。即把多个子查询合并成一个子查询，作用是减少元组扫描的次数 子查询反嵌套。也称作子查询上拉，即将子查询重写为等价的多表连接 聚集子查询消除。即聚集函数上推，将消除聚集函数的子查询与父查询做外连接 提前终止查询 用了limit关键字(他就索引limit长度的数据)或者使用不存在的条件; IN的优化 先进性排序，再采用二分查找的方式; 在条件多的情况下，or还是会一个个去扫描判断，而in他会先找到中间的在判断;再根据大小判断。这样就会减少多余的扫描。 MySQL的查询优化器是基于成本计算的原则。他会尝试各种执行计划。数据抽样的方式进行试验（随机的读取一个4K的数据块进行分析） 查询执行引擎 执行计划顺序 select查询的序列号，标识执行的顺序 1. id相同是相当于一组，执行顺序由上至下 2. id不同，如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行 3. id相同又不同即两种情况同时存在，id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行 查询类型select_type 查询的类型，主要是用于区分普通查询、联合查询、子查询等 SIMPLE：简单的select查询，查询中不包含子查询或者union PRIMARY：查询中包含子部分，最外层查询则被标记为primary SUBQUERY/MATERIALIZED：SUBQUERY表示在select 或 where列表中包含了子查询 MATERIALIZED表示where 后面in条件的子查询 UNION：若第二个select出现在union之后，则被标记为union UNION RESULT：从union表获取结果的select 执行计划table 查询涉及到的表,直接显示表名或者表的别名 \u003cunionM,N\u003e 由ID为M,N 查询union产生的结果 \u003csubqueyN\u003e 由ID为N查询生产的结果 执行计划type 所以优化得至少在range及以上 访问类型，sql查询优化中一个很重要的指标，结果值从好到坏依次是：system \u003e const \u003e eq_ref \u003e ref \u003e range \u003e index \u003e ALL system：表只有一行记录（等于系统表），const类型的特例，基本不会出现，可以忽略不计 const：表示通过索引一次就找到了，const用于比较primary key 或者 unique索引 eq_ref：唯一索引扫描，对于每个索引键，表中只有一条记录与之匹配。常见于主键 或 唯一索引扫描 ref：非唯一性索引扫描，返回匹配某个单独值的所有行，本质是也是一种索引访问 range：只检索给定\u003cfont color=red\u003e范围\u003c/font\u003e的行，使用一个索引来选择行 index：Full Index Scan，索引全表扫描，把索引从头到尾扫一遍 ALL：Full Table Scan，遍历全表以找到匹配的行 其他关键字 possible_keys:查询过程中有可能用到的索引 key:实际使用的索引，如果为NULL，则没有使用索引 rows:根据表统计信息或者索引选用情况，大致估算出找到所需的记录所需要读取的行数 filtered:它指返回结果的行占需要读到的行(rows列的值)的百分比表示返回结果的行数占需读取行数的百分比,filtered的值越大越好 额外信息Extra Using filesort ： MySQL对数据使用一个外部的文件内容进行了排序，而不是按照表内的索引进行排序读取 Using temporary： 使用临时表保存中间结果，也就是说MySQL在对查询结果排序时使用了临时表，常见于order by 或 group by但是innodb用id排序分组是不会出现临时表的。 Using index： 表示相应的select操作中使用了覆盖索引（Covering Index），避免了访问表的数据行，效率高 Using where ： 使用了where过滤条件 select tables optimized away： 基于索引优化MIN/MAX操作或者MyISAM存储引擎优化COUNT(*)操作，不必等到执行阶段在进行计算，查询执行计划生成的阶段即可完成优化 分析一个组合查询 这条语句就是把两个子查询查出来再结合 这里有个特殊情况：id等于null那个是最后执行的，因为他需要的到其他查询结果结合。 //-------------------------------- 1. 这里就看id最大的4：select_type是subquery说明是子查询；type为ref非唯一性索引扫描；用到了id和address索引。 2. id为3的开始执行，select_type为union说明第二个select出现在union之后，则被标记为union； 3. id等于2和id等于1就是之前的子查询与查询；并且用到了主键索引。 4. 然后最后再把1和3合并；这里可以看出组合查询使用到了临时表。 执行引擎 调用插件式的存储引擎的原子API的功能进行执行计划的执行 返回客户端 增量的返回结果： 开始生成第一条结果时,MySQL就开始往请求方逐步返回数据 好处： MySQL服务器无须保存过多的数据，浪费内存用户体验好，马上就拿到了数据 慢查询日志 // 查看是否打开慢查询日志 show variables like 'slow_query_log' // 打开慢查询日志 set global slow_query_log = on // 保存日志位置 set global slow_query_log_file ='/var/lib/mysql/gupaoedu-slow.log' // 没有命中索引的都保存 set global log_queries_not_using_indexes = on // 超过多少秒记录日志 set global long_query_time = 0.1 (秒) //查看是否开启 show variables like 'slow_query%'; 慢日志文件查看 Time ：日志记录的时间 User@Host：执行的用户及主机 ","date":"2018-12-06","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%983%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E8%AF%A6%E8%A7%A3/:0:0","tags":["MySQL"],"title":"MySQL性能调优(3)查询优化详解","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%983%E6%9F%A5%E8%AF%A2%E4%BC%98%E5%8C%96%E8%AF%A6%E8%A7%A3/"},{"categories":["工具"],"content":"自动编译开关 勾选上：你每次切换界面都会自动编译 忽略大小写开关 关闭这个开关：string 能直接提示String 智能导包开关 你在代码中，只要敲list，就会出现提示，自动导入java.util.List这个类 悬浮提示开关 把鼠标放在类上会出现相应提示 取消单行显示tabs的操作 效果如下 项目文件编码 Transparent native-to-ascii conversion的意思是：自动转换ASCII编码。 他的工作原理是：在文件中输入文字时他会自动的转换为Unicode编码，然后在idea中发开文件时他会自动转回文字来显示。 这样做是为了防止文件乱码。 这样你的properties文件，一般都不会出现中文乱码！ 滚轴修改字体大小 ctrl+鼠标滚动就可以设置字体大小，类似于windows桌面图标放大 设置行号显示 如果用了lombok注解要把注解打开 ","date":"2018-12-04","objectID":"/idea%E4%B8%80%E5%AE%9A%E8%A6%81%E6%94%B9%E7%9A%84%E5%87%A0%E6%9D%A1%E9%85%8D%E7%BD%AE/:0:0","tags":["IDEA"],"title":"IDEA一定要改的几条配置","uri":"/idea%E4%B8%80%E5%AE%9A%E8%A6%81%E6%94%B9%E7%9A%84%E5%87%A0%E6%9D%A1%E9%85%8D%E7%BD%AE/"},{"categories":["数据结构与算法"],"content":"以前别人面试我,这个问题的时候我一般都是回答：linkendlist增删改块，arraylist查找块。直到最近我看了掘金的一篇博文，才发现，实践出真知啊。 测试结果 分别在ArrayList和LinkedList的头部、尾部和中间三个位置插入与查找100000个元素所消耗的时间来进行对比测试，下面是测试结果 List 插入 查找 ArrayList头部 2859ms 7ms ArrayList尾部 26ms 12ms ArrayList中间 848ms 13ms LinkedList头部 15ms 11ms LinkedList尾部 28ms 11ms LinkedList中间 15981ms 34928ms 测试结论 ArrayList的查找性能绝对是一流的，无论查询的是哪个位置的元素 ArrayList除了尾部插入的性能较好外（位置越靠后性能越好），其他位置性能就不如人意了 LinkedList在头尾查找头尾性能都很棒，但是在中间位置进行操作的话，性能就差很远了，而且跟ArrayList完全不是一个量级的，并且Linkedlist并不是插入哪里性能都比Arraylist快，越靠中间，插入越慢。 源码分析 我们把Java中的ArrayList和LinkedList就是分别对顺序表和双向链表的一种实现： 顺序表：需要申请连续的内存空间保存元素，可以通过内存中的物理位置直接找到元素的逻辑位置。在顺序表中间插入or删除元素需要把该元素之后的所有元素向前or向后移动。 双向链表：不需要申请连续的内存空间保存元素，需要通过元素的头尾指针找到前继与后继元素（查找元素的时候需要从头or尾开始遍历整个链表，直到找到目标元素）。在双向链表中插入or删除元素不需要移动元素，只需要改变相关元素的头尾指针即可。 所以我们潜意识会认为：ArrayList查找快，增删慢。LinkedList查找慢，增删快；但实际上并不是这样的。 ArrayList尾部插入 add(E e)方法 public boolean add(E e) { // 检查是否需要扩容 ensureCapacityInternal(size + 1); // Increments modCount!! // 直接在尾部添加元素 elementData[size++] = e; return true; } LinkedList尾部插入 LinkedList中定义了头尾节点 /** * Pointer to first node. */ transient Node\u003cE\u003e first; /** * Pointer to last node. */ transient Node\u003cE\u003e last; add(E e)方法，该方法中调用了linkLast(E e)方法 public boolean add(E e) { linkLast(e); return true; } linkLast(E e)方法，可以看出，在尾部插入的时候，并不需要从头开始遍历整个链表，因为已经事先保存了尾结点，所以可以直接在尾结点后面插入元素 /** * Links e as last element. */ void linkLast(E e) { // 先把原来的尾结点保存下来 final Node\u003cE\u003e l = last; // 创建一个新的结点，其头结点指向last final Node\u003cE\u003e newNode = new Node\u003c\u003e(l, e, null); // 尾结点置为newNode last = newNode; if (l == null) first = newNode; else // 修改原先的尾结点的尾结点，使其指向新的尾结点 l.next = newNode; size++; modCount++; } 对于尾部插入而言，ArrayList与LinkedList的性能几乎是一致的 ArrayList头部插入 add(int index, E element)方法，可以看到通过调用系统的数组复制方法来实现了元素的移动。所以，插入的位置越靠前，需要移动的元素就会越多 public void add(int index, E element) { rangeCheckForAdd(index); ensureCapacityInternal(size + 1); // 把原来数组中的index位置开始的元素全部复制到index+1开始的位置（其实就是index后面的元素向后移动一位） System.arraycopy(elementData,index,elementData, index + 1,size - index); // 插入元素 elementData[index] = element; size++; } LinkedList头部插入 add(int index, E element)方法，该方法先判断是否是在尾部插入，如果是调用linkLast()方法，否则调用linkBefore()，那么是否真的就是需要重头开始遍历呢？我们一起来看看 public void add(int index, E element) { checkPositionIndex(index); if (index == size) linkLast(element); else linkBefore(element, node(index)); } linkBefore方法 这个函数的工作就只是负责把元素插入到相应的位置而已，关键的工作在node()方法中已经完成了 void linkBefore(E e, Node\u003cE\u003e succ) { // assert succ != null; final Node\u003cE\u003e pred = succ.prev; final Node\u003cE\u003e newNode = new Node\u003c\u003e(pred, e, succ); succ.prev = newNode; if (pred == null) first = newNode; else pred.next = newNode; size++; modCount++; } node方法 Node\u003cE\u003e node(int index) { // assert isElementIndex(index); if (index \u003c (size \u003e\u003e 1)) { Node\u003cE\u003e x = first; for (int i = 0; i \u003c index; i++) x = x.next; return x; } else { Node\u003cE\u003e x = last; for (int i = size - 1; i \u003e index; i--) x = x.prev; return x; } } 在头尾以外的位置插入元素当然得找出这个位置在哪里，这里面的node()方法就是关键所在，这个函数的作用就是根据索引查找元素，但是它会先判断index的位置，如果index比size的一半(size » 1,右移运算，相当于除以2)要小，就从头开始遍历。否则，从尾部开始遍历。从而可以知道，对于LinkedList来说，操作的元素的位置越往中间靠拢，效率就越低。 ArrayList、LinkedList查找 这就没啥好说的了，对于ArrayList，无论什么位置，都是直接通过索引定位到元素，时间复杂度O(1) 而对于LinkedList查找，其核心方法就是上面所说的node()方法，所以头尾查找速度极快，越往中间靠拢效率越低 总结 对于LinkedList来说，头部插入和尾部插入时间复杂度都是O(1) 但是对于ArrayList来说，头部的每一次插入都需要移动size-1个元素，效率可想而知 但是如果都是在最中间的位置插入的话，ArrayList速度比LinkedList的速度快将近10倍 ","date":"2018-12-04","objectID":"/linkedlist%E7%9C%9F%E7%9A%84%E6%98%AF%E6%9F%A5%E6%89%BE%E6%85%A2%E5%A2%9E%E5%88%A0%E5%BF%AB%E5%90%97/:0:0","tags":["List"],"title":"LinkedList真的是查找慢增删快吗","uri":"/linkedlist%E7%9C%9F%E7%9A%84%E6%98%AF%E6%9F%A5%E6%89%BE%E6%85%A2%E5%A2%9E%E5%88%A0%E5%BF%AB%E5%90%97/"},{"categories":["MySQL性能调优"],"content":"索引是谁实现的 索引是存储引擎实现的： 本文章主要对MySQL常用的MyISAM与InnoDB这两个存储引擎做分析。 索引是什么 索引是为了加速对表中的数据行的检索而创建的一种分散存储的数据结构。 为什么要用索引 索引能极大的减少存储引擎需要扫描的数据量。(比如全表扫描就是在找数据) 索引可以把随机IO变成顺序IO。（因为索引是有序的这样就能保证找数据的时候稳定性,在程序中不允许有不稳定因素。） 为什么MySQL要用b+tree来实现索引 在这里先推荐一个网址来学习二叉树由来地址。 先来看看二叉查找树 Binary Search Tree 在来看看平衡二叉查找树（所有节点数高度不会超过1）AVL Trees (Balanced binary search trees)记住图上的磁盘块上存储了数据区的磁盘地址。 说说为什么MySQL没有选择这些算法而去选择B+Tree 它太深了,数据存在的（高）深度决定着他的IO操作次数，IO操作耗时大这个大家都是知道的。 他太小了,IO操作是很耗时，他一次IO也只能加载一个关键字；保存的东西太少了。 没有很好的利用操作磁盘IO的数据交换特性(操作系统通过硬盘读取数据一次IO操作读取的大小是4k(页为单位))这就是为什么SSD在分区时选择4k对其的原因，能使他大大提升读写性能。 二叉树每个节点只有一个关键字，他是存满不了4k的，这样会浪费资源。 也没有利用好磁盘IO的预读能力（空间局部性原理），从而带来频繁的IO操作。空间局部性原理就是：操作系统每次IO操作读取一页，他会有预读能力，把下一页或者后面几页的数据读取。（注：MySQL定义的一页为16k） 多路平衡查找树B-Tree 上图是一个二三树（没画完 画不下了）。2路以上为多路。关键字个数为路数-1。 从IO太深了来看，他就比平衡二叉树好的多，他一个节点就可以存储更多的数据，这里设置的是3路，我们可以改成更多路。一个节点存储更多关键字。 还可以把一个节点大小设置为16k这样 用id做索引 int大小 4byte+冗余4byte 节点16k*1024byte 那么我可以保存16*1024/8 个关键字 这就是为什么我们字段大小要设计精确，不要有太多的冗余 这里的关键字多了，也就解决了空间局部性原理，读取更多数据。 加强版多路平衡查找树B+Tree B+非叶节点不保存数据相关信息，只保存关键字和子节点的引用（扫库扫表能力更强，磁盘读写能力更强）。 B+关键字对应的数据保存在叶子节点中（查询更稳定）。 B+对排序有天然的优势，叶子结点是顺序排列（排序能力更强）。 采用的是左闭合区间，这样就是关键字于区间就是相同个数。 在存储引擎MyISAM中B+Tree的体现形式 1.用单独文件(后缀myd文件)来存放索引数据，是通过页子节点保存数据地址 单个索引 多个索引 （对应的数据是同一条就是指向一个位置） 在存储引擎Innodb中B+Tree的体现形式 Innodb中的索引是与数据存放在一起的，是以主键为索引来组织数据的存储。如果没有主键，那么他会默认生成一个int长度为6 byte的隐藏主键。 Innodb中辅助索引，他会存放主键索引，找辅助数据时，他会再索引一遍主键索引。 聚集索引只在innodb中会出现，因为他是基于主键的索引，有序的。 Innodb与MyISAM对比索引流程 补充知识 列的离散性: 离散性越高、选择性就越好（列数据差别越大，离散性越好）。 最左匹配原则： 对索引中关键字进行计算（对比），一定是从左往右依次进行，且不可跳过。 联合索引 //单列索引是特殊的联合索引 单列索引:节点中关键字[name] 联合索引:节点中关键字[name,phoneNum] 联合索引列选择原则 经常用的列优先 【最左匹配原则】 选择性（离散度）高的列优先【离散度高原则】 宽度小的列优先【最少空间原则】 常用解决方案误区 经排查发现最常用的sql语句： Select * from users where name = ? ; Select * from users where name = ? and phoneNum = ?; 机灵的李二狗的解决方案： create index idx_name on users(name);× create index idx_name_phoneNum on users(name,phoneNum); 这上面的两种查询：只有第二种方案才能都命中索引。 覆盖索引 如果查询列可通过索引节点中的关键字直接返回，则该索引称之为覆盖索引。 覆盖索引可减少数据库IO，将随机IO变为顺序IO，可提高查询性能 如果我们建立了一个联合索引：name+phone select name,phone from user where name=?; 这就叫覆盖索引。他就不需要遍历到页子节点，直接就返回关键字数据了。这也就是很多公司不让用select* 的原因。 因为索引的过程是一个顺序的IO 全表扫描是一个随机IO。这一点就体现了索引的效率。 理解一下 索引列的数据长度能少则少。 长度决定磁盘块关键字多少，一次IO操作得到的数据量。 索引一定不是越多越好，越全越好，一定是建合适的。 因为索引在增删改的时候，数据库都会去维护索引。 匹配列前缀可用到索引 like 9999%，like %9999%、like %9999用不到索引； like 9999% 他是可能用到索引可能用不到索引，因为列离散性太差的话，数据量大的话。数据库觉得你索引用上与没用上效率差不多，他就不会用索引。其余两个最左匹配原则索引会失效。 Where 条件中 not in 和 \u003c\u003e操作无法使用索引； 因为在索引查找中B+Tree他是根据区间查找数据，如果是不等于：他是不知道去哪个区间的。 匹配范围值，order by 也可用到索引； MYISAM本身是具有排序功能的，还有就是索引是根据区间找数据，所以匹配范围值可以用到索引(单个索引)。 多用指定列查询，只返回自己想到的数据列，少用select *； 联合索引中如果不是按照索引最左列开始查找，无法使用索引； 最左匹配原则。 联合索引中精确匹配最左前列并范围匹配另外一列可以用到索引； 最左匹配原则。精确就直接指向那条路，然后再范围。 联合索引中如果查询中有某个列的范围查询，则其右边的所有列都无法使用索引； 因为他毕竟是个机器 (mysql的索引机制，第一步就是要确立，搜索的顺序) age\u003e18 他可以通过关键的比对确认走那边子树，但是通过age\u003e18 又加上name=张三这样的条件，他无法确认你要搜索的顺序。 索引列少计算 索引列不能参与计算，保持列“干净”。比如from_unixtime(create_time) = ’2017-05-29’就不能使用到索引，原因很简单，b+树中存的都是数据表中的字段值，但进行检索时，需要把所有元素都应用函数才能比较，显然成本太大。 为了解决索引列上计算引起的索引失效问题，将计算放到索引列外的表达式上,所以语句应该写成create_time = unix_timestamp(’2017-05-29’) 来一首打油诗 全值匹配我最爱,最左前缀要遵守； 带头大哥不能死,中间兄弟不能断； 索引列上少计算,范围之后全失效； LIKE百分写最右,覆盖索引不写星； 不等空值还有or,索引失效要少用； ","date":"2018-12-03","objectID":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%981%E7%90%86%E8%A7%A3%E5%BA%95%E5%B1%82b-tree%E6%9C%BA%E5%88%B6/:0:0","tags":["MySQL"],"title":"MySQL性能调优(1)理解底层B+tree机制","uri":"/mysql%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%981%E7%90%86%E8%A7%A3%E5%BA%95%E5%B1%82b-tree%E6%9C%BA%E5%88%B6/"},{"categories":["设计模式"],"content":"单例的好处 单例模式的初衷就是为了使资源能够共享，只需要赋值或者初始化一次，大家都能够重复利用。 一个类Class只有一个实例存在。 使用Singleton的好处还在于可以节省内存，因为它限制了实例的个数，有利于Java垃圾回收；常见的单例有枚举常量类、IOC容器、配置项等等。 普通单例模式-饿汉式 public class Singleton { private static Singleton singleton = new Singleton(); private Singleton() { } public static Singleton getInstance() { return singleton; } } 饿汉式单例，就是一个私有的构造方法加一个私有的静态当前类实例对象和一个公有的静态获取实例方法组成由于类实例对象为静态变量，所以在加载类的时候我们就会创建类的实例对象，这样的话比较消耗内存，浪费性能。 普通单例模式-懒汉式 public class Singleton { private static Singleton singleton; private Singleton() { } public static Singleton getInstance() { if (singleton == null) { singleton = new Singleton(); } return singleton; } } 懒汉式在饿汉式的基础上做了改进，加了判断，类实例对象做了懒加载，也就是所谓的延时加载，所以提升了一些性能。 多线程 关键词：volatile 被volatile修饰的变量能够保证每个线程能够获取该变量的最新值，从而避免出现数据脏读的现象。 因为在多线程情况下执行顺序与我们最初想的是不同的，导致数据可能不是最新的。 我们就强制每次都直接读内存，阻止重排序，确保voltile类型的值一旦被写入缓存必定会被立即更新到主存。 多线程 使用双重校验机制 public class Singleton { private static volatile Singleton singleton; private Singleton() { } public static Singleton getInstance() { // 先判断是否有线程实例对象已经创建。这样会让其他线程不会去等待获取锁 if (singleton == null) { // 让先进入线程的线程创建示例 synchronized (Singleton.class) { // 可能在同时进入时别的线程已经创建成功了 if (singleton == null) { singleton = new Singleton(); } } } return singleton; } } volatile 关键词在jdk1.5之后才完善的。 静态内部类实现 单例模式 （这种实现比较好） public class Singleton { private Singleton() { } private static class SingleTonBuilder { private static final Singleton singleton = new Singleton(); } public static Singleton getInstance() { return SingleTonBuilder.singleton; } } 主要原理为：Java中静态内部类可以访问其外部类的静态成员属性，同时，静态内部类只有当被调用的时候才开始首次被加载，(也就是这个时候占用内存)利用了classloader的机制来保证初始化instance时只有一个线程，所以也是线程安全的，同时没有性能损耗。（这里说的损耗是synchronized带来的损耗） 这个静态内部类的东西也可以写在类里，不过这里就变成线程安全的饿汉式了。 解决单例模式反序列化产生新对象的方法 只要在对象中加入readResolve方法 这样当JVM从内存中反序列化地\"组装\"一个新对象时,就会自动调用这个 readResolve方法来返回我们指定好的对象了, 单例规则也就得到了保证. private Object readResolve() { return INSTANCE; } ","date":"2018-11-14","objectID":"/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/:0:0","tags":["单例模式","volatile"],"title":"单例设计模式","uri":"/%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F/"},{"categories":["网络"],"content":"之前在看老齐的视频，做了一个百思不得姐爬虫的后台，效果还不错，准备过段时间买个服务器放上去。不过在这个项目中最重要的就是模拟请求、爬数据，让我了解到了okhttp这个工具贼好用，这里就不介绍了这个工具了，这里先分析原理、了解请求头和响应头。 请求头(Request Headers) Accept:浏览器能够处理的内容类型 Accept-Charset:浏览器能够显示的字符集 Accept-Encoding：浏览器能够处理的压缩编码 Accept-Language：浏览器当前设置的语言 Connection：浏览器与服务器之间连接的类型 Cookie：当前页面设置的任何Cookie Host：发出请求的页面所在的域 Referer：发出请求的页面的URL User-Agent：浏览器的用户代理字符串（当时我就是通过服务器增加这个请求信息模拟请求获取数据的） 响应头(Response Headers) Date：表示消息发送的时间，时间的描述格式由rfc822定义 server:服务器名字。 Connection：浏览器与服务器之间连接的类型 content-type:表示后面的文档属于什么MIME类型 Cache-Control：控制HTTP缓存 以上请求响应不能概全，因为每个请求响应都可能不相同，这只是一般请求会有的一些信息。 ","date":"2018-11-13","objectID":"/http%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E5%93%8D%E5%BA%94%E5%A4%B4%E9%83%A8%E5%8C%85%E6%8B%AC%E7%9A%84%E4%BF%A1%E6%81%AF%E4%B8%80%E8%88%AC%E6%9C%89%E5%93%AA%E4%BA%9B/:0:0","tags":["HTTP"],"title":"HTTP请求头和响应头部包括的信息一般有哪些","uri":"/http%E8%AF%B7%E6%B1%82%E5%A4%B4%E5%92%8C%E5%93%8D%E5%BA%94%E5%A4%B4%E9%83%A8%E5%8C%85%E6%8B%AC%E7%9A%84%E4%BF%A1%E6%81%AF%E4%B8%80%E8%88%AC%E6%9C%89%E5%93%AA%E4%BA%9B/"},{"categories":["面试"],"content":"Java努力让一切都变为对象。但是为了方便还是保留了基本类型 基本类型 包装器类型 boolean Boolean char Character int Integer byte Byte long Long short Short float Float double Double 实际在开发中的用处 我们知道Java是一个面相对象的编程语言，基本类型并不具有对象的性质，为了让基本类型也具有对象的特征，就出现了包装类型（如我们在使用集合时就一定要使用包装类型而非基本类型），它相当于将基本类型“包装起来”，使得它具有了对象的性质，并且为其添加了属性和方法，丰富了基本类型的操作。比如list 他的类型就必须写包装类型。 以int和Integer为例。 int不存在null值，一经初始化，就被赋予默认值0. 但Integer是存在null值的，只做初始化而不赋值，那它就是个null。当你需要用一个值来表示无意义或者非法数据时，那就得考虑一下用哪个了。当我要表示一次数据库更新操作影响的数据行数，那用int rows=-1;就可以表示操作异常，因为不可能更新了-1行数据。但如果要表示一个整数加法的结果时，就只能用Integer sum =null;了，因为任意一个整数都可能是有意义的结果，因而不能用来表示异常情况。 为什么基本类型都还存在呢 我们都知道在Java语言中，new一个对象存储在堆里，我们通过栈中的引用来使用这些对象；但是对于经常用到的一系列类型如int，如果我们用new将其存储在堆里就不是很有效——特别是简单的小的变量。所以就出现了基本类型，对于这些类型不是用new关键字来创建，而是直接将变量的值存储在栈中，因此更加高效。 ","date":"2018-11-13","objectID":"/java%E4%B8%AD%E6%9C%89%E4%BA%86%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E6%9C%89%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/:0:0","tags":["基本类型","包装类型"],"title":"Java中有了基本类型为什么还要有包装类型","uri":"/java%E4%B8%AD%E6%9C%89%E4%BA%86%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B%E4%B8%BA%E4%BB%80%E4%B9%88%E8%BF%98%E8%A6%81%E6%9C%89%E5%8C%85%E8%A3%85%E7%B1%BB%E5%9E%8B/"},{"categories":["Spring"],"content":"spring有5种隔离机制设置 其中数据库有四种隔离级别，而spring 多的那种就是默认设置数据库隔离设置，下面主要讲讲这些隔离机制的区别 数据库的四种隔离级别 读未提交(Read Uncommitted)：允许脏读取，但不允许更新丢失。如果一个事务已经开始写数据，则另外一个数据则不允许同时进行写操作，但允许其他事务读此行数据。 读已提交(Read Committed)：允许不可重复读取，但不允许脏读取。读取数据的事务允许其他事务继续访问该行数据，但是未提交的写事务将会禁止其他事务访问该行。 可重复读(Repeatable Read)：禁止不可重复读取和脏读取，但是有时可能出现幻影数据。读取数据的事务将会禁止写事务(但允许读事务)，写事务则禁止任何其他事务 序列化(Serializable)：提供严格的事务隔离。它要求事务序列化执行，事务只能一个接着一个地执行，但不能并发执行。如果仅仅通过“行级锁”是无法实现事务序列化的，必须通过其他机制保证新插入的数据不会被刚执行查询操作的事务访问到。 默认Isolation.DEFAULT 使用后端数据库默认的隔离级别，MySQL默认采用的 REPEATABLE_READ隔离级别 Oracle 默认采用的 READ_COMMITTED隔离级别。 Isolation.READ_UNCOMMITTED 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读 Isolation.READ_COMMITTED 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生 Isolation.REPEATABLE_READ 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。 Isolation.SERIALIZABLE 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。 一般项目中会采用默认的保证数据的4大特性（原子性、一致性、隔离性、持久性），在一定的业务情况下根据需求设置事务来保证数据的正确。 ","date":"2018-11-13","objectID":"/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/:0:0","tags":["事务"],"title":"Spring事务的隔离级别","uri":"/%E4%BA%8B%E5%8A%A1%E7%9A%84%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%AB/"},{"categories":["面试"],"content":"之前面试被问懵了一个很简单的问题，switch中的参数支持string吗，我当时想都没想，直接回答string可以为switch的参数，因为在我写的代码中我的确拿string当过参数：然而面试官说不可以。 (经过我的验证，我发现jdk1.7以后switch就支持String类型了；面试官傻逼！) 官方示例 https://docs.oracle.com/javase/tutorial/java/nutsandbolts/switch.html 语法格式如下： switch(expression){ case value : //语句 break; //可选 case value : //语句 break; //可选 //你可以有任意数量的case语句 default : //可选 //语句 } 这里的 expression 都支持哪些类型呢？ 基本数据类型：byte, short, char, int 包装数据类型：Byte, Short, Character, Integer 枚举类型：Enum 字符串类型：String（Jdk 7+ 开始支持） 基本数据类型和字符串很简单不用说，下面举一个使用包装类型和枚举的，其实也不难，注意只能用在 switch 块里面。 // 使用包装类型 Integer value = 5; switch (value) { case 3: System.out.println(\"3\"); break; case 5: System.out.println(\"5\"); break; default: System.out.println(\"default\"); } // 使用枚举类型 Status status = Status.PROCESSING; // 也可以直接把枚举类放进去（官方文档有讲） switch (status) { case OPEN: System.out.println(\"open\"); break; case PROCESSING: System.out.println(\"processing\"); break; case CLOSE: System.out.println(\"close\"); break; default: System.out.println(\"default\"); } 使用 switch case 语句也有以下几点需要注意。 case 里面必须跟 break，不然程序会一个个 case 执行下去，直到最后一个 break 的 case 或者 default 出现。 case 条件里面只能是常量或者字面常量。 default 语句可有可无，最多只能有一个。 ","date":"2018-11-13","objectID":"/switch%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E5%8F%AF%E4%BB%A5%E4%B8%BA%E9%82%A3%E4%BA%9B%E7%B1%BB%E5%9E%8B/:0:0","tags":["Switch","String"],"title":"Switch中的参数可以为那些类型","uri":"/switch%E4%B8%AD%E7%9A%84%E5%8F%82%E6%95%B0%E5%8F%AF%E4%BB%A5%E4%B8%BA%E9%82%A3%E4%BA%9B%E7%B1%BB%E5%9E%8B/"},{"categories":["网络"],"content":"今天写公司项目的小demo的时候发现自己一个理解很不对的地方，用户的session 是否重复(在我之前的理解就是给的key 是一样的就会重复，或者覆盖)：后台经过测试才发现，我共享session的时候给的key是一样的为什么获取的对象还是不一样，明明都是在内存当中（经过深究发现并不在内存中），为什么会存在这种情况 ","date":"2018-11-12","objectID":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/:0:0","tags":["Session","Cookie"],"title":"Session与Cookie的区别","uri":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["网络"],"content":"Cookie机制 cookie的内容主要包括：名字，值，过期时间，路径和域。路径与域一起构成cookie的作用范围。若不设置过期时间，则表示这个cookie的生命期为浏览器会话期间，关闭浏览器窗口，cookie就消失。这种生命期为浏览器会话期的cookie被称为会话cookie。会话cookie一般不存储在硬盘上而是保存在内存里，当然这种行为并不是规范规定的。若设置了过期时间，浏览器就会把cookie保存到硬盘上，关闭后再次打开浏览器，这些cookie仍然有效直到超过设定的过期时间。存储在硬盘上的cookie可以在不同的浏览器进程间共享，比如两个IE窗口。而对于保存在内存里的cookie，不同的浏览器有不同的处理方式。 而session机制采用的是一种在服务器端保持状态的解决方案。同时我们也看到，由于采用服务器端保持状态的方案在客户端也需要保存一个标识，所以session机制可能需要借助于cookie机制来达到保存标识的目的。而session提供了方便管理全局变量的方式 。 session是针对每一个用户的，变量的值保存在服务器上，用一个sessionID来区分是哪个用户session变量,这个值是通过用户的浏览器在访问的时候返回给服务器，当客户禁用cookie时，这个值也可能设置为由get来返回给服务器。 Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头 ","date":"2018-11-12","objectID":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/:1:0","tags":["Session","Cookie"],"title":"Session与Cookie的区别","uri":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["网络"],"content":"Session机制 当程序需要为某个客户端的请求创建一个session时，服务器首先检查这个客户端的请求里是否已包含了一个session标识（称为session id），如果已包含则说明以前已经为此客户端创建过session，服务器就按照session id把这个session检索出来使用（检索不到，会新建一个），如果客户端请求不包含session id，则为此客户端创建一个session并且生成一个与此session相关联的session id，session id的值应该是一个既不会重复，又不容易被找到规律以仿造的字符串，这个session id将被在本次响应中返回给客户端保存(这个取决于项目)。 ","date":"2018-11-12","objectID":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/:2:0","tags":["Session","Cookie"],"title":"Session与Cookie的区别","uri":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["网络"],"content":"所以服务端如何识别特定的客户 这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。（这里就是我之前想错的地方） ","date":"2018-11-12","objectID":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/:3:0","tags":["Session","Cookie"],"title":"Session与Cookie的区别","uri":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["网络"],"content":"总结 Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中； Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。 session 在服务器端，cookie 在客户端（浏览器） session 默认被存在在服务器的一个文件里（不是内存） session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说，如果浏览器禁用了 cookie ，同时 session 也会失效（但是可以通过其它方式实现，比如在 url 中传递 session_id） 用户验证这种场合一般会用 session ","date":"2018-11-12","objectID":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/:4:0","tags":["Session","Cookie"],"title":"Session与Cookie的区别","uri":"/session%E4%B8%8Ecookie%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":["翻墙"],"content":"shadowsocks 文档总结自秋水逸冰大神 更详细可以看 https://teddysun.com/(需要外网访问) 首先，买服务器我就不说了，一般只翻墙的话就买搬瓦工或者vultr都行；里面最便宜的都行（除了vultr2.5刀的）；找一个ip如果ping 的延时在200以内算是挺可以的了； 这里我采用Vultr3.5刀的；选哪个地方主要看你的网络对他们哪个服务器友好,下面附上测速脚本。 @echo off echo =========================================== echo 东京 ping hnd-jp-ping.vultr.com echo ============================================ echo 新加坡 ping sgp-ping.vultr.com echo =========================================== echo (AU) Sydney, Australia[悉尼] ping syd-au-ping.vultr.com echo =========================================== echo 德国 法兰克福 ping fra-de-ping.vultr.com echo =========================================== echo 荷兰 阿姆斯特丹 ping ams-nl-ping.vultr.com echo =========================================== echo 英国 伦敦 ping lon-gb-ping.vultr.com echo =========================================== echo 法国 巴黎 ping par-fr-ping.vultr.com echo =========================================== echo 美东 华盛顿州 西雅图 ping wa-us-ping.vultr.com echo =========================================== echo 美西 加州 硅谷 ping sjo-ca-us-ping.vultr.com echo =========================================== echo 美西 加州 洛杉矶 ping lax-ca-us-ping.vultr.com echo =========================================== echo 美东 芝加哥 Chicago, Illinois[美东 芝加哥] ping il-us-ping.vultr.com echo =========================================== echo 美中 德克萨斯州 达拉斯 ping tx-us-ping.vultr.com echo =========================================== echo 美东 新泽西 ping nj-us-ping.vultr.com echo =========================================== echo 美东 乔治亚州 亚特兰大 ping ga-us-ping.vultr.com echo =========================================== echo 美东 佛罗里达州 迈阿密 ping fl-us-ping.vultr.com pause 服务器信息：用于Xshell连接服务器 服务器连接 使用一键安装ss 脚本 wget --no-check-certificate -O shadowsocks-all.sh https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh chmod +x shadowsocks-all.sh ./shadowsocks-all.sh 2\u003e\u00261 | tee shadowsocks-all.log 安装成功图 6. 用ss软件连接 https://github.com/shadowsocks https://github.com/shadowsocksrr 打开软件填入数据 代理即可 这里要注意全局代理是把本地出入ip完全代理你设置的ip；PAC模式是访问国内用本地ip访问外网会自动代理； 配置加速 使用root用户登录，运行以下命令： wget --no-check-certificate https://github.com/teddysun/across/raw/master/bbr.sh \u0026\u0026 chmod +x bbr.sh \u0026\u0026 ./bbr.sh 安装完成后，脚本会提示需要重启 VPS，输入 y 并回车后重启。 重启完成后，进入 VPS，验证一下是否成功安装最新内核并开启 TCP BBR，输入以下命令： uname -r 查看内核版本，显示为最新版就表示 OK 了 sysctl net.ipv4.tcp_available_congestion_control 返回值一般为： net.ipv4.tcp_available_congestion_control = bbr cubic reno 或者为： net.ipv4.tcp_available_congestion_control = reno cubic bbr sysctl net.ipv4.tcp_congestion_control 返回值一般为： net.ipv4.tcp_congestion_control = bbr sysctl net.core.default_qdisc 返回值一般为： net.core.default_qdisc = fq lsmod | grep bbr 返回值有 tcp_bbr 模块即说明 bbr 已启动。注意：并不是所有的 VPS 都会有此返回值，若没有也属正常。 配置完后，基本上洛杉矶硅谷东京这几个地方的ip跑7m左右秒没问题。 ","date":"2018-11-05","objectID":"/%E6%90%AD%E5%BB%BA%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91/:1:0","tags":["shadowsocks","V2Ray"],"title":"搭建代理访问外网","uri":"/%E6%90%AD%E5%BB%BA%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91/"},{"categories":["翻墙"],"content":"V2Ray 目前我用的方式 bash \u003c(curl -s -L https://git.io/v2ray.sh) 一键脚本 bash \u003c(curl -Ls https://blog.sprov.xyz/v2-ui.sh) 带有控制台的一键脚本 ","date":"2018-11-05","objectID":"/%E6%90%AD%E5%BB%BA%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91/:2:0","tags":["shadowsocks","V2Ray"],"title":"搭建代理访问外网","uri":"/%E6%90%AD%E5%BB%BA%E4%BB%A3%E7%90%86%E8%AE%BF%E9%97%AE%E5%A4%96%E7%BD%91/"},{"categories":["工具"],"content":" 参考注册机 https://www.lanzous.com/i9o3eej 原文：https://www.jianshu.com/p/aa3c00b87bec 下载Navicat注册机然后自行下载最新版navicat 确保已关闭Navicat Premium。无需断网，无需将注册机放到Navicat Premium安装目录下。以管理员身份运行此注册机； 打开注册机后， Patch勾选Backup、Host和Navicat v12，然后点击Patch按钮： 找到Navicat Premium 12安装路径下的navicat.exe，选中并点击打开： 此时出现如下弹窗，提示navicat.exe - x64 -\u003e Cracked.，提示已破解（别高兴，还没结束）。若提示libcc.dll或navicat.exe出错，检查是否未关闭Navicat Premium，或到安装目录下将libcc.dll和navicat.exe删除，并将libcc.dll.BAK或navicat.exe.BAK去掉.BAK后缀名。否则卸载已安装的Navicat Premium并清理文件残留和注册表残留： License, Product and Language确保License为Enterprise，Products为Premium，Languages为Simplified Chinese（简体中文，其它语言版本请自选）； Resale License确保Resale Version为Site license； Keygen / Offline Activation中Your Name和Your Organization可以任意填写或者默认，然后点击Generate，将自动生成Serial Keygen（即注册码）： 打开Navicat Premium 12，点击菜单栏的帮助，选择注册，在注册窗口键处填入上一步生成的Serial Keygen（即注册码），然后点击激活： 点击手动激活： 将Navicat手动激活窗口的请求码框中内容复制到注册机Request Code框中，点击Activation Code下面的Generate按钮： 将注册机Activation Code处生成的激活码内容复制到Navicat手动激活窗口的激活码框中（或点击Activation Code处下面的Copy按钮），然后点击激活按钮： 提示Navicat现已激活： ok! ","date":"2018-11-05","objectID":"/%E7%A0%B4%E8%A7%A3%E6%9C%80%E6%96%B0%E7%89%88navicat/:0:0","tags":["Navicat","MySQL"],"title":"破解最新版navicat","uri":"/%E7%A0%B4%E8%A7%A3%E6%9C%80%E6%96%B0%E7%89%88navicat/"},{"categories":["云原生"],"content":"因为在10月15号换了新工作，入职了半月左右，这半月时间一直在搞前端的ts和angular，再加上在忙着自考，后端的代码一直没有碰，正好这周在家时间就圆一下我的好奇心去搞一搞docker，顺便把我ss的服务器利用一下做一个MySQL在上面，后期会上点项目上去； 以后服务器就安装一个Docker就行了 本次采用的是Centos7.5版本的Linux系统，镜像是阿里云下载的DVD版本；（因为Docker需要内核3版本以上的Linux）； 执行一键安装脚本 curl -sSL https://get.docker.com/ | sh //这是官方的 curl -sSL http://acs-public-mirror.oss-cn-hangzhou.aliyuncs.com/docker-engine/internet | sh - // 这是阿里云的 curl -sSL https://get.daocloud.io/docker | sh // 这是DaoCloud的 （实测这个最快） 或者采用离线安装指定版本:下载rpm安装包 https://download.docker.com/linux/centos/7/x86_64/stable/Packages/ 然后把安装包上传至服务器执行yum install 安装包文件地址 卸载干净docker yum remove -y docker \\ docker-client \\ docker-client-latest \\ docker-common \\ docker-latest \\ docker-latest-logrotate \\ docker-logrotate \\ docker-selinux \\ docker-engine-selinux \\ docker-engine 官网方法安装 # 删除所有旧的数据 sudo rm -rf /var/lib/docker # 安装依赖包 sudo yum install -y yum-utils \\ device-mapper-persistent-data \\ lvm2 # 添加源，使用了阿里云镜像 sudo yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo # 配置缓存 sudo yum makecache fast # 安装最新稳定版本的docker sudo yum install -y docker-ce # 配置镜像加速器 阿里云自己的 # 启动docker引擎并设置开机启动 sudo systemctl start docker sudo systemctl enable docker 配置镜像加速器，不然下载速度会很慢;(采用阿里云提供的) 个人自定义地址 设置自启动 由于需要MySQL跟随linux自启动，MySql是依赖于docker启动，所以这里先设置docker自启动 service docker start //启动docker（新安装默认启动了） chkconfig docker on //设置自动启动 把MySQL的docker镜像下载到本地（这里就可以看到加速地址的作用了） docker pull mysql //下载的最新版MySQL docker pull mysql：版本号 // 运行一个MySQL镜像 并设置自动启动 docker run --name 运行名称 -p 3306:3306 -v /usr/local/mysql/conf:/etc/mysql/conf.d -v /usr/local/mysql/log:/var/log/mysql -v /usr/local/mysql/data:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=密码 -d --restart=always mysql -d 是后台运行 --restart=always 是docker官方的给的自启动方案 -v （导入外部文件到MySQL启动比如配置文件）/usr/local/mysql/conf/my.cnf:/etc/my.cnf -e 映射MySQL 文件到本地 这个时候差不多就可以测试是否可以连接成功了 如果出现这样，下载最新版数据库连接工具 Navicat连接即可，我们就不去修改加密方式了；东西总得用新的嘛。 Navicat最新破解教程 如果遇到权限问题 docker exec -it mysql bash // 进入MySQL docker mysql -u root -p // 这个之后会叫你输入密码 mysql\u003e SET GLOBAL innodb_fast_shutdown = 1;// 在MySQL 里面输入这个；然后ctrl+D 退出来 mysql_upgrade -u root -p 执行自动更新权限 ","date":"2018-11-05","objectID":"/%E5%AD%A6%E4%B9%A0docker%E5%B9%B6%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AA%E8%87%AA%E5%90%AF%E5%8A%A8mysql8/:0:0","tags":["docker","MySQL"],"title":"自启动docker并安装自启动镜像MySQL8","uri":"/%E5%AD%A6%E4%B9%A0docker%E5%B9%B6%E5%AE%89%E8%A3%85%E4%B8%80%E4%B8%AA%E8%87%AA%E5%90%AF%E5%8A%A8mysql8/"},{"categories":["面试"],"content":"序列化的作用 序列化 有的时候我们想要把一个Java对象变成字节流的形式以便存储在文件或者网络传输：比如序列化成json格式通过http发送出去。 反序列化 有的时候我们想要从一个字节流中恢复一个Java对象。：比如在网络中接受到一个json格式字符串，把它转化为某个对象。 序列化的什么特点： 如果某个类能够被序列化，其子类也可以被序列化。声明为static和transient类型的成员数据不能被序列化。因为static代表类的状态， transient代表对象的临时数据。 序列化的方式 是相应的对象实现了序列化接口Serializable，这个使用的比较多，对于序列化接口Serializable接口是一个空的接口，它的主要作用就是标识这个对象时可序列化的。 package com.shop.domain; import java.util.Date; public class Article implements java.io.Serializable { private Integer id; public Integer getId() { return id; } public void setId(Integer id) { this.id = id; } } 实现接口Externalizable 由于这个类实现了Externalizable 接口，在writeExternal()方法里定义了哪些属性可以序列化， 序列化的方式有很多，这上面的是最基本的序列化，我们在项目中常用的一般是json转换对象，对象转换json也算是序列化。 ","date":"2018-10-09","objectID":"/%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E4%BD%9C%E7%94%A8/:0:0","tags":["序列化"],"title":"序列化的作用","uri":"/%E5%BA%8F%E5%88%97%E5%8C%96%E7%9A%84%E4%BD%9C%E7%94%A8/"},{"categories":["面试"],"content":"目前我接触的多线程编程基本上都是基于java.util.concurrent 这个包下的开发的，下面就以这个包来分析Java的多线程与高并发 分析面试题 在java中wait和sleep方法的不同？ 最大的不同是在等待时wait会释放锁，而sleep一直持有锁。Wait通常被用于线程间交互，sleep通常被用于暂停执行。 创建多线程的三种方法 1.继承Thread() 2.实现Runnable()接口 3.实现Callable接口 继承Thread与实现Runnable区别 类可能只要求可执行即可,因此继承整个Thread类的开销过大 Runnable和Callable的区别 Runnable接口中的run()方法的返回值是void;而Callable接口中的call()方法是有返回值的,和Future/FutureTask配合可以用来获取异步执行的结果。 notify作用 1.notify：唤醒一个正在wait当前对象锁的线程，并让它拿到对象锁 2.notifyAll：唤醒所有正在wait前对象锁的线程 3.在调用wait，notify，notifyall的时候当前线程必须获得这个对象的锁。 线程的5种状态 你有哪些多线程开发良好的实践 1.考虑使用线程池 2.优先使用volatile 保证可见性 3.最小化同步范围 4.给线程命名 来认识认识java5以后提供多线程的东西： ExecutorService cachedThreadPool = Executors.线程池方法 Java通过Executors提供四种线程池: newCachedThreadPool创建一个可缓存线程池，需要就增加，不需要就减少。 newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。 newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。 newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。(这个一般不用:一般框架就会有定时器) 同步工具 Semaphone(信号量)可以将任何一种容器变为有界阻塞容器：比如服务器登录人数过多排队等待。 CyclicBarrier(循环屏障)他可以做出让几个线程都执行到某个地方之后，才让几个线程同时继续执行。 CountDownLatch(倒计时锁)这个类能够使一个线程等待其他线程完成各自的工作后再执行。 ReentrantLock(重入锁)把锁机制分为读锁、写锁；让锁更灵活 Condition用于让指定线程等待与唤醒，按预期顺序执行，他必须和ReentrantLock重入锁配合使用。 Callable配合Future/FutureTask获取线程信息。 原子对象 AtomicInteger、AtomicLong、AtomicBoolean（Atomic则通过 CAS（乐观锁）实现自动同步） 主要是i++不是原子操作，非线程安全的，多线程访问的时候需要用到synchronized关键字保持线程同步。synchronized是悲观锁，在多线程竞争下，加锁、释放锁会导致比较多的上下文切换和调度延时，代价就是效率低下 并发容器 CopyOnWriteArrayList （写复制列表） CopyOnWriteArraySet （写复制集合） 底层实现主要是操作时复制了一份出来，底层采用的是重入锁解决并发更改问题。 ConcurrentHashMap （分段锁映射） hashtable锁住的是一整张hash表而ConcurrentHashMap底层默认分段分成16份分别锁住，利用乐观锁来操作效率问题。 ConcurrentSkipListMap、ConcurrentSkipListSet、ConcurrentLinkedQueue、这三个都是有序的支持并发的。 具体可以看Jdk文档 ","date":"2018-10-08","objectID":"/%E8%B0%88%E8%B0%88java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E9%AB%98%E5%B9%B6%E5%8F%91/:0:0","tags":["多线程","高并发"],"title":"谈谈Java多线程与高并发","uri":"/%E8%B0%88%E8%B0%88java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E4%B8%8E%E9%AB%98%E5%B9%B6%E5%8F%91/"},{"categories":["面试"],"content":"一个经典的类型转换问题 高位转低位需要强制转换，低位转高位是自动转换，表达式两侧的转换规则是向左边的类型看齐 short s1=1;s1=s1+1; s1+1值是自动转换int类型，而是s1是short类型 所以会提示你转为short类型，s1= (short) (s1+1)就正确了。 short s1=1;s1=+=1; 这个为什么不报错 因为java语言规范中关于复合赋值的解释是这样的：e1+=e2实际上是e1=(T1)(e1+e2),这里的T1的数据类型。所以不会报错。 short s1=1,s2=1;short s3=s1+s2 这个为什么会报错 首先java编译时会出于安全考虑：因为如果s1的值接近short类型取值范围的最大值，同时s2的值也接近short类型取值范围的最大值，那么s1+s2的肯定超出了short的取值范围，此时二者之和就是int型的数据，此时就需要强制把左边的int型数据转换为右边的short型。那么这里明明声明了short类型为什么还会错，因为此时还不知道s1和s2的值到底是多少，处于安全考虑会让强转高位转低位 short s3=(short)(s1+s2) short s1+=1 java没有这种写法 在声明变量时运算 以上的分析都对byte、short或char类型有效；因为这些类型在运算时都会自动扩展，因为在栈中不会存在这些类型的数据都会被转换为int类型。 ","date":"2018-10-07","objectID":"/%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%BD%AC%E6%8D%A2%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98/:0:0","tags":["Short"],"title":"关于类型转换与运算问题","uri":"/%E7%B1%BB%E5%9E%8B%E7%9A%84%E8%BD%AC%E6%8D%A2%E5%9F%BA%E6%9C%AC%E9%97%AE%E9%A2%98/"},{"categories":["面试"],"content":"Java 反射是可以让我们在运行时获取类的方法、属性、父类、接口等类的内部信息的机制 为什么用反射 假如你写了一段代码：Object o=new Object(); 运行了起来！扔给jvm去跑，跑完就over了，jvm关闭，你的程序也停止了; 想想上面的程序对象是自己new的，程序相当于写死了给jvm去跑。假如一个服务器上突然遇到某个请求哦要用到某个类，哎呀但没加载进jvm，是不是要停下来自己写段代码，new一下，哦启动一下服务器，（脑残）！ 反射是什么呢？当我们的程序在运行时，需要动态的加载一些类这些类可能之前用不到所以不用加载到jvm，而是在运行时根据需要才加载，这样的好处对于服务器来说不言而喻。 反射是框架的灵魂 怎么创建 但是这个怎么实现动态的呢，这里的forName方法里面的字符串就是动态的。不同的类我们用不同的字符串 这样就可以在运行时加载类来用了。 Object object = c.newInstance(); // 所有共有方法，这就包括自身的所有public方法，和从基类继承的、从接口实现的所有public方法 Method[] methods = c.getMethods(); // 自身声明的所有方法，包含public、protected和private方法 Method[] declaredMethods = c.getDeclaredMethods(); //获取methodClass类的add方法 Method method = c.getMethod(\"add\",int.class,int.class); Object result = method.invoke(obj,1,4); 反射调用一般分为3个步骤： 得到要调用类的class 得到要调用的类中的方法(Method) 方法调用(invoke) ","date":"2018-10-07","objectID":"/%E7%90%86%E8%A7%A3java%E5%8F%8D%E5%B0%84/:0:0","tags":["反射"],"title":"理解java反射","uri":"/%E7%90%86%E8%A7%A3java%E5%8F%8D%E5%B0%84/"},{"categories":["面试"],"content":"JVM的分区可分为三个：堆（heap）、栈（stack）和方法区（method）：堆主要用来存放对象的，栈主要是用来执行程序的 堆区 存储的是对象，并且JVM只有一个堆区，被所有线程共享。 比如new string等等对象 由于要在运行时动态分配内存，存取速度较慢 先进先出 栈区 每个线程包含自己的一个栈区，栈中只保存基本数据类型的对象和自定义对象的引用记住是引用地址，指向堆中的对象 栈与栈之间不能直接访问 存取速度快，但是生命周期是固定的区域。 先进后出 栈有一个很重要的特殊性，就是存在栈中的数据可以共享。假设我们同时定义： int a = 3; int b = 3； 编译器先处理int a = 3；首先它会在栈中创建一个变量为a的引用，然后查找栈中是否有3这个值，如果没找到，就将3存放进来，然后将a指向3。接着处理int b = 3；在创建完b的引用变量后，因为在栈中已经有3这个值，便将b直接指向3。这样，就出现了a与b同时均指向3的情况。 方法区 又称为‘静态区’，和堆一样，被所有的线程共享。 分析 String str1 = \"abc\"; String str2 = \"abc\"; System.out.println(str1==str2); //true String str1 =new String (\"abc\"); String str2 =new String (\"abc\"); System.out.println(str1==str2); // false 两种的形式来创建，第一种是用new()来新建对象的，它会在存放于堆中。每调用一次就会创建一个新的对象。 这样就清晰了，new 会是新的对象地址：==比较的是内存地址 另一种就是先在栈中创建一个对String类的对象引用变量str，然后查找栈中有没有存放\"abc\"，如果没有，则将\"abc\"存放进栈，并令str指向”abc”，如果已经有”abc” 则直接令str指向“abc”。 String s1 = \"ja\"; String s2 = \"va\"; String s3 = \"java\"; String s4 = s1 + s2; System.out.println(s3 == s4);//false System.out.println(s3.equals(s4));// 比较类里面的数值是否相等时，用equals()方法；s4最后指向的是新的一个java字符串地址。 String s3 = “ja”+“va\"在编译期就已经变成了String s3 = “java” String s4 = s1 + s2 在编译期无法确定 所以在运行的时候动态分配新的地址给他。 如果s1与s2是final修饰，那么\"ja”+“va\"跟s1 + s2 是没区别的，因为final修饰的变量会解析为常量值。 ","date":"2018-10-05","objectID":"/%E8%B0%88%E8%B0%88%E5%AF%B9jvm%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/:0:0","tags":["堆","栈","JVM"],"title":"谈谈对JVM内存分配","uri":"/%E8%B0%88%E8%B0%88%E5%AF%B9jvm%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D/"},{"categories":["面试"],"content":"总的来说，Hibernate使用的是封装好的，通用的SQL来应付所有场景，而Mybatis是针对响应的场景设计的SQL。Mybatis的SQL会更灵活、可控性更好、更优化 Hibernate的查询会将表中的所有字段查询出来，这一点会有性能消耗。 Hibernate也可以自己写SQL来指定需要查询的字段，但这样就破坏了Hibernate开发的简洁性。 Mybatis的SQL是手动编写的，所以可以按需求指定查询的字段。 hibernate迁移性相对mybatis较好，因为hibernate不是依赖数据库。 hibernate 一级缓存与二级缓存 减少对数据库的访问次数！从而提升hibernate的执行效率！ 一级缓存又称为“Session的缓存”，它是内置的，不能被卸载（不能被卸载的意思就是这种缓存不具有可选性，必须有的功能，不可以取消session缓存） 二级缓存又称为\"SessionFactory的缓存\"，由于SessionFactory对象的生命周期和应用程序的整个过程对应，因此Hibernate二级缓存是进程范围或者集群范围的缓存。 很少被修改的数据与不会被并发的数据放入二级缓存中 访问顺序 一级缓存-\u003e二级缓存-\u003e数据库 具体方法功能 session.flush(); 让一级缓存与数据库同步 - session.evict(arg0);清空一级缓存中指定的对象 - session.clear();清空一级缓存中缓存的所有对象 Hibernate缓存的是对象 Hibernate List与iterator查询的区别 list()一次把所有的记录都查询出来，会放入缓存，但不会从缓存中获取数据 Iterator N+1查询； N表示所有的记录总数，即会先发送一条语句查询所有记录的主键（1），再根据每一个主键再去数据库查询（N）！会放入缓存，也会从缓存中取数据 Mybatis缓存 访问顺序 二级缓存-\u003e一级缓存-\u003e数据库 一级缓存是SqlSession级别的缓存。在操作数据库时需要构造 sqlSession对象，在对象中有一个(内存区域)数据结构（HashMap）用于存储缓存数据 缓存作用域在用一个session之间 二级缓存相对于一级缓存来说，实现了SqlSession之间缓存数据的共享，同时粒度更加的细，能够到namespace级别，通过Cache接口实现类不同的组合，对Cache的可控性也更强。 二级缓存是mapper级别的缓存，多个SqlSession去操作同一个Mapper的sql语句，多个SqlSession去操作数据库得到数据会存在二级缓存区域，多个SqlSession可以共用二级缓存，二级缓存是跨SqlSession的 MyBatis在多表查询时，极大可能会出现脏数据，有设计上的缺陷，安全使用二级缓存的条件比较苛刻。 在分布式环境下，由于默认的MyBatis Cache实现都是基于本地的，分布式环境下必然会出现读取到脏数据，需要使用集中式缓存将MyBatis的Cache接口实现，有一定的开发成本，直接使用Redis,Memcached等分布式缓存可能成本更低，安全性也更高。 一般情况下，增删改后都会commit，这样要避免读取脏数据。也可以在具体mapper方法里加入flushCache=“true”。 缓存相对比较 两者比较：因为Hibernate对查询对象有着良好的管理机制，用户无需关心SQL。所以在使用二级缓存时如果出现脏数据，系统会报出错误并提示。 而MyBatis在这一方面，使用二级缓存时需要特别小心。如果不能完全确定数据更新操作的波及范围，避免Cache的盲目使用。否则，脏数据的出现会给系统的正常运行带来很大的隐患。 ","date":"2018-10-03","objectID":"/mybatis%E4%B8%8Ehibernate/:0:0","tags":["Mybatis","Hibernate"],"title":"Mybatis与hibernate","uri":"/mybatis%E4%B8%8Ehibernate/"},{"categories":["面试"],"content":"事实上，Servlet就是一个Java接口：具体可以看这里： Servlet是干嘛的 Servlet接口定义的是一套处理网络请求的规范，所有实现Servlet的类，都需要实现它那五个方法，其中最主要的是两个生命周期方法 init()和destroy()，还有一个处理请求的service()，也就是说，所有实现servlet接口的类，或者说，所有想要处理网络请求的类，都需要回答这三个问题： 你初始化时要做什么 你销毁时要做什么 你接受到请求时要做什么 Servlet只是定义了一套规范（接口即是规范），处理请求还是需要一个容器，不然Servlet不会起作用，比如Tomcat 他监听了端口，根据url确定交给那个Servlet处理，然后才有了Servlet的操作。 Servlet生命周期 发送请求 解析请求 加载和实例化init（）方法 第一次创建Servlet调用在后续每次请求这个Servlet都不会调用。 响应客户端请求调用Service（）方法具体执行doGet()或者doPost()方法 输出响应信息 返回响应信息 终止阶段，调用destroy()方法 destroy() 方法只会被调用一次，在 Servlet 生命周期结束时被调用。destroy() 方法可以让您的 Servlet 关闭数据库连接、停止后台线程、等等清理操作。 最后，Servlet 是由 JVM 的垃圾回收器进行垃圾回收的 请求转发（Forward）和重定向（sendredirect）的区别 Forward是在服务器端的跳转，就是客户端一个请求发给服务器，服务器直接将请求相关的参数的信息原封不动的传递到该服务器的其他jsp或Servlet去处理。 sendredirect是在客户端的跳转，服务器会返回给客户端一个响应报头和新的URL地址，原来的参数什么的信息如果服务器端没有特别处理就不存在了，浏览器会访问新的URL所指向的Servlet或jsp：重定向可以防止表单重复提交。 转发的是同一次请求；重定向是两次不同请求 转发地址栏没有变化；重定向地址栏有变化 讲讲filter listener 与Servlet 执行顺序 listener(监听器) -\u003e filter（过滤器） -\u003e Servlet Servlet 当客户端向服务器发出HTTP请求时，首先会由服务器中的 Web 容器（如Tomcat）对请求进行路由，交给该URL对应的 Servlet 进行处理，Servlet 所要做的事情就是返回适当的内容给用户 filter Filter 是介于 Web 容器和 Servlet 之间的过滤器，用于过滤未到达 Servlet 的请求或者由 Servlet 生成但还未返回响应。 listener 基于观察者模式：Listener 用于监听 java web程序中的事件，例如创建、修改、删除Session、request、context等，并触发响应的事件。比如显示在线人数；单态登录：一个账号只能在一台机器上登录；等等全局的操作。 ","date":"2018-10-03","objectID":"/servlet%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/:0:0","tags":["Servlet"],"title":"Servlet面试题总结","uri":"/servlet%E9%9D%A2%E8%AF%95%E9%A2%98%E6%80%BB%E7%BB%93/"},{"categories":["面试"],"content":" MyBatis：持久层框架，负责数据库访问。 Spring MVC：表现层框架，把模型、视图、控制器分离，组合成一个灵活的系统。 Spring： 整合项目的所有框架，管理各种Java Bean（mapper、service、controller），事务控制。 Spring中的IOC（控制反转）、DI（依赖注入）、AOP（切面） IOC（Spring核心）可以认为是一个生产和管理bean的容器（可以管理bean的生命周期）；在原来需要new对象时，现在不需要了。 DI注入方式有三种 set注入（根据属性注入） 构造方法注入 根据接口注入 IOC与DI可以理解为： 控制反转是动态的向某个对象提供他所需要的对象，这一点是通过DI依赖注入实现的。 AOP面向切面:是一种编程思想，将程序中的交叉业务逻辑（比如安全，日志，事务等），封装成一个切面,注入到目标对象中。 AOP是通过代理实现的：可以是JDK动态代理，也可以是cglib代理，前者基于接口，后者基于子类。 Spring Mvc （Model、View、Controller） SpringMvc是基于过滤器对servlet进行了封装的一个框架，我们使用的时候就是在web.xml文件中配置DispatcherServlet类。 SpringMvc工作时主要是通过DispatcherServlet管理接收到的请求并进行处理。 流程 发送请求到DispatcherServlet-\u003e前端控制器通过HandlerMapping找映射地址-\u003e通过映射地址执行方法-\u003e返回ModelAndView（视图）-\u003e通过视图渲染返回界面 Mybatis 一个持久层框架,负责数据库操作； ","date":"2018-10-02","objectID":"/%E5%AF%B9%E4%BA%8Essm%E7%90%86%E8%A7%A3/:0:0","tags":["Spring","Spring Mvc","Mybatis"],"title":"对SSM的理解","uri":"/%E5%AF%B9%E4%BA%8Essm%E7%90%86%E8%A7%A3/"},{"categories":["面试"],"content":"记住两点 final不可改变 static 静态全局. final、finally、finalize的区别 final 用于声明属性,方法和类, 分别表示属性不可变, 方法不可覆盖, 类不可继承. finally 是异常处理语句结构的一部分，表示总是执行. finalize 是Object类的一个方法，在垃圾收集器执行的时候会调用被回收对象的此方法，可以覆盖此方法提供垃圾收集时的其他资源回收，例如关闭文件等. JVM不保证此方法总被调用. Static关键字 static 关键字可以用来修饰类的变量，方法和 内部类。static 是静态的意思，也就它定义的东西是全局的意思，属于全局与类相关，不与具体实例相关。就是说它调用的时候，只是ClassName.method(),而不是 new ClassName().method()。new ClassName()不就是一个对象了吗？static 的变量和方法不可以这样调用的。它不与具体的实例有关。 JAVA类首次装入时，会对静态成员变量或方法进行一次初始化分配内存空间,但方法不被调用是不会执行的。所以静态内部类在调用时才加载分配空间。仔细一想：静态内部类其实和外部类的静态变量，静态方法一样，只要被调用了都会让外部类的被加载。 Static final为什么要一起用 final与static final的区别是：final在一个对象类唯一，static final在全局唯一； final关键字的好处 final关键字提高了性能。JVM和Java应用都会缓存final变量。 final变量可以安全的在多线程环境下进行共享，而不需要额外的同步开销。 final类型的值是被保证其他线程访问该对象时，它们的值是可见安全的 使用final关键字，JVM会对方法、变量及类进行优化。 如果一个对象将会在多个线程中访问并且你并没有将其成员声明为final，则必须提供其他方式保证线程安全。 final与volatile的区别 final修饰的变量时不能同时多个修改，这样来确保值得正确性。 volatil修饰的变量主要是保证变量被写时其结果其他线程可见，强制每次都直接读内存，阻止重排序，确保voltile类型的值一旦被写入缓存必定会被立即更新到主存。 volatile 是保证变量被写时其结果其他线程可见，final 已经让该变量不能再次写了。 volatile与synchronized volatile本质是在告诉jvm当前变量在寄存器（工作内存）中的值是不确定的，需要从主存中读取； synchronized则是锁定当前变量，只有当前线程可以访问该变量，释放锁后，其他线程被才可以访问。 volatile仅能使用在变量级别；synchronized则可以使用在变量、方法、和类级别的 volatile仅能实现变量的修改可见性，不能保证原子性；而synchronized则可以保证变量的修改可见性和原子性（也就是在写操作时别的线程无法访问，这样保证数据是理想值） volatile不会造成线程的阻塞；synchronized可能会造成线程的阻塞。 ","date":"2018-09-29","objectID":"/final%E4%B8%8Estatic-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/:0:0","tags":["final","static","volatile","synchronized"],"title":"final、static、volatile、synchronized常见面试问题","uri":"/final%E4%B8%8Estatic-%E5%B8%B8%E8%A7%81%E9%9D%A2%E8%AF%95%E9%97%AE%E9%A2%98/"},{"categories":["面试"],"content":"String 类型和 StringBuffer 类型的主要性能区别其实在于 String 是不可变的对象 简要的说 String 字符串常量 StringBuffer 字符串变量（线程安全） StringBuilder 字符串变量（非线程安全） String产生新的对象来操作 Stringbuffer类型对自身操作 论效率来说 StringBuilder -\u003e StringBuffer -\u003e String 区别 在每次对 String 类型进行改变的时候其实都等同于生成了一个新的 String 对象，然后将指针指向新的 String 对象，所以经常改变内容的字符串最好不要用 String ，因为每次生成对象都会对系统性能产生影响，特别当内存中无引用对象多了以后,性能可想而知。 StringBuffer与StringBuilder 的区别 StringBuilder 不保证线程同步 方法跟StringBuffer 一样 总结 如果要操作少量的数据，用String ；单线程操作大量数据，用StringBuilder ；多线程操作大量数据，用StringBuffer。 ","date":"2018-09-28","objectID":"/stringstringbuffer%E4%B8%8Estringbuilder%E7%9A%84%E5%8C%BA%E5%88%AB/:0:0","tags":["String"],"title":"String、StringBuffer、StringBuilder的区别","uri":"/stringstringbuffer%E4%B8%8Estringbuilder%E7%9A%84%E5%8C%BA%E5%88%AB/"},{"categories":null,"content":"联系方式 E-mail：yakax1569@gmail.com 基本信息 目前所在城市：成都 个人简介 后台开发工程师 ","date":"2018-09-21","objectID":"/about/:0:0","tags":null,"title":"关于","uri":"/about/"},{"categories":null,"content":" v2rayN https://github.com/2dust/v2rayN 谷歌浏览器 https://www.google.com/intl/zh-CN_ALL/chrome/ jdk环境 下载链接 环境 JAVA_HOME ：java 安装目录如 E:\\Java\\jdk1.7.0 Path :%JAVA_HOME%\\bin;%JAVA_HOME%\\jre\\bin; CLASSPATH:.;%JAVA_HOME%\\lib;%JAVA_HOME%\\lib\\tools.jar 测试:java -version java javac 安装git 下载链接 生成 ssh ssh-keygen ssh-keygen -t rsa -C \"1569678378@qq.com\" 安装smartgit 下载链接 然后调整为系统的ssh运行 注册为个人使用然后申请(选择个人)秘钥 https://www.syntevo.com/cn/smartgit/register-non-commercial/ idea插件 破解jrebel:https://www.lanzous.com/i7gxh1e 填写：http://127.0.0.1:8888/88414687-3b91-4286-89ba-2dc813b107ce 邮箱随便输入 然后 进入jrbel 勾选离线work online 更改IDEA配置 Navicat安装 破解文档 Oracle VM VirtualBox和vagrant 安装 VirtualBox下载链接 vagrant下载链接 sublime text 下载链接 ctrl+shift+p）--》package control --》install package 加插件 ChineseLocalizations ConvertToUTF8 office 下载链接 破解过程office RETAIL=\u003eVL 然后 activite office redis 客户端 下载链接 ","date":"2018-04-06","objectID":"/%E4%B8%80%E4%BA%9B%E9%9B%B6%E7%A2%8E%E7%9A%84%E8%AE%B0%E5%BD%95/:0:0","tags":null,"title":"一些零碎的记录","uri":"/%E4%B8%80%E4%BA%9B%E9%9B%B6%E7%A2%8E%E7%9A%84%E8%AE%B0%E5%BD%95/"}]